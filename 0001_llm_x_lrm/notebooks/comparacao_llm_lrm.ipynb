{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "UR5UUSs1vSa9",
        "MBglR7um29yg",
        "WMx9g-Xf3pae"
      ],
      "authorship_tag": "ABX9TyOKx5DG5FLX10f8DvQKyGNS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pathbit/pathbit-academy-ai/blob/master/0001_llm_x_lrm/notebooks/comparacao_llm_lrm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚ú® **Pathbit Academy AI**\n",
        "---"
      ],
      "metadata": {
        "id": "1CWaJaNkr5JB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üö® **IMPORTANTE:**\n",
        "\n",
        "*üí• QUALQUER PESSOA QUE CONSIGA RESOLVER A EQUA√á√ÉO `2 + 2 = ?` PODE CONTINUAR OS PASSOS ABAIXO*"
      ],
      "metadata": {
        "id": "Xv670htP1LDr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Artigo de refer√™ncia:** [https://github.com/pathbit/pathbit-academy-ai/blob/master/0001_llm_x_lrm/article/ARTICLE.md](https://github.com/pathbit/pathbit-academy-ai/blob/master/0001_llm_x_lrm/article/ARTICLE.md)"
      ],
      "metadata": {
        "id": "IjPFjjMwyR7a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ÷é **Compara√ß√£o LLM vs LRM**\n",
        "---"
      ],
      "metadata": {
        "id": "xVCGcybMsiqE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ‚ÅâÔ∏è O que √© LLM de forma pr√°tica?\n",
        "\n",
        "**Foco:** gera√ß√£o de linguagem natural.\n",
        "Treinamento: enormes volumes de texto para aprender padr√µes lingu√≠sticos.\n",
        "\n",
        "**Ponto forte:** velocidade e flexibilidade para responder qualquer tipo de pergunta textual.\n",
        "\n",
        "**Ponto fraco:** racioc√≠nio profundo e consist√™ncia em decis√µes complexas.\n",
        "\n",
        "\n",
        "#### ‚ÅâÔ∏è O que √© LRM de forma pr√°tica?\n",
        "\n",
        "**Foco:** racioc√≠nio estruturado e resolu√ß√£o de problemas.\n",
        "Treinamento: combina dados textuais com t√©cnicas que for√ßam o modelo a explicar e validar seu racioc√≠nio (cadeia de pensamento, decomposi√ß√£o de problemas, verifica√ß√£o de hip√≥teses).\n",
        "\n",
        "**Ponto forte:** consist√™ncia em tomadas de decis√£o complexas.\n",
        "\n",
        "**Ponto fraco:** pode ser mais lento e caro que um LLM para tarefas simples."
      ],
      "metadata": {
        "id": "vtuur6nhxuec"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ‚õ≥ Cria√ß√£o de conta no Groq\n",
        "---"
      ],
      "metadata": {
        "id": "UR5UUSs1vSa9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ‚ñ∂ Acessar o site abaixo para criar sua conta\n",
        "\n",
        "**[Groq.com](https://console.groq.com/)**"
      ],
      "metadata": {
        "id": "AZ0vnkOtvX_K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ‚ñ∂ Criar uma `API KEY` para a sua conta\n",
        "\n",
        "![Cria√ß√£o API KEY](https://raw.githubusercontent.com/pathbit/pathbit-academy-ai/refs/heads/master/0001_llm_x_lrm/assets/01.png)"
      ],
      "metadata": {
        "id": "qwSYsld9wklD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ‚ñ∂ Copiar e salvar a Api Key em um lugar seguro\n",
        "\n",
        "> **Observa√ß√µes**: *Voc√™ pode acessar Api Keys criados atrav√©s deste link: [https://console.groq.com/keys](https://console.groq.com/keys)*\n",
        "\n",
        "![Copiar API KEY](https://raw.githubusercontent.com/pathbit/pathbit-academy-ai/refs/heads/master/0001_llm_x_lrm/assets/02.png)"
      ],
      "metadata": {
        "id": "hW3KkWLhwmAF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ‚ñ∂ Adicionar a Api Key criada nas `secrets` do seu Notebook\n",
        "\n",
        "![Adicionar API KEY no Secrets](https://raw.githubusercontent.com/pathbit/pathbit-academy-ai/refs/heads/master/0001_llm_x_lrm/assets/03.png)\n",
        "\n",
        "\n",
        "![Adicionar API KEY no Secrets](https://raw.githubusercontent.com/pathbit/pathbit-academy-ai/refs/heads/master/0001_llm_x_lrm/assets/04.png)"
      ],
      "metadata": {
        "id": "lBz8FBEFzVrg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ‚öôÔ∏è Configura√ß√£o do ambiente\n",
        "---"
      ],
      "metadata": {
        "id": "RqiwoNKc1GOU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ‚ñ∂ Instalar os pacotes que iremos utilizar neste projeto"
      ],
      "metadata": {
        "id": "MBglR7um29yg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "zC78TBpW-9J_"
      },
      "outputs": [],
      "source": [
        "# ==============================\n",
        "# Instalar a biblioteca do groq\n",
        "# ==============================\n",
        "!pip -q install groq"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ‚ñ∂ Criar e recuperar a vari√°vel de ambiente para utilizar no Groq"
      ],
      "metadata": {
        "id": "WMx9g-Xf3pae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Criar e recuperar a vari√°vel de ambiente do Groq\n",
        "# ------------------------------\n",
        "\n",
        "# ==============================\n",
        "# Importar os m√≥dulos\n",
        "# ==============================\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# ==============================\n",
        "# Constante com o nome da secrete adicionada no Notebook\n",
        "# ==============================\n",
        "GROQ_API_KEY_NAME = \"GROQ_API_KEY\"\n",
        "\n",
        "try:\n",
        "  # ==============================\n",
        "  # Apagar vari√°vel de ambiente criada anteriormente\n",
        "  # ==============================\n",
        "  os.environ.pop(GROQ_API_KEY_NAME, None)\n",
        "\n",
        "  # ==============================\n",
        "  # Recupera a secrete adicionada no Notebook\n",
        "  # ==============================\n",
        "  groq_api_key = userdata.get(GROQ_API_KEY_NAME)\n",
        "\n",
        "  # ==============================\n",
        "  # Cria a vari√°vel de ambiente para o Groq\n",
        "  # ==============================\n",
        "  os.environ[GROQ_API_KEY_NAME] = groq_api_key\n",
        "\n",
        "  # ==============================\n",
        "  # Verifica se a vari√°vel de ambiente n√£o existe\n",
        "  # ==============================\n",
        "  if GROQ_API_KEY_NAME not in os.environ:\n",
        "    raise ValueError(f\"Vari√°vel de ambiente {GROQ_API_KEY_NAME} n√£o definida\")\n",
        "\n",
        "  # ==============================\n",
        "  # Imprime o valor da vari√°vel de ambiente criada para o Groq\n",
        "  # ==============================\n",
        "  print(f\"‚úÖ {GROQ_API_KEY_NAME}: {os.environ['GROQ_API_KEY'][:6]}******\")\n",
        "except Exception as e:\n",
        "  # ==============================\n",
        "  # Imprime o erro ao tentar recuperar e atualizar a vari√°vel de ambiente\n",
        "  # ==============================\n",
        "  print(f\"‚ùå Erro ao atualizar a vari√°vel de ambiente {GROQ_API_KEY_NAME}: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRkCX4pC3p3P",
        "outputId": "fa772a04-c21f-4aa1-d482-2302eb8f1ad0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ GROQ_API_KEY: gsk_P1******\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ‚ñ∂ Validar se o Groq est√° funcionando corretamente\n",
        "\n",
        "‚ö†Ô∏è Este modelo `compound-beta` √© da pr√≥pria `Groq`, tem um resultado excelente at√© estes momentos dos meus testes."
      ],
      "metadata": {
        "id": "lvBbPHNVvd9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# Importar os m√≥dulos\n",
        "# ==============================\n",
        "import os\n",
        "from groq import Groq\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "# ==============================\n",
        "# Configurar a API Key da Groq\n",
        "# (recomendo guardar em segredos do Colab)\n",
        "# ==============================\n",
        "groq_api_key = os.environ[GROQ_API_KEY_NAME]\n",
        "client = Groq(api_key=groq_api_key)\n",
        "\n",
        "# ==============================\n",
        "# Definir o modelo de LLM que iremos utilizar\n",
        "# Modelos: https://console.groq.com/docs/models\n",
        "# ==============================\n",
        "LLM_MODEL = \"compound-beta\"\n",
        "\n",
        "# ==============================\n",
        "# Criar a prompt do sistema\n",
        "# ==============================\n",
        "prompt_sistema = \"\"\"\n",
        "REGRAS:\n",
        "\n",
        "++++\n",
        "\n",
        "Voc√™ √© um especialista da √°rea de intelig√™ncia artificial e est√° instruindo\n",
        "crian√ßas e adolescentes no per√≠odo escolar fundamental.\n",
        "\n",
        "  1. Utilize respostas f√°ceis para o seu p√∫blico.\n",
        "\n",
        "  2. Utilize exemplos pr√°ticos do dia-a-dia desse p√∫blico.\n",
        "\n",
        "  3. A resposta deve ser formatada no padr√£o Markdown, seus t√≠tulos devem come√ßar\n",
        "  com 3 \"#\", utilizar emojis e ter as quebras adequadas para separar bem cada\n",
        "  parte do texto, facilitando a leitura.\n",
        "\n",
        "  4. Voc√™ pode utiliar um pouco de linguagem t√©cnica at√© para que a pessoa tenha\n",
        "  interesse em continuar pesquisando sobre o tema e seus subtemas.\n",
        "\n",
        "++++\n",
        "\n",
        "\"\"\".strip()\n",
        "\n",
        "# ==============================\n",
        "# Pergunta do usu√°rio\n",
        "# ==============================\n",
        "pergunta_usuario = \"Qual a diferen√ßa entre LLMs e LRMs?\"\n",
        "\n",
        "# ==============================\n",
        "# Criar o prompt final concatenado (para exibir ou logar)\n",
        "# ==============================\n",
        "prompt_final = f\"{prompt_sistema}\\n\\nUSU√ÅRIO:\\n\\n{pergunta_usuario}\"\n",
        "\n",
        "# ==============================\n",
        "# Montar mensagens no formato da Groq e chamar o modelo\n",
        "# ==============================\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": prompt_sistema},\n",
        "    {\"role\": \"user\", \"content\": pergunta_usuario},\n",
        "]\n",
        "\n",
        "resp = client.chat.completions.create(\n",
        "    model=LLM_MODEL,\n",
        "    messages=messages,\n",
        "    temperature=0.3,\n",
        "    max_completion_tokens=1024,\n",
        ")\n",
        "\n",
        "resposta_modelo = resp.choices[0].message.content\n",
        "\n",
        "# ==============================\n",
        "# Visualizando o resultado formatado em Markdown\n",
        "# ==============================\n",
        "display(Markdown(f\"\"\"\n",
        "## > Prompt do sistema\n",
        "{prompt_sistema}\n",
        "\n",
        "## > Pergunta do usu√°rio\n",
        "{pergunta_usuario}\n",
        "\n",
        "## > Prompt final enviado ao LLM\n",
        "{prompt_final}\n",
        "\n",
        "## > Resposta do LLM\n",
        "{resposta_modelo}\n",
        "\"\"\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IKUwA6xYvThe",
        "outputId": "8ff33333-79fe-4dfe-d9ec-0e2c2693452e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n## > Prompt do sistema\nREGRAS:\n\n++++\n\nVoc√™ √© um especialista da √°rea de intelig√™ncia artificial e est√° instruindo\ncrian√ßas e adolescentes no per√≠odo escolar fundamental.\n\n  1. Utilize respostas f√°ceis para o seu p√∫blico.\n\n  2. Utilize exemplos pr√°ticos do dia-a-dia desse p√∫blico.\n\n  3. A resposta deve ser formatada no padr√£o Markdown, seus t√≠tulos devem come√ßar\n  com 3 \"#\", utilizar emojis e ter as quebras adequadas para separar bem cada \n  parte do texto, facilitando a leitura.\n\n  4. Voc√™ pode utiliar um pouco de linguagem t√©cnica at√© para que a pessoa tenha\n  interesse em continuar pesquisando sobre o tema e seus subtemas.\n\n++++\n\n## > Pergunta do usu√°rio\nQual a diferen√ßa entre LLMs e LRMs?\n\n## > Prompt final enviado ao LLM\nREGRAS:\n\n++++\n\nVoc√™ √© um especialista da √°rea de intelig√™ncia artificial e est√° instruindo\ncrian√ßas e adolescentes no per√≠odo escolar fundamental.\n\n  1. Utilize respostas f√°ceis para o seu p√∫blico.\n\n  2. Utilize exemplos pr√°ticos do dia-a-dia desse p√∫blico.\n\n  3. A resposta deve ser formatada no padr√£o Markdown, seus t√≠tulos devem come√ßar\n  com 3 \"#\", utilizar emojis e ter as quebras adequadas para separar bem cada \n  parte do texto, facilitando a leitura.\n\n  4. Voc√™ pode utiliar um pouco de linguagem t√©cnica at√© para que a pessoa tenha\n  interesse em continuar pesquisando sobre o tema e seus subtemas.\n\n++++\n\nUSU√ÅRIO:\n\nQual a diferen√ßa entre LLMs e LRMs?\n\n## > Resposta do LLM\n### Diferen√ßa entre LLMs e LRMs\nA diferen√ßa entre LLMs (Large Language Models) e LRMs (Large Reasoning Models) √© fundamentalmente baseada no seu prop√≥sito e abordagem.\n\n#### LLMs\nOs LLMs s√£o projetados para processar e gerar texto de forma muito realista, imitando a linguagem humana. Eles s√£o treinados com grandes conjuntos de dados de texto para aprender padr√µes e estruturas da linguagem. Isso os torna capazes de realizar tarefas como:\n- Gerar texto coerente e natural\n- Traduzir textos de um idioma para outro\n- Responder a perguntas de forma apropriada\n- Sumarizar textos longos\n\n#### LRMs\nPor outro lado, os LRMs s√£o projetados para raciocinar e gerar respostas l√≥gicas e coerentes. Eles s√£o treinados com conjuntos de dados que enfatizam a l√≥gica e o racioc√≠nio, permitindo que realizem tarefas como:\n- Resolver problemas l√≥gicos\n- Fazer infer√™ncias baseadas em informa√ß√µes dispon√≠veis\n- Tomar decis√µes com base em regras e condi√ß√µes\n- Diagnosticar problemas complexos\n\n#### Resumo da Diferen√ßa\nEm resumo, a principal diferen√ßa entre LLMs e LRMs est√° no seu foco:\n- **LLMs** se concentram em **processar e gerar linguagem natural**, buscando imitar a forma como os humanos se comunicam.\n- **LRMs** se concentram em **raciocinar e tomar decis√µes l√≥gicas**, buscando resolver problemas e tomar decis√µes com base em regras e l√≥gica.\n\nEssa diferen√ßa reflete os diferentes objetivos e aplica√ß√µes desses modelos, com LLMs sendo mais comuns em tarefas de processamento de linguagem natural e LRMs sendo mais adequados para tarefas que requerem racioc√≠nio e tomada de decis√µes l√≥gicas.\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### </> Criando as fun√ß√µes para utilizar modelos de LLM e LRM\n",
        "---\n",
        "\n",
        "**‚ÑπÔ∏èÔ∏è Observa√ß√µes:**\n",
        "\n",
        "*Tentei fazer a melhor documenta√ß√£o poss√≠vel para explicar o c√≥digo.*\n",
        "\n",
        "**‚ö†Ô∏è Importante:**\n",
        "\n",
        "*Para garantir os melhores resultados, o c√≥digo utiliza dois modelos de linguagem diferentes, cada um com uma finalidade espec√≠fica:*\n",
        "\n",
        "- **`Llama3-70B-8192:`** Um modelo mais gen√©rico, com um foco em compreens√£o de linguagem natural e tarefas de conversa√ß√£o. Ele √© ideal para interpretar o contexto do texto e gerar respostas fluentes e coerentes, garantindo que a comunica√ß√£o seja clara e natural. O `llama3-70b-8192` est√° focado gerar texto de forma flu√≠da e natural, como uma conversa, mas n√£o em resolver problemas l√≥gicos ou matem√°ticos complexos passo a passo como o outro modelo.\n",
        "\n",
        "- **`DeepSeek R1 Distill Llama 70B:`** Este modelo √© especializado em tarefas que exigem um racioc√≠nio complexo, como l√≥gica, matem√°tica e programa√ß√£o. Sua arquitetura √© otimizada para resolver problemas estruturados de forma eficiente. O `deepseek-r1-distill-llama-70b` √© um exemplo, focado em \"pensar\" passo a passo antes de chegar a uma resposta."
      ],
      "metadata": {
        "id": "OAOrEU7q7F1J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **FUN√á√ïES B√ÅSICAS**"
      ],
      "metadata": {
        "id": "d95kpV6QA3oc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**1. Fun√ß√£o para recuperar o cliente do Groq**\n"
      ],
      "metadata": {
        "id": "qd0IVDMn-MaJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# Importar os m√≥dulos\n",
        "# ==============================\n",
        "import os\n",
        "import time\n",
        "from groq import Groq\n",
        "from typing import Tuple, Optional\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "\n",
        "def criar_cliente_groq() -> Groq:\n",
        "    \"\"\"\n",
        "    Cria o cliente da Groq usando a vari√°vel de ambiente GROQ_API_KEY.\n",
        "\n",
        "    Por que usar env var?\n",
        "    - Seguran√ßa: evita hardcode de chaves no notebook.\n",
        "    - Reprodutibilidade: o mesmo c√≥digo funciona em diferentes ambientes.\n",
        "    \"\"\"\n",
        "    groq_api_key = os.environ[GROQ_API_KEY_NAME]\n",
        "    if not groq_api_key:\n",
        "        raise RuntimeError(\n",
        "            \"GROQ_API_KEY n√£o definida. No Colab, use: os.environ['GROQ_API_KEY']='sua_chave'\"\n",
        "        )\n",
        "    return Groq(api_key=groq_api_key)\n",
        "\n",
        "\n",
        "def exibir_resposta(\n",
        "    modelo: str,\n",
        "    pergunta: str,\n",
        "    resposta: str,\n",
        "    raciocinio: str = None,\n",
        "    tempo: float = 0.0\n",
        "  ):\n",
        "    \"\"\"\n",
        "    Exibe sa√≠da formatada em Markdown no Jupyter/Colab.\n",
        "    - modelo     : nome do modelo utilizado (LLM ou LRM)\n",
        "    - pergunta   : texto enviado\n",
        "    - resposta   : resposta final ao usu√°rio\n",
        "    - raciocinio : cadeia de racioc√≠nio (opcional)\n",
        "    - tempo      : tempo total da execu√ß√£o\n",
        "    \"\"\"\n",
        "    texto_raciocinio = \"\"\n",
        "\n",
        "    if raciocinio:\n",
        "        texto_raciocinio += f\"\\n\\n\"\n",
        "        texto_raciocinio += f\"## üßê Racioc√≠nio                                \\n\"\n",
        "        texto_raciocinio += f\"================================================\\n\\n\"\n",
        "        texto_raciocinio += f\"{raciocinio.strip()}\"\n",
        "\n",
        "    texto_md = f\"\"\"\n",
        "## üß† Modelo: `{modelo}`\n",
        "**‚è± Tempo de execu√ß√£o:** {tempo:.2f}s\n",
        "\n",
        "{texto_raciocinio}\n",
        "\n",
        "### üì• Pergunta\n",
        "\n",
        "{pergunta.strip()}\n",
        "\n",
        "### üì§ Resposta\n",
        "\n",
        "{resposta.strip()}\n",
        "    \"\"\"\n",
        "\n",
        "    display(Markdown(texto_md))"
      ],
      "metadata": {
        "id": "2syPSkoZ8Tgg"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **LLM: FUN√á√ïES E TESTES**\n",
        "\n"
      ],
      "metadata": {
        "id": "HcQRCQ4yAljB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. LLM: Fun√ß√£o para responder usando modelo llama3 do Groq**"
      ],
      "metadata": {
        "id": "UMaq_7lW-r2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# Importar os m√≥dulos\n",
        "# ==============================\n",
        "import os\n",
        "import time\n",
        "from typing import Tuple, Optional\n",
        "from groq import Groq\n",
        "\n",
        "# ==============================\n",
        "# Fun√ß√£o: perguntar_llm\n",
        "# Objetivo: enviar pergunta a um modelo de LINGUAGEM (LLM)\n",
        "# Retorna: (texto_resposta, tempo_em_segundos)\n",
        "# ==============================\n",
        "def perguntar_llm(\n",
        "    pergunta: str,\n",
        "    modelo: str = \"llama3-70b-8192\",       # Modelo padr√£o para linguagem natural\n",
        "    temperatura: float = 0.2,              # Controla a aleatoriedade da resposta\n",
        "    max_tokens_resposta: int = 1024,       # Limita tamanho da resposta (custo/lat√™ncia)\n",
        "    sistema: Optional[str] = None          # Regras ou persona opcionais\n",
        ") -> Tuple[str, float]:\n",
        "    \"\"\"\n",
        "    Envia uma pergunta para um modelo LLM.\n",
        "\n",
        "    Par√¢metros:\n",
        "    - pergunta           : texto enviado ao modelo.\n",
        "    - modelo             : modelo LLM para tarefas gerais (resumo, reescrita, Q&A).\n",
        "    - temperatura        : controla a \"criatividade\" (0.1‚Äì0.3 = mais determin√≠stico).\n",
        "    - max_tokens_resposta: limita tamanho da sa√≠da, evitando custo ou lentid√£o.\n",
        "    - sistema            : mensagem opcional para regras ou persona.\n",
        "\n",
        "    Retorna:\n",
        "    - texto: resposta final do modelo.\n",
        "    - tempo: tempo total da chamada em segundos.\n",
        "    \"\"\"\n",
        "    # cria o cliente autenticado\n",
        "    cliente = criar_cliente_groq()\n",
        "\n",
        "    # monta mensagens no formato da API (sistema opcional + usu√°rio obrigat√≥rio)\n",
        "    mensagens = []\n",
        "    if sistema:\n",
        "        mensagens.append({\"role\": \"system\", \"content\": sistema})\n",
        "\n",
        "    mensagens.append({\"role\": \"user\", \"content\": pergunta})\n",
        "\n",
        "    # marca in√≠cio para medir tempo de execu√ß√£o\n",
        "    inicio = time.perf_counter()\n",
        "\n",
        "    # chamada √† API da Groq\n",
        "    resposta = cliente.chat.completions.create(\n",
        "        model=modelo,\n",
        "        messages=mensagens,\n",
        "        temperature=temperatura,\n",
        "        max_completion_tokens=max_tokens_resposta,\n",
        "    )\n",
        "\n",
        "    # calcula tempo total\n",
        "    tempo_execucao = time.perf_counter() - inicio\n",
        "\n",
        "    # extrai conte√∫do textual da resposta\n",
        "    texto_resposta = resposta.choices[0].message.content.strip()\n",
        "\n",
        "    return modelo, texto_resposta, tempo_execucao"
      ],
      "metadata": {
        "id": "VPoE6SHo-qCJ"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. LLM: Testando a fun√ß√£o para responder com LLM**"
      ],
      "metadata": {
        "id": "ijpWGrbX_bX7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# Testando a LLM\n",
        "# ==============================\n",
        "\n",
        "# LLM: resumo e reescrita de tom (usando perguntar_llm)\n",
        "texto = \"\"\"\n",
        "O Banco Central manteve a taxa Selic inalterada. Analistas projetam estabilidade\n",
        "nos pr√≥ximos meses, com aten√ß√£o √† infla√ß√£o de servi√ßos e ao mercado de trabalho.\n",
        "\"\"\"\n",
        "\n",
        "# Pedimos um RESUMO curto (linguagem neutra) ‚Äî tarefa t√≠pica de LLM\n",
        "prompt_resumo = f\"Resuma em 2 frases, com linguagem neutra e direta: {texto}\"\n",
        "modelo_resumo, resposta_resumo, tempo_resumo = perguntar_llm(\n",
        "    pergunta=prompt_resumo,\n",
        "    sistema=\"Responda em Markdown de forma clara e concisa.\"\n",
        ")\n",
        "\n",
        "exibir_resposta(\n",
        "    modelo=modelo_resumo,\n",
        "    pergunta=prompt_resumo,\n",
        "    resposta=resposta_resumo,\n",
        "    tempo=tempo_resumo\n",
        ")\n",
        "\n",
        "# Agora pedimos para REESCREVER o pr√≥prio resumo com outro tom.\n",
        "#   - IMPORTANTE: inclu√≠mos explicitamente o 'resumo' no prompt (o modelo n√£o ‚Äúlembra‚Äù sozinho).\n",
        "prompt_tom = (\n",
        "    \"Reescreva o resumo abaixo com tom mais informal e amig√°vel, mantendo precis√£o.\\n\\n\"\n",
        "    f\"RESUMO ORIGINAL:\\n{resposta_resumo}\"\n",
        ")\n",
        "modelo_tom, resposta_tom, tempo_tom = perguntar_llm(\n",
        "    pergunta=prompt_tom,\n",
        "    sistema=\"Responda em Markdown e use linguagem acess√≠vel; evite jarg√µes.\"\n",
        ")\n",
        "\n",
        "exibir_resposta(\n",
        "    modelo=modelo_tom,\n",
        "    pergunta=prompt_tom,\n",
        "    resposta=resposta_tom,\n",
        "    tempo=tempo_tom\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "id": "XxprzeEJ_bJL",
        "outputId": "5deeedab-80c7-4705-b36f-4d79870a4a79"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n## üß† Modelo: `llama3-70b-8192`  \n**‚è± Tempo de execu√ß√£o:** 0.38s  \n\n\n\n### üì• Pergunta\n\nResuma em 2 frases, com linguagem neutra e direta: \nO Banco Central manteve a taxa Selic inalterada. Analistas projetam estabilidade\nnos pr√≥ximos meses, com aten√ß√£o √† infla√ß√£o de servi√ßos e ao mercado de trabalho.\n\n### üì§ Resposta\n\nO Banco Central manteve a taxa Selic inalterada, indicando estabilidade nos pr√≥ximos meses. Analistas seguem atentos √† infla√ß√£o de servi√ßos e ao mercado de trabalho para avaliar poss√≠veis mudan√ßas futuras.\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n## üß† Modelo: `llama3-70b-8192`  \n**‚è± Tempo de execu√ß√£o:** 0.42s  \n\n\n\n### üì• Pergunta\n\nReescreva o resumo abaixo com tom mais informal e amig√°vel, mantendo precis√£o.\n\nRESUMO ORIGINAL:\nO Banco Central manteve a taxa Selic inalterada, indicando estabilidade nos pr√≥ximos meses. Analistas seguem atentos √† infla√ß√£o de servi√ßos e ao mercado de trabalho para avaliar poss√≠veis mudan√ßas futuras.\n\n### üì§ Resposta\n\n**Boas Not√≠cias!**\nO Banco Central decidiu manter a taxa Selic igual, o que significa que as coisas devem ficar est√°veis nos pr√≥ximos meses. Agora, os especialistas est√£o de olho na infla√ß√£o de servi√ßos e no mercado de trabalho para ver se h√° alguma mudan√ßa no horizonte.\n    "
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **LRM: FUN√á√ïES E TESTES**"
      ],
      "metadata": {
        "id": "cJs63UUhAwja"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. LRM: Fun√ß√£o para responder usando modelo deepseek do Groq**"
      ],
      "metadata": {
        "id": "gYh2luuPAzrJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# Importar os m√≥dulos\n",
        "# ==============================\n",
        "import os\n",
        "import time\n",
        "from typing import Tuple, Optional\n",
        "from groq import Groq\n",
        "\n",
        "# ==============================\n",
        "# Fun√ß√£o: perguntar_lrm\n",
        "# Objetivo: enviar pergunta a um modelo de RACIOC√çNIO (LRM)\n",
        "# Retorna: (texto_ao_usuario, cadeia_de_raciocinio, tempo_em_segundos)\n",
        "# ==============================\n",
        "def perguntar_lrm(\n",
        "    pergunta: str,\n",
        "    modelo: str = \"deepseek-r1-distill-llama-70b\",   # Modelo otimizado para racioc√≠nio\n",
        "    temperatura: float = 0.2,                        # Baixa varia√ß√£o para consist√™ncia\n",
        "    formato_raciocinio: Optional[str] = \"parsed\",    # 'parsed', 'raw', 'hidden' ou None\n",
        "    incluir_raciocinio: bool = True,                 # S√≥ usado se formato_raciocinio=None\n",
        "    max_tokens_resposta: int = 2000,                 # Limite da resposta final\n",
        "    max_tokens_raciocinio: Optional[int] = None,     # Limite da cadeia de racioc√≠nio\n",
        "    sistema: Optional[str] = None                    # Mensagem de sistema opcional\n",
        ") -> Tuple[str, Optional[str], float]:\n",
        "    \"\"\"\n",
        "    Envia uma pergunta a um modelo de racioc√≠nio (LRM) da Groq.\n",
        "\n",
        "    Comportamento:\n",
        "    - Se `formato_raciocinio` ‚àà {'parsed','raw','hidden'} ‚Üí usa `reasoning_format` e\n",
        "      **N√ÉO** envia `include_reasoning` (evita erro 400).\n",
        "      ‚Ä¢ 'parsed'  ‚Üí cadeia vem separada em `message.reasoning`\n",
        "      ‚Ä¢ 'raw'     ‚Üí cadeia vem crua, com marca√ß√µes\n",
        "      ‚Ä¢ 'hidden'  ‚Üí o modelo raciocina, mas n√£o retorna a cadeia\n",
        "    - Se `formato_raciocinio` for None ‚Üí n√£o envia `reasoning_format` e usa `include_reasoning`.\n",
        "\n",
        "    Retorno:\n",
        "      (conteudo_final, cadeia_de_raciocinio|None, tempo_execucao_s)\n",
        "    \"\"\"\n",
        "    # Cliente autenticado\n",
        "    chave = os.getenv(\"GROQ_API_KEY\")\n",
        "    if not chave:\n",
        "        raise RuntimeError(\"Defina GROQ_API_KEY no ambiente.\")\n",
        "    cliente = Groq(api_key=chave)\n",
        "\n",
        "    # Mensagens no formato Chat\n",
        "    mensagens = []\n",
        "    if sistema:\n",
        "        mensagens.append({\"role\": \"system\", \"content\": sistema})\n",
        "    mensagens.append({\"role\": \"user\", \"content\": pergunta})\n",
        "\n",
        "    # Monta kwargs din√¢micos conforme as regras da API\n",
        "    kwargs = {\n",
        "        \"model\": modelo,\n",
        "        \"messages\": mensagens,\n",
        "        \"temperature\": temperatura,\n",
        "        \"max_completion_tokens\": max_tokens_resposta,\n",
        "    }\n",
        "\n",
        "    # Se limite de tokens do racioc√≠nio foi definido, adiciona\n",
        "    if max_tokens_raciocinio is not None:\n",
        "        kwargs[\"max_reasoning_tokens\"] = max_tokens_raciocinio\n",
        "\n",
        "    # Regra de exclus√£o m√∫tua:\n",
        "    # - Se formato_raciocinio estiver definido, usa reasoning_format e IGNORA include_reasoning\n",
        "    # - Se formato_raciocinio for None, usa include_reasoning (sem reasoning_format)\n",
        "    if formato_raciocinio is not None:\n",
        "        kwargs[\"reasoning_format\"] = formato_raciocinio\n",
        "        # N√ÉO adicionar include_reasoning para evitar o erro 400\n",
        "    else:\n",
        "        kwargs[\"include_reasoning\"] = bool(incluir_raciocinio)\n",
        "\n",
        "    # Chamada e cron√¥metro\n",
        "    inicio = time.perf_counter()\n",
        "    resp = cliente.chat.completions.create(**kwargs)\n",
        "    tempo = time.perf_counter() - inicio\n",
        "\n",
        "    # Extra√ß√£o da resposta e (se houver) do racioc√≠nio\n",
        "    msg = resp.choices[0].message\n",
        "    conteudo = (msg.content or \"\").strip()\n",
        "\n",
        "    # A cadeia s√≥ vem quando:\n",
        "    # - usamos reasoning_format ('parsed'/'raw'), E\n",
        "    # - n√£o √© 'hidden'\n",
        "    if formato_raciocinio in (\"parsed\", \"raw\"):\n",
        "        raciocinio = getattr(msg, \"reasoning\", None)\n",
        "    else:\n",
        "        # Quando formato=None e include_reasoning=True, algumas combina√ß√µes\n",
        "        # podem retornar o racioc√≠nio embutido; a API nem sempre exp√µe em `reasoning`.\n",
        "        raciocinio = getattr(msg, \"reasoning\", None)\n",
        "\n",
        "    return modelo, conteudo, raciocinio, tempo"
      ],
      "metadata": {
        "id": "pqYmYrwP-RoL"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. LRM: Testando a fun√ß√£o para responder com LRM**\n",
        "\n",
        "-\n",
        "\n",
        "**‚ÑπÔ∏èÔ∏è IMPORTANTE:**\n",
        "\n",
        "> *Para ver como o modelo chegou √† resposta final, procure a se√ß√£o **üßê Racioc√≠nio.** Essa √© uma demonstra√ß√£o do poder dos modelos especializados em l√≥gica e racioc√≠nio.*\n"
      ],
      "metadata": {
        "id": "9eS76H1EDeJD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# Testando a LRM\n",
        "# ==============================\n",
        "\n",
        "# LRM: an√°lise estruturada com justificativa\n",
        "import json\n",
        "\n",
        "cenario = {\n",
        "    \"perfil\": \"conservador\",\n",
        "    \"inflacao_projetada_aa\": 4.0,\n",
        "    \"cdi_aa\": 13.25,\n",
        "    \"vencimentos_anos\": [2026, 2027],\n",
        "    \"objetivo\": \"renda previs√≠vel com baixa volatilidade\",\n",
        "}\n",
        "cenario_fmt = json.dumps(cenario, ensure_ascii=False)\n",
        "\n",
        "prompt_lrm = f\"\"\"\n",
        "Voc√™ √© um analista de investimentos.\n",
        "\n",
        "Dado o cen√°rio:\n",
        "- {cenario}\n",
        "\n",
        "Decida entre:\n",
        "- t√≠tulo atrelado ao CDI;\n",
        "- prefixado curto;\n",
        "- IPCA+ longo.\n",
        "\n",
        "Importante:\n",
        "\n",
        "1. Resposta e racioc√≠nio sempre em portug√™s do brasil.\n",
        "2. Estruture o racioc√≠nio em etapas (riscos, liquidez, duration, sensibilidade √† infla√ß√£o).\n",
        "3. Apresente a recomenda√ß√£o final e uma pol√≠tica de rebalanceamento simples.\n",
        "4. Use n√∫meros aproximados e justificativas claras.\n",
        "\"\"\"\n",
        "\n",
        "modelo, resposta, cadeia, tempo = perguntar_lrm(\n",
        "    pergunta=prompt_lrm,\n",
        "    sistema=\"Responda em Markdown, com listas numeradas nas etapas.\"\n",
        ")\n",
        "\n",
        "exibir_resposta(\n",
        "    modelo=modelo,\n",
        "    pergunta=prompt_lrm,\n",
        "    resposta=resposta,\n",
        "    raciocinio=cadeia,\n",
        "    tempo=tempo\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3ejlx82kDZ2P",
        "outputId": "c90f98a3-ddff-4360-a7dc-f4519934f45a"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n## üß† Modelo: `deepseek-r1-distill-llama-70b`  \n**‚è± Tempo de execu√ß√£o:** 4.03s  \n\n\n\n## üßê Racioc√≠nio                                \n================================================\n\nOk, vou come√ßar analisando o perfil do investidor. Ele √© conservador, ent√£o busca renda previs√≠vel com baixa volatilidade. Isso me faz pensar que ele n√£o quer correr riscos altos e prefere estabilidade.\n\nAgora, olhando para as op√ß√µes: t√≠tulo atrelado ao CDI, prefixado curto e IPCA+ longo. O CDI est√° em 13,25% ao ano, e a infla√ß√£o projetada √© 4%. O investidor tem vencimentos em 2026 e 2027, ent√£o o prazo √© m√©dio.\n\nPrimeiro, vou considerar os riscos. O t√≠tulo atrelado ao CDI acompanha a taxa de juros, o que pode ser bom, mas se os juros ca√≠rem, o valor pode diminuir. O prefixado curto tem menor risco de mercado, j√° que o valor √© fixo, e o IPCA+ longo est√° mais protegido contra infla√ß√£o, mas pode ter mais volatilidade.\n\nEm rela√ß√£o √† liquidez, o t√≠tulo atrelado ao CDI pode ser negociado com facilidade, assim como o prefixado curto. O IPCA+ longo, por ser mais espec√≠fico, pode ter menos liquidez, o que pode ser um problema se o investidor precisar resgatar r√°pido.\n\nO duration tamb√©m √© importante. O IPCA+ longo tem um duration maior, o que o torna mais sens√≠vel a mudan√ßas nas taxas de juros. J√° o prefixado curto tem duration menor, menos afetado por isso. O t√≠tulo atrelado ao CDI acompanha as taxas de curto prazo, ent√£o tamb√©m menos exposi√ß√£o a mudan√ßas.\n\nSobre a infla√ß√£o, o IPCA+ longo oferece prote√ß√£o contra a infla√ß√£o, o que √© bom, mas o investidor j√° tem uma infla√ß√£o projetada de 4%, e o CDI est√° em 13,25%, ent√£o talvez o IPCA+ n√£o ofere√ßa um retorno significativamente maior.\n\nConsiderando o objetivo de renda previs√≠vel e baixa volatilidade, o prefixado curto parece mais adequado. Ele oferece retorno fixo, menor risco de mercado e liquidez razo√°vel. O IPCA+ longo, apesar de proteger contra infla√ß√£o, pode ter mais volatilidade, o que n√£o √© ideal para um perfil conservador.\n\nPara a pol√≠tica de rebalanceamento, recomendaria rever a carteira anualmente ou sempre que houver mudan√ßas significativas nas taxas de juros ou na infla√ß√£o. Isso ajudaria a manter a exposi√ß√£o de risco sob controle e ajustar conforme necess√°rio.\n\nPortanto, a recomenda√ß√£o seria investir no prefixado curto, pois ele atende melhor √†s necessidades do investidor em termos de risco, liquidez e retorno previs√≠vel.\n\n### üì• Pergunta\n\nVoc√™ √© um analista de investimentos. \n\nDado o cen√°rio:\n- {'perfil': 'conservador', 'inflacao_projetada_aa': 4.0, 'cdi_aa': 13.25, 'vencimentos_anos': [2026, 2027], 'objetivo': 'renda previs√≠vel com baixa volatilidade'}\n\nDecida entre: \n- t√≠tulo atrelado ao CDI; \n- prefixado curto; \n- IPCA+ longo.\n\nImportante:\n\n1. Resposta e racioc√≠nio sempre em portug√™s do brasil.\n2. Estruture o racioc√≠nio em etapas (riscos, liquidez, duration, sensibilidade √† infla√ß√£o).\n3. Apresente a recomenda√ß√£o final e uma pol√≠tica de rebalanceamento simples.\n4. Use n√∫meros aproximados e justificativas claras.\n\n### üì§ Resposta\n\n### An√°lise de Investimento para Perfil Conservador\n\n#### 1. **Riscos**\n   - **T√≠tulo atrelado ao CDI**: Oferece retorno ligado ao CDI (13,25% a.a.), mas pode sofrer impacto se os juros ca√≠rem.\n   - **Prefixado curto**: Retorno fixo menor, mas com menor risco de mercado.\n   - **IPCA+ longo**: Prote√ß√£o contra infla√ß√£o, mas com maior volatilidade.\n\n#### 2. **Liquidez**\n   - **T√≠tulo atrelado ao CDI**: Alta liquidez.\n   - **Prefixado curto**: Liquidez moderada.\n   - **IPCA+ longo**: Liquidez baixa devido ao prazo mais longo.\n\n#### 3. **Duration**\n   - **T√≠tulo atrelado ao CDI**: Duration baixo, menos sens√≠vel a mudan√ßas nas taxas.\n   - **Prefixado curto**: Duration menor, menos impactado por taxas.\n   - **IPCA+ longo**: Duration maior, mais sens√≠vel a mudan√ßas nas taxas.\n\n#### 4. **Sensibilidade √† Infla√ß√£o**\n   - **T√≠tulo atrelado ao CDI**: Sens√≠vel a mudan√ßas nos juros.\n   - **Prefixado curto**: Menos sens√≠vel √† infla√ß√£o.\n   - **IPCA+ longo**: Protege contra infla√ß√£o, mas com maior volatilidade.\n\n### Recomenda√ß√£o Final\n**Investimento Recomendado:** **Prefixado curto**\n\n**Justificativa:**\n- Oferece retorno fixo e baixo risco de mercado.\n- Menor exposi√ß√£o a mudan√ßas nas taxas de juros.\n- Liquidez moderada, adequada para vencimentos em 2026 e 2027.\n\n### Pol√≠tica de Rebalanceamento\n- **Revis√£o Anual:** Ajustar a carteira anualmente para manter o perfil de risco.\n- **Monitoramento de Taxas:** Acompanhar mudan√ßas nas taxas de juros e infla√ß√£o para poss√≠veis ajustes.\n\n**Conclus√£o:** O prefixado curto √© a op√ß√£o mais adequada para um investidor conservador, oferecendo estabilidade e retorno previs√≠vel com baixa volatilidade.\n    "
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparando ÷é LLM vs LRM üß†\n",
        "---"
      ],
      "metadata": {
        "id": "bIbj9yHPDx5l"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EFQKdnzCELm6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}