{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/pathbit/pathbit-academy-ai/blob/master/0001_llm_x_lrm%20/notebooks/comparacao_llm_lrm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1CWaJaNkr5JB"
   },
   "source": [
    "# ✨ **Pathbit Academy AI**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xv670htP1LDr"
   },
   "source": [
    "🚨 **IMPORTANTE:**\n",
    "\n",
    "*💥 QUALQUER PESSOA QUE CONSIGA RESOLVER A EQUAÇÃO `2 + 2 = ?` PODE CONTINUAR OS PASSOS ABAIXO*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IjPFjjMwyR7a"
   },
   "source": [
    "**Artigo de referência:** [https://github.com/pathbit/pathbit-academy-ai/blob/master/0001_llm_x_lrm/article/ARTICLE.md](https://github.com/pathbit/pathbit-academy-ai/blob/master/0001_llm_x_lrm/article/ARTICLE.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xVCGcybMsiqE"
   },
   "source": [
    "### ֎ **Comparação LLM vs LRM**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vtuur6nhxuec"
   },
   "source": [
    "#### ⁉️ O que é LLM de forma prática?\n",
    "\n",
    "**Foco:** geração de linguagem natural.\n",
    "Treinamento: enormes volumes de texto para aprender padrões linguísticos.\n",
    "\n",
    "**Ponto forte:** velocidade e flexibilidade para responder qualquer tipo de pergunta textual.\n",
    "\n",
    "**Ponto fraco:** raciocínio profundo e consistência em decisões complexas.\n",
    "\n",
    "\n",
    "#### ⁉️ O que é LRM de forma prática?\n",
    "\n",
    "**Foco:** raciocínio estruturado e resolução de problemas.\n",
    "Treinamento: combina dados textuais com técnicas que forçam o modelo a explicar e validar seu raciocínio (cadeia de pensamento, decomposição de problemas, verificação de hipóteses).\n",
    "\n",
    "**Ponto forte:** consistência em tomadas de decisão complexas.\n",
    "\n",
    "**Ponto fraco:** pode ser mais lento e caro que um LLM para tarefas simples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UR5UUSs1vSa9"
   },
   "source": [
    "#### ⛳ Criação de conta no Groq\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZ0vnkOtvX_K"
   },
   "source": [
    "##### ▶ Acessar o site abaixo para criar sua conta\n",
    "\n",
    "**[Groq.com](https://console.groq.com/)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qwSYsld9wklD"
   },
   "source": [
    "##### ▶ Criar uma `API KEY` para a sua conta\n",
    "\n",
    "![Criação API KEY](https://raw.githubusercontent.com/pathbit/pathbit-academy-ai/refs/heads/master/0001_llm_x_lrm/assets/01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hW3KkWLhwmAF"
   },
   "source": [
    "##### ▶ Copiar e salvar a Api Key em um lugar seguro\n",
    "\n",
    "> **Observações**: *Você pode acessar Api Keys criados através deste link: [https://console.groq.com/keys](https://console.groq.com/keys)*\n",
    "\n",
    "![Copiar API KEY](https://raw.githubusercontent.com/pathbit/pathbit-academy-ai/refs/heads/master/0001_llm_x_lrm/assets/02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lBz8FBEFzVrg"
   },
   "source": [
    "##### ▶ Adicionar a Api Key criada nas `secrets` do seu Notebook\n",
    "\n",
    "![Adicionar API KEY no Secrets](https://raw.githubusercontent.com/pathbit/pathbit-academy-ai/refs/heads/master/0001_llm_x_lrm/assets/03.png)\n",
    "\n",
    "\n",
    "![Adicionar API KEY no Secrets](https://raw.githubusercontent.com/pathbit/pathbit-academy-ai/refs/heads/master/0001_llm_x_lrm/assets/04.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RqiwoNKc1GOU"
   },
   "source": [
    "#### ⚙️ Configuração do ambiente\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ▶ Correção automática para Google Colab\n",
    "\n",
    "🚨 **IMPORTANTE:** Se você estiver executando no Google Colab, esta célula corrige automaticamente problemas de compatibilidade do `tqdm`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔧 CORREÇÃO AUTOMÁTICA PARA GOOGLE COLAB\n",
    "# ==========================================\n",
    "# Esta célula resolve automaticamente conflitos de dependências do tqdm\n",
    "\n",
    "# Detectar se estamos no Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"🌐 Detectado: Google Colab\")\n",
    "    print(\"🔧 Aplicando correção para conflito de tqdm...\")\n",
    "    \n",
    "    # CORREÇÃO: Atualizar tqdm para resolver conflitos de dependências\n",
    "    get_ipython().run_line_magic('pip', 'install --upgrade tqdm>=4.67 --force-reinstall --quiet')\n",
    "    print(\"✅ tqdm atualizado com sucesso!\")\n",
    "    print(\"📦 Versão do tqdm corrigida para resolver conflitos com datasets e dataproc-spark-connect\")\n",
    "    \n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"💻 Detectado: Ambiente Local\")\n",
    "    print(\"ℹ️  Correção do tqdm não necessária no ambiente local\")\n",
    "\n",
    "print(\"\\n🎯 Ambiente configurado! Continue com a próxima célula.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MBglR7um29yg"
   },
   "source": [
    "##### ▶ Instalar os pacotes que iremos utilizar neste projeto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ▶ Instalar dependências do projeto\n",
    "\n",
    "📦 **Instalação das bibliotecas necessárias para o projeto.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📦 INSTALAÇÃO DAS DEPENDÊNCIAS\n",
    "# ===============================\n",
    "# Instalação das bibliotecas necessárias para o projeto\n",
    "\n",
    "# Verificar a versão do python\n",
    "import sys\n",
    "import os\n",
    "import site\n",
    "\n",
    "print(\"📊 Informações do ambiente:\")\n",
    "print(\"Caminho do Python:\", sys.executable)\n",
    "print(\"Versão do Python:\", sys.version)\n",
    "print(\"PATH:\", os.environ.get('PATH', 'Não encontrado'))\n",
    "print(\"Sites de pacotes:\", site.getsitepackages())\n",
    "\n",
    "# Instalar a biblioteca do groq\n",
    "print(\"\\n📦 Instalando dependências...\")\n",
    "if IN_COLAB:\n",
    "    get_ipython().run_line_magic('pip', 'install -q groq')\n",
    "else:\n",
    "    import subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"groq\"])\n",
    "\n",
    "print(\"✅ Instalação concluída!\")\n",
    "print(\"🚀 Pronto para usar a API do Groq!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zC78TBpW-9J_",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caminho do Python: /Users/elielsousa/Projects/pathbit/github/pathbit-academy-ai/0001_llm_x_lrm/.venv/bin/python3.12\n",
      "Versão do Python: 3.12.7 (main, Oct 25 2024, 15:25:54) [Clang 16.0.0 (clang-1600.0.26.3)]\n",
      "PATH: /Users/elielsousa/Projects/pathbit/github/pathbit-academy-ai/0001_llm_x_lrm/.venv/bin:/Users/elielsousa/.codeium/windsurf/bin:/opt/homebrew/opt/openjdk@17/bin:/opt/homebrew/opt/ruby/bin:/Users/elielsousa/.gvm/pkgsets/go1.24.3/global/bin:/Users/elielsousa/.gvm/gos/go1.24.3/bin:/Users/elielsousa/.gvm/pkgsets/go1.24.3/global/overlay/bin:/Users/elielsousa/.gvm/bin:/Users/elielsousa/.pyenv/shims:/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/System/Cryptexes/App/usr/bin:/usr/bin:/bin:/usr/sbin:/sbin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/local/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/appleinternal/bin:/Library/Apple/usr/bin:/usr/local/share/dotnet:~/.dotnet/tools:/Users/elielsousa/.codeium/windsurf/bin:/opt/homebrew/opt/openjdk@17/bin:/opt/homebrew/opt/ruby/bin:/Users/elielsousa/.dotnet/tools:/Users/elielsousa/Library/Android/sdk/tools:/Users/elielsousa/Library/Android/sdk/platform-tools:/Users/elielsousa/.cursor/extensions/ms-python.debugpy-2025.10.0-darwin-arm64/bundled/scripts/noConfigScripts:/Users/elielsousa/Library/Android/sdk/tools:/Users/elielsousa/Library/Android/sdk/platform-tools\n",
      "Sites de pacotes: ['/Users/elielsousa/Projects/pathbit/github/pathbit-academy-ai/0001_llm_x_lrm/.venv/lib/python3.12/site-packages']\n"
     ]
    }
   ],
   "source": [
    "# 🔧 Correção automática para Google Colab\n",
    "# ==========================================\n",
    "\n",
    "# Detectar se estamos no Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"🌐 Detectado: Google Colab\")\n",
    "    \n",
    "    # 🔧 CORREÇÃO: Atualizar tqdm para resolver conflitos de dependências\n",
    "    print(\"🔧 Aplicando correção para conflito de tqdm...\")\n",
    "    %pip install --upgrade tqdm>=4.67 --force-reinstall --quiet\n",
    "    print(\"✅ tqdm atualizado com sucesso!\")\n",
    "    \n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"💻 Detectado: Ambiente Local\")\n",
    "\n",
    "# Verificar a versão do python\n",
    "import sys\n",
    "import os\n",
    "import site\n",
    "\n",
    "print(\"\\n📊 Informações do ambiente:\")\n",
    "print(\"Caminho do Python:\", sys.executable)\n",
    "print(\"Versão do Python:\", sys.version)\n",
    "print(\"PATH:\", os.environ.get('PATH', 'Não encontrado'))\n",
    "print(\"Sites de pacotes:\", site.getsitepackages())\n",
    "\n",
    "# Instalar a biblioteca do groq\n",
    "print(\"\\n📦 Instalando dependências...\")\n",
    "%pip install -q groq\n",
    "\n",
    "print(\"✅ Instalação concluída!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WMx9g-Xf3pae"
   },
   "source": [
    "##### ▶ Criar e recuperar a variável de ambiente para utilizar no Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "KRkCX4pC3p3P"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Configuração local carregada\n",
      "✅ GROQ_API_KEY: gsk_P1******\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 🚀 Configuração para execução local e Colab\n",
    "# ============================================\n",
    "\n",
    "# Adicionar o diretório src ao path para importar módulos locais\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Constante com o nome da secret adicionada no Notebook\n",
    "GROQ_API_KEY_NAME = \"GROQ_API_KEY\"\n",
    "\n",
    "# Importar configuração local\n",
    "try:\n",
    "    from config_local import configurar_api_key, exibir_markdown, exibir_resposta_formatada\n",
    "    print(\"✅ Configuração local carregada\")\n",
    "except ImportError:\n",
    "    print(\"⚠️  Executando em modo Colab\")\n",
    "    \n",
    "    # Funções para Colab\n",
    "    def configurar_api_key():\n",
    "        from google.colab import userdata\n",
    "        try:\n",
    "            groq_api_key = userdata.get(GROQ_API_KEY_NAME)\n",
    "            os.environ[GROQ_API_KEY_NAME] = groq_api_key\n",
    "            print(f\"✅ {GROQ_API_KEY_NAME}: {os.environ[GROQ_API_KEY_NAME][:6]}******\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Erro ao configurar API Key: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def exibir_markdown(texto):\n",
    "        from IPython.display import Markdown, display\n",
    "        display(Markdown(texto))\n",
    "    \n",
    "    def exibir_resposta_formatada(modelo, pergunta, resposta, raciocinio=None, tempo=0.0):\n",
    "        from IPython.display import Markdown, display\n",
    "        texto_raciocinio = \"\"\n",
    "        if raciocinio:\n",
    "            texto_raciocinio = f\"\"\"\n",
    "        \n",
    "## 🧐 Raciocínio\n",
    "================================================\n",
    "\n",
    "{raciocinio.strip()}\n",
    "\"\"\"\n",
    "        \n",
    "        texto_md = f\"\"\"\n",
    "## 🧠 Modelo: `{modelo}`\n",
    "**⏱ Tempo de execução:** {tempo:.2f}s{texto_raciocinio}\n",
    "\n",
    "### 📥 Pergunta\n",
    "\n",
    "{pergunta.strip()}\n",
    "\n",
    "### 📤 Resposta\n",
    "\n",
    "{resposta.strip()}\n",
    "        \"\"\"\n",
    "        \n",
    "        exibir_markdown(texto_md)\n",
    "\n",
    "# Configurar API Key\n",
    "configurar_api_key()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lvBbPHNVvd9P"
   },
   "source": [
    "##### ▶ Validar se o Groq está funcionando corretamente\n",
    "\n",
    "⚠️ Este modelo `compound-beta` é da própria `Groq`, tem um resultado excelente até estes momentos dos meus testes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "IKUwA6xYvThe",
    "outputId": "11bc4202-b1e9-496c-b93a-e2a675ca1946"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "## > Prompt do sistema\n",
       "REGRAS:\n",
       "\n",
       "++++\n",
       "\n",
       "Você é um especialista da área de inteligência artificial e está instruindo\n",
       "crianças e adolescentes no período escolar fundamental.\n",
       "\n",
       "  1. Utilize respostas fáceis para o seu público.\n",
       "\n",
       "  2. Utilize exemplos práticos do dia-a-dia desse público.\n",
       "\n",
       "  3. A resposta deve ser formatada no padrão Markdown, seus títulos devem começar\n",
       "  com 3 \"#\", utilizar emojis e ter as quebras adequadas para separar bem cada\n",
       "  parte do texto, facilitando a leitura.\n",
       "\n",
       "  4. Você pode utiliar um pouco de linguagem técnica até para que a pessoa tenha\n",
       "  interesse em continuar pesquisando sobre o tema e seus subtemas.\n",
       "\n",
       "++++\n",
       "\n",
       "## > Pergunta do usuário\n",
       "Qual a diferença entre LLMs e LRMs?\n",
       "\n",
       "## > Prompt final enviado ao LLM\n",
       "REGRAS:\n",
       "\n",
       "++++\n",
       "\n",
       "Você é um especialista da área de inteligência artificial e está instruindo\n",
       "crianças e adolescentes no período escolar fundamental.\n",
       "\n",
       "  1. Utilize respostas fáceis para o seu público.\n",
       "\n",
       "  2. Utilize exemplos práticos do dia-a-dia desse público.\n",
       "\n",
       "  3. A resposta deve ser formatada no padrão Markdown, seus títulos devem começar\n",
       "  com 3 \"#\", utilizar emojis e ter as quebras adequadas para separar bem cada\n",
       "  parte do texto, facilitando a leitura.\n",
       "\n",
       "  4. Você pode utiliar um pouco de linguagem técnica até para que a pessoa tenha\n",
       "  interesse em continuar pesquisando sobre o tema e seus subtemas.\n",
       "\n",
       "++++\n",
       "\n",
       "USUÁRIO:\n",
       "\n",
       "Qual a diferença entre LLMs e LRMs?\n",
       "\n",
       "## > Resposta do LLM\n",
       "### Diferença entre LLMs e LRMs\n",
       "A diferença entre LLMs (Large Language Models) e LRMs (Large Response Models) é que:\n",
       "\n",
       "*   **LLMs** são modelos de linguagem que utilizam uma grande quantidade de dados e capacidade de processamento para aprender padrões e relações na linguagem, enquanto \n",
       "*   **LRMs** não é um termo amplamente utilizado, mas podemos considerar que se refere a modelos que geram respostas rápidas e eficientes.\n",
       "\n",
       "Em resumo, LLMs são mais focados em aprender e representar a linguagem, enquanto LRMs são projetados para gerar respostas rápidas e eficientes. \n",
       "\n",
       "### Exemplos Práticos\n",
       "Um exemplo de LLM é um modelo que pode gerar textos coherentes e naturais, como um artigo de notícias ou um conto. Já um exemplo de LRM seria um modelo que pode responder rapidamente a perguntas frequentes, como um chatbot de suporte ao cliente.\n",
       "\n",
       "### Conclusão\n",
       "Portanto, a diferença entre LLMs e LRMs está no seu objetivo e aplicação. LLMs são mais focados em aprender e representar a linguagem, enquanto LRMs são projetados para gerar respostas rápidas e eficientes. \n",
       "\n",
       "### Próximos Passos\n",
       "Para saber mais sobre LLMs e LRMs, você pode pesquisar sobre as aplicações e limitações de cada um, e explorar como eles são utilizados em diferentes áreas, como processamento de linguagem natural, inteligência artificial e machine learning. Além disso, você pode experimentar desenvolver seu próprio modelo de linguagem simples usando ferramentas como Python e bibliotecas de NLP. \n",
       "\n",
       "Lembre-se de que a aprendizagem é um processo contínuo, e há sempre mais a aprender sobre esses fascinantes tópicos! 📚💻\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Validar se o Groq está funcionando corretamente\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# Importar os módulos\n",
    "import os\n",
    "from groq import Groq\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Configurar a API Key da Groq\n",
    "# (recomendo guardar em segredos do Colab)\n",
    "groq_api_key = os.environ[GROQ_API_KEY_NAME]\n",
    "client = Groq(api_key=groq_api_key)\n",
    "\n",
    "# Definir o modelo de LLM que iremos utilizar\n",
    "# Modelos: https://console.groq.com/docs/models\n",
    "LLM_MODEL = \"compound-beta\"\n",
    "\n",
    "# Criar a prompt do sistema\n",
    "prompt_sistema = \"\"\"\n",
    "REGRAS:\n",
    "\n",
    "++++\n",
    "\n",
    "Você é um especialista da área de inteligência artificial e está instruindo\n",
    "crianças e adolescentes no período escolar fundamental.\n",
    "\n",
    "  1. Utilize respostas fáceis para o seu público.\n",
    "\n",
    "  2. Utilize exemplos práticos do dia-a-dia desse público.\n",
    "\n",
    "  3. A resposta deve ser formatada no padrão Markdown, seus títulos devem começar\n",
    "  com 3 \"#\", utilizar emojis e ter as quebras adequadas para separar bem cada\n",
    "  parte do texto, facilitando a leitura.\n",
    "\n",
    "  4. Você pode utiliar um pouco de linguagem técnica até para que a pessoa tenha\n",
    "  interesse em continuar pesquisando sobre o tema e seus subtemas.\n",
    "\n",
    "++++\n",
    "\n",
    "\"\"\".strip()\n",
    "\n",
    "# Pergunta do usuário\n",
    "pergunta_usuario = \"Qual a diferença entre LLMs e LRMs?\"\n",
    "\n",
    "# Criar o prompt final concatenado (para exibir ou logar)\n",
    "prompt_final = f\"{prompt_sistema}\\n\\nUSUÁRIO:\\n\\n{pergunta_usuario}\"\n",
    "\n",
    "# Montar mensagens no formato da Groq e chamar o modelo\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": prompt_sistema},\n",
    "    {\"role\": \"user\", \"content\": pergunta_usuario},\n",
    "]\n",
    "\n",
    "resp = client.chat.completions.create(\n",
    "    model=LLM_MODEL,\n",
    "    messages=messages,\n",
    "    temperature=0.3,\n",
    "    max_completion_tokens=1024,\n",
    ")\n",
    "\n",
    "resposta_modelo = resp.choices[0].message.content\n",
    "\n",
    "# Visualizando o resultado formatado em Markdown\n",
    "display(Markdown(f\"\"\"\n",
    "## > Prompt do sistema\n",
    "{prompt_sistema}\n",
    "\n",
    "## > Pergunta do usuário\n",
    "{pergunta_usuario}\n",
    "\n",
    "## > Prompt final enviado ao LLM\n",
    "{prompt_final}\n",
    "\n",
    "## > Resposta do LLM\n",
    "{resposta_modelo}\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OAOrEU7q7F1J"
   },
   "source": [
    "### </> Criando as funções para utilizar modelos de LLM e LRM\n",
    "---\n",
    "\n",
    "**ℹ️️ Observações:**\n",
    "\n",
    "*Tentei fazer a melhor documentação possível para explicar o código.*\n",
    "\n",
    "**⚠️ Importante:**\n",
    "\n",
    "*Para garantir os melhores resultados, o código utiliza dois modelos de linguagem diferentes, cada um com uma finalidade específica:*\n",
    "\n",
    "- **`Llama3-70B-8192:`** Um modelo mais genérico, com um foco em compreensão de linguagem natural e tarefas de conversação. Ele é ideal para interpretar o contexto do texto e gerar respostas fluentes e coerentes, garantindo que a comunicação seja clara e natural. O `llama3-70b-8192` está focado gerar texto de forma fluída e natural, como uma conversa, mas não em resolver problemas lógicos ou matemáticos complexos passo a passo como o outro modelo.\n",
    "\n",
    "- **`DeepSeek R1 Distill Llama 70B:`** Este modelo é especializado em tarefas que exigem um raciocínio complexo, como lógica, matemática e programação. Sua arquitetura é otimizada para resolver problemas estruturados de forma eficiente. O `deepseek-r1-distill-llama-70b` é um exemplo, focado em \"pensar\" passo a passo antes de chegar a uma resposta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d95kpV6QA3oc"
   },
   "source": [
    "> **FUNÇÕES BÁSICAS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qd0IVDMn-MaJ"
   },
   "source": [
    "\n",
    "**1. Função para recuperar o cliente do Groq**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "2syPSkoZ8Tgg"
   },
   "outputs": [],
   "source": [
    "# Definições das funções auxiliares\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# Importar os módulos\n",
    "import os\n",
    "import time\n",
    "from groq import Groq\n",
    "from typing import Tuple, Optional\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "\n",
    "def criar_cliente_groq() -> Groq:\n",
    "    \"\"\"\n",
    "    Cria o cliente da Groq usando a variável de ambiente GROQ_API_KEY.\n",
    "\n",
    "    Por que usar env var?\n",
    "    - Segurança: evita hardcode de chaves no notebook.\n",
    "    - Reprodutibilidade: o mesmo código funciona em diferentes ambientes.\n",
    "    \"\"\"\n",
    "    groq_api_key = os.environ[GROQ_API_KEY_NAME]\n",
    "    if not groq_api_key:\n",
    "        raise RuntimeError(\n",
    "            \"GROQ_API_KEY não definida. No Colab, use: os.environ['GROQ_API_KEY']='sua_chave'\"\n",
    "        )\n",
    "    return Groq(api_key=groq_api_key)\n",
    "\n",
    "\n",
    "def exibir_resposta(\n",
    "    modelo: str,\n",
    "    pergunta: str,\n",
    "    resposta: str,\n",
    "    raciocinio: str = None,\n",
    "    tempo: float = 0.0\n",
    "  ):\n",
    "    \"\"\"\n",
    "    Exibe saída formatada em Markdown no Jupyter/Colab.\n",
    "    - modelo     : nome do modelo utilizado (LLM ou LRM)\n",
    "    - pergunta   : texto enviado\n",
    "    - resposta   : resposta final ao usuário\n",
    "    - raciocinio : cadeia de raciocínio (opcional)\n",
    "    - tempo      : tempo total da execução\n",
    "    \"\"\"\n",
    "    texto_raciocinio = \"\"\n",
    "\n",
    "    if raciocinio:\n",
    "        texto_raciocinio += f\"\\n\\n\"\n",
    "        texto_raciocinio += f\"## 🧐 Raciocínio                                \\n\"\n",
    "        texto_raciocinio += f\"================================================\\n\\n\"\n",
    "        texto_raciocinio += f\"{raciocinio.strip()}\"\n",
    "\n",
    "    texto_md = f\"\"\"\n",
    "## 🧠 Modelo: `{modelo}`\n",
    "**⏱ Tempo de execução:** {tempo:.2f}s\n",
    "\n",
    "{texto_raciocinio}\n",
    "\n",
    "### 📥 Pergunta\n",
    "\n",
    "{pergunta.strip()}\n",
    "\n",
    "### 📤 Resposta\n",
    "\n",
    "{resposta.strip()}\n",
    "    \"\"\"\n",
    "\n",
    "    display(Markdown(texto_md))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HcQRCQ4yAljB"
   },
   "source": [
    "> **LLM: FUNÇÕES E TESTES**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UMaq_7lW-r2R"
   },
   "source": [
    "**2. LLM: Função para responder usando modelo llama3 do Groq**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "VPoE6SHo-qCJ"
   },
   "outputs": [],
   "source": [
    "# Definição da função para responder a pergunta usando LLM\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# Importar os módulos\n",
    "import os\n",
    "import time\n",
    "from typing import Tuple, Optional\n",
    "from groq import Groq\n",
    "\n",
    "# ==============================\n",
    "# Função: perguntar_llm\n",
    "# Objetivo: enviar pergunta a um modelo de LINGUAGEM (LLM)\n",
    "# Retorna: (texto_resposta, tempo_em_segundos)\n",
    "# ==============================\n",
    "def perguntar_llm(\n",
    "    pergunta: str,\n",
    "    modelo: str = \"llama-3.3-70b-versatile\", # Modelo padrão para linguagem natural\n",
    "    temperatura: float = 0.2,                # Controla a aleatoriedade da resposta\n",
    "    max_tokens_resposta: int = 1024,         # Limita tamanho da resposta (custo/latência)\n",
    "    sistema: Optional[str] = None            # Regras ou persona opcionais\n",
    ") -> Tuple[str, float]:\n",
    "    \"\"\"\n",
    "    Envia uma pergunta para um modelo LLM.\n",
    "\n",
    "    Parâmetros:\n",
    "    - pergunta           : texto enviado ao modelo.\n",
    "    - modelo             : modelo LLM para tarefas gerais (resumo, reescrita, Q&A).\n",
    "    - temperatura        : controla a \"criatividade\" (0.1–0.3 = mais determinístico).\n",
    "    - max_tokens_resposta: limita tamanho da saída, evitando custo ou lentidão.\n",
    "    - sistema            : mensagem opcional para regras ou persona.\n",
    "\n",
    "    Retorna:\n",
    "    - texto: resposta final do modelo.\n",
    "    - tempo: tempo total da chamada em segundos.\n",
    "    \"\"\"\n",
    "    # cria o cliente autenticado\n",
    "    cliente = criar_cliente_groq()\n",
    "\n",
    "    # monta mensagens no formato da API (sistema opcional + usuário obrigatório)\n",
    "    mensagens = []\n",
    "    if sistema:\n",
    "        mensagens.append({\"role\": \"system\", \"content\": sistema})\n",
    "\n",
    "    mensagens.append({\"role\": \"user\", \"content\": pergunta})\n",
    "\n",
    "    # marca início para medir tempo de execução\n",
    "    inicio = time.perf_counter()\n",
    "\n",
    "    # chamada à API da Groq\n",
    "    resposta = cliente.chat.completions.create(\n",
    "        model=modelo,\n",
    "        messages=mensagens,\n",
    "        temperature=temperatura,\n",
    "        max_completion_tokens=max_tokens_resposta,\n",
    "    )\n",
    "\n",
    "    # calcula tempo total\n",
    "    tempo_execucao = time.perf_counter() - inicio\n",
    "\n",
    "    # extrai conteúdo textual da resposta\n",
    "    texto_resposta = resposta.choices[0].message.content.strip()\n",
    "\n",
    "    return modelo, texto_resposta, tempo_execucao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ijpWGrbX_bX7"
   },
   "source": [
    "**3. LLM: Testando a função para responder com LLM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "id": "XxprzeEJ_bJL",
    "outputId": "be120c51-8c26-424b-916d-e74150d1123a"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "## 🧠 Modelo: `llama-3.3-70b-versatile`\n",
       "**⏱ Tempo de execução:** 0.39s\n",
       "\n",
       "\n",
       "\n",
       "### 📥 Pergunta\n",
       "\n",
       "Resuma em 2 frases, com linguagem neutra e direta: \n",
       "O Banco Central manteve a taxa Selic inalterada. Analistas projetam estabilidade\n",
       "nos próximos meses, com atenção à inflação de serviços e ao mercado de trabalho.\n",
       "\n",
       "### 📤 Resposta\n",
       "\n",
       "O Banco Central decidiu manter a taxa Selic sem alterações. Analistas preveem estabilidade nos próximos meses, com foco na inflação de serviços e no desempenho do mercado de trabalho.\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "## 🧠 Modelo: `llama-3.3-70b-versatile`\n",
       "**⏱ Tempo de execução:** 0.51s\n",
       "\n",
       "\n",
       "\n",
       "### 📥 Pergunta\n",
       "\n",
       "Reescreva o resumo abaixo com tom mais informal e amigável, mantendo precisão.\n",
       "\n",
       "RESUMO ORIGINAL:\n",
       "O Banco Central decidiu manter a taxa Selic sem alterações. Analistas preveem estabilidade nos próximos meses, com foco na inflação de serviços e no desempenho do mercado de trabalho.\n",
       "\n",
       "### 📤 Resposta\n",
       "\n",
       "## Resumo Reescrito\n",
       "\n",
       "O Banco Central resolveu não mexer na taxa Selic. Os especialistas acham que as coisas vão continuar mais ou menos como estão nos próximos meses. Eles estão de olho na inflação, especialmente nos serviços, e também no que acontece com o mercado de trabalho.\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Teste da função para responder a pergunta usando LLM\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# Importar os módulos\n",
    "import os\n",
    "import time\n",
    "from typing import Tuple, Optional\n",
    "from groq import Groq\n",
    "\n",
    "# LLM: resumo e reescrita de tom (usando perguntar_llm)\n",
    "texto = \"\"\"\n",
    "O Banco Central manteve a taxa Selic inalterada. Analistas projetam estabilidade\n",
    "nos próximos meses, com atenção à inflação de serviços e ao mercado de trabalho.\n",
    "\"\"\"\n",
    "\n",
    "# Pedimos um RESUMO curto (linguagem neutra) — tarefa típica de LLM\n",
    "prompt_resumo = f\"Resuma em 2 frases, com linguagem neutra e direta: {texto}\"\n",
    "modelo_resumo, resposta_resumo, tempo_resumo = perguntar_llm(\n",
    "    pergunta=prompt_resumo,\n",
    "    sistema=\"Responda em Markdown de forma clara e concisa.\"\n",
    ")\n",
    "\n",
    "exibir_resposta(\n",
    "    modelo=modelo_resumo,\n",
    "    pergunta=prompt_resumo,\n",
    "    resposta=resposta_resumo,\n",
    "    tempo=tempo_resumo\n",
    ")\n",
    "\n",
    "# Agora pedimos para REESCREVER o próprio resumo com outro tom.\n",
    "#   - IMPORTANTE: incluímos explicitamente o 'resumo' no prompt (o modelo não “lembra” sozinho).\n",
    "prompt_tom = (\n",
    "    \"Reescreva o resumo abaixo com tom mais informal e amigável, mantendo precisão.\\n\\n\"\n",
    "    f\"RESUMO ORIGINAL:\\n{resposta_resumo}\"\n",
    ")\n",
    "modelo_tom, resposta_tom, tempo_tom = perguntar_llm(\n",
    "    pergunta=prompt_tom,\n",
    "    sistema=\"Responda em Markdown e use linguagem acessível; evite jargões.\"\n",
    ")\n",
    "\n",
    "exibir_resposta(\n",
    "    modelo=modelo_tom,\n",
    "    pergunta=prompt_tom,\n",
    "    resposta=resposta_tom,\n",
    "    tempo=tempo_tom\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cJs63UUhAwja"
   },
   "source": [
    "> **LRM: FUNÇÕES E TESTES**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gYh2luuPAzrJ"
   },
   "source": [
    "**4. LRM: Função para responder usando modelo deepseek do Groq**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "pqYmYrwP-RoL"
   },
   "outputs": [],
   "source": [
    "# Definição da função para responder a pergunta usando LRM\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# Importar os módulos\n",
    "import os\n",
    "import time\n",
    "from typing import Tuple, Optional\n",
    "from groq import Groq\n",
    "\n",
    "# ==============================\n",
    "# Função: perguntar_lrm\n",
    "# Objetivo: enviar pergunta a um modelo de RACIOCÍNIO (LRM)\n",
    "# Retorna: (texto_ao_usuario, cadeia_de_raciocinio, tempo_em_segundos)\n",
    "# ==============================\n",
    "def perguntar_lrm(\n",
    "    pergunta: str,\n",
    "    modelo: str = \"deepseek-r1-distill-llama-70b\",   # Modelo otimizado para raciocínio\n",
    "    temperatura: float = 0.2,                        # Baixa variação para consistência\n",
    "    formato_raciocinio: Optional[str] = \"parsed\",    # 'parsed', 'raw', 'hidden' ou None\n",
    "    incluir_raciocinio: bool = True,                 # Só usado se formato_raciocinio=None\n",
    "    max_tokens_resposta: int = 2000,                 # Limite da resposta final\n",
    "    max_tokens_raciocinio: Optional[int] = None,     # Limite da cadeia de raciocínio\n",
    "    sistema: Optional[str] = None                    # Mensagem de sistema opcional\n",
    ") -> Tuple[str, Optional[str], float]:\n",
    "    \"\"\"\n",
    "    Envia uma pergunta a um modelo de raciocínio (LRM) da Groq.\n",
    "\n",
    "    Comportamento:\n",
    "    - Se `formato_raciocinio` ∈ {'parsed','raw','hidden'} → usa `reasoning_format` e\n",
    "      **NÃO** envia `include_reasoning` (evita erro 400).\n",
    "      • 'parsed'  → cadeia vem separada em `message.reasoning`\n",
    "      • 'raw'     → cadeia vem crua, com marcações\n",
    "      • 'hidden'  → o modelo raciocina, mas não retorna a cadeia\n",
    "    - Se `formato_raciocinio` for None → não envia `reasoning_format` e usa `include_reasoning`.\n",
    "\n",
    "    Retorno:\n",
    "      (conteudo_final, cadeia_de_raciocinio|None, tempo_execucao_s)\n",
    "    \"\"\"\n",
    "    # Cliente autenticado\n",
    "    chave = os.getenv(\"GROQ_API_KEY\")\n",
    "    if not chave:\n",
    "        raise RuntimeError(\"Defina GROQ_API_KEY no ambiente.\")\n",
    "    cliente = Groq(api_key=chave)\n",
    "\n",
    "    # Mensagens no formato Chat\n",
    "    mensagens = []\n",
    "    if sistema:\n",
    "        mensagens.append({\"role\": \"system\", \"content\": sistema})\n",
    "    mensagens.append({\"role\": \"user\", \"content\": pergunta})\n",
    "\n",
    "    # Monta kwargs dinâmicos conforme as regras da API\n",
    "    kwargs = {\n",
    "        \"model\": modelo,\n",
    "        \"messages\": mensagens,\n",
    "        \"temperature\": temperatura,\n",
    "        \"max_completion_tokens\": max_tokens_resposta,\n",
    "    }\n",
    "\n",
    "    # Se limite de tokens do raciocínio foi definido, adiciona\n",
    "    if max_tokens_raciocinio is not None:\n",
    "        kwargs[\"max_reasoning_tokens\"] = max_tokens_raciocinio\n",
    "\n",
    "    # Regra de exclusão mútua:\n",
    "    # - Se formato_raciocinio estiver definido, usa reasoning_format e IGNORA include_reasoning\n",
    "    # - Se formato_raciocinio for None, usa include_reasoning (sem reasoning_format)\n",
    "    if formato_raciocinio is not None:\n",
    "        kwargs[\"reasoning_format\"] = formato_raciocinio\n",
    "        # NÃO adicionar include_reasoning para evitar o erro 400\n",
    "    else:\n",
    "        kwargs[\"include_reasoning\"] = bool(incluir_raciocinio)\n",
    "\n",
    "    # Chamada e cronômetro\n",
    "    inicio = time.perf_counter()\n",
    "    resp = cliente.chat.completions.create(**kwargs)\n",
    "    tempo = time.perf_counter() - inicio\n",
    "\n",
    "    # Extração da resposta e (se houver) do raciocínio\n",
    "    msg = resp.choices[0].message\n",
    "    conteudo = (msg.content or \"\").strip()\n",
    "\n",
    "    # A cadeia só vem quando:\n",
    "    # - usamos reasoning_format ('parsed'/'raw'), E\n",
    "    # - não é 'hidden'\n",
    "    if formato_raciocinio in (\"parsed\", \"raw\"):\n",
    "        raciocinio = getattr(msg, \"reasoning\", None)\n",
    "    else:\n",
    "        # Quando formato=None e include_reasoning=True, algumas combinações\n",
    "        # podem retornar o raciocínio embutido; a API nem sempre expõe em `reasoning`.\n",
    "        raciocinio = getattr(msg, \"reasoning\", None)\n",
    "\n",
    "    return modelo, conteudo, raciocinio, tempo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9eS76H1EDeJD"
   },
   "source": [
    "**5. LRM: Testando a função para responder com LRM**\n",
    "\n",
    "-\n",
    "\n",
    "**ℹ️️ IMPORTANTE:**\n",
    "\n",
    "> *Para ver como o modelo chegou à resposta final, procure a seção **🧐 Raciocínio.** Essa é uma demonstração do poder dos modelos especializados em lógica e raciocínio.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "3ejlx82kDZ2P",
    "outputId": "3cc94834-8e71-4ceb-ccd0-a2c2d0279ecc"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "## 🧠 Modelo: `deepseek-r1-distill-llama-70b`\n",
       "**⏱ Tempo de execução:** 5.09s\n",
       "\n",
       "\n",
       "\n",
       "## 🧐 Raciocínio                                \n",
       "================================================\n",
       "\n",
       "Ok, vamos lá. Primeiro, preciso entender o perfil do investidor. Ele é conservador, então provavelmente busca estabilidade e baixo risco. Seu objetivo é ter uma renda previsível com baixa volatilidade. Isso me faz pensar que ele não quer surpresas e valoriza a segurança do capital.\n",
       "\n",
       "Agora, vamos analisar as opções disponíveis: título atrelado ao CDI, prefixado curto e IPCA+ longo. Cada um tem suas características.\n",
       "\n",
       "Primeiro, o título atrelado ao CDI. Como o CDI está em 13,25% ao ano, isso oferece uma rentabilidade líquida de aproximadamente 13,25% menos o imposto de renda. Considerando a alíquota de 15%, a rentabilidade líquida seria de 11,25%. Isso é bom, mas preciso considerar a inflação projetada de 4%. Então, a rentabilidade real seria de 7,25%, o que é positivo. No entanto, o risco de crédito pode ser um problema se o emitente não for confiável. Além disso, a liquidez pode ser um desafio, especialmente se precisar resgatar o dinheiro antes do vencimento.\n",
       "\n",
       "Em seguida, o prefixado curto. Com vencimento em 2026 e 2027, ele oferece uma rentabilidade menor, mas mais segurança. A rentabilidade líquida seria de 11,25% menos a inflação de 4%, resultando em 7,25% real. A vantagem aqui é a menor exposição à volatilidade das taxas de juros, já que o prazo é curto. A liquidez também é melhor, pois títulos curtos costumam ter mais liquidez no mercado secundário. Além disso, o risco de crédito é menor, já que são títulos do Tesouro Direto, considerados de baixo risco.\n",
       "\n",
       "Por fim, o IPCA+ longo. Com vencimento mais longo, ele oferece uma rentabilidade real de 4% mais a inflação. Isso pode parecer atraente, mas o risco é maior devido à exposição à inflação. Se a inflação ultrapassar a projetada, o investidor pode perder poder de compra. Além disso, a liquidez pode ser um problema devido ao longo prazo, e a volatilidade maior pode afetar o valor do título.\n",
       "\n",
       "Considerando o perfil conservador do investidor, o prefixado curto parece mais adequado. Ele oferece uma rentabilidade decente com menor risco e melhor liquidez. Além disso, o duration menor protege contra variações nas taxas de juros, o que é importante para manter a estabilidade.\n",
       "\n",
       "Para a política de rebalanceamento, sugiro revisar a carteira anualmente e realocar recursos para manter a distribuição entre os ativos, garantindo que o perfil de risco permaneça alinhado com o objetivo do investidor.\n",
       "\n",
       "Em resumo, o prefixado curto atende melhor às necessidades do investidor conservador, oferecendo renda previsível com baixa volatilidade e riscos controlados.\n",
       "\n",
       "### 📥 Pergunta\n",
       "\n",
       "Você é um analista de investimentos.\n",
       "\n",
       "Dado o cenário:\n",
       "- {\"perfil\": \"conservador\", \"inflacao_projetada_aa\": 4.0, \"cdi_aa\": 13.25, \"vencimentos_anos\": [2026, 2027], \"objetivo\": \"renda previsível com baixa volatilidade\"}\n",
       "\n",
       "Decida entre:\n",
       "- título atrelado ao CDI;\n",
       "- prefixado curto;\n",
       "- IPCA+ longo.\n",
       "\n",
       "Importante:\n",
       "\n",
       "1. Resposta e raciocínio sempre em portugês do brasil.\n",
       "2. Estruture o raciocínio em etapas (riscos, liquidez, duration, sensibilidade à inflação).\n",
       "3. Apresente a recomendação final e uma política de rebalanceamento simples.\n",
       "4. Use números aproximados e justificativas claras.\n",
       "\n",
       "### 📤 Resposta\n",
       "\n",
       "### Análise e Recomendação para Investimento\n",
       "\n",
       "#### 1. **Perfil do Investidor e Objetivo**\n",
       "   - **Perfil:** Conservador\n",
       "   - **Objetivo:** Renda previsível com baixa volatilidade\n",
       "\n",
       "#### 2. **Etapas de Análise**\n",
       "\n",
       "   ##### 2.1. **Riscos**\n",
       "   - **Título atrelado ao CDI:** \n",
       "     - Risco de crédito do emitente.\n",
       "     - Sensibilidade a mudanças nas taxas de juros.\n",
       "   - **Prefixado Curto:** \n",
       "     - Baixo risco de crédito (normalmente emitidos por instituições seguras).\n",
       "     - Menor exposição a mudanças nas taxas de juros devido ao prazo curto.\n",
       "   - **IPCA+ Longo:** \n",
       "     - Risco de inflação superior à projetada.\n",
       "     - Maior exposição a variações nas taxas de juros.\n",
       "\n",
       "   ##### 2.2. **Liquidez**\n",
       "   - **Título atrelado ao CDI:** \n",
       "     - Liquidez moderada, dependendo do mercado secundário.\n",
       "   - **Prefixado Curto:** \n",
       "     - Maior liquidez devido ao prazo curto e baixo risco.\n",
       "   - **IPCA+ Longo:** \n",
       "     - Menor liquidez devido ao prazo mais longo.\n",
       "\n",
       "   ##### 2.3. **Duration**\n",
       "   - **Título atrelado ao CDI:** \n",
       "     - Duration mais longo, mais sensível a mudanças nas taxas de juros.\n",
       "   - **Prefixado Curto:** \n",
       "     - Duration menor, menos sensível a mudanças nas taxas de juros.\n",
       "   - **IPCA+ Longo:** \n",
       "     - Duration mais longo, mais sensível a mudanças nas taxas de juros.\n",
       "\n",
       "   ##### 2.4. **Sensibilidade à Inflação**\n",
       "   - **Título atrelado ao CDI:** \n",
       "     - Proteção contra inflação via rentabilidade atrelada ao CDI.\n",
       "   - **Prefixado Curto:** \n",
       "     - Menor proteção contra inflação devido ao prazo curto.\n",
       "   - **IPCA+ Longo:** \n",
       "     - Proteção explícita contra inflação, mas com risco de inflação superior à projetada.\n",
       "\n",
       "#### 3. **Recomendação Final**\n",
       "   - **Recomendação:** **Prefixado Curto**\n",
       "     - **Justificativa:** \n",
       "       - Oferece uma rentabilidade líquida de aproximadamente 11,25% ao ano (considerando imposto de renda de 15%).\n",
       "       - Menor exposição a riscos de crédito e mudanças nas taxas de juros.\n",
       "       - Maior liquidez e menor duration, adequados ao perfil conservador do investidor.\n",
       "\n",
       "#### 4. **Política de Rebalanceamento**\n",
       "   - **Revisão Anual:** \n",
       "     - Verificar a distribuição dos investimentos e realocar recursos para manter o perfil de risco desejado.\n",
       "     - Ajustar a carteira para manter a exposição adequada aos diferentes tipos de títulos.\n",
       "\n",
       "### Conclusão\n",
       "O **Prefixado Curto** é a opção mais adequada para o investidor conservador, oferecendo uma renda previsível com baixa volatilidade e riscos controlados. A política de rebalanceamento simples ajudará a manter a carteira alinhada com os objetivos do investidor.\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Testando a função para responder a pergunta usando LRM\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# Importar os módulos\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from typing import Tuple, Optional\n",
    "from groq import Groq\n",
    "\n",
    "cenario = {\n",
    "    \"perfil\": \"conservador\",\n",
    "    \"inflacao_projetada_aa\": 4.0,\n",
    "    \"cdi_aa\": 13.25,\n",
    "    \"vencimentos_anos\": [2026, 2027],\n",
    "    \"objetivo\": \"renda previsível com baixa volatilidade\",\n",
    "}\n",
    "cenario_fmt = json.dumps(cenario, ensure_ascii=False)\n",
    "\n",
    "prompt_lrm = f\"\"\"\n",
    "Você é um analista de investimentos.\n",
    "\n",
    "Dado o cenário:\n",
    "- {cenario_fmt}\n",
    "\n",
    "Decida entre:\n",
    "- título atrelado ao CDI;\n",
    "- prefixado curto;\n",
    "- IPCA+ longo.\n",
    "\n",
    "Importante:\n",
    "\n",
    "1. Resposta e raciocínio sempre em portugês do brasil.\n",
    "2. Estruture o raciocínio em etapas (riscos, liquidez, duration, sensibilidade à inflação).\n",
    "3. Apresente a recomendação final e uma política de rebalanceamento simples.\n",
    "4. Use números aproximados e justificativas claras.\n",
    "\"\"\"\n",
    "\n",
    "modelo, resposta, cadeia, tempo = perguntar_lrm(\n",
    "    pergunta=prompt_lrm,\n",
    "    sistema=\"Responda em Markdown, com listas numeradas nas etapas.\"\n",
    ")\n",
    "\n",
    "exibir_resposta(\n",
    "    modelo=modelo,\n",
    "    pergunta=prompt_lrm,\n",
    "    resposta=resposta,\n",
    "    raciocinio=cadeia,\n",
    "    tempo=tempo\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bIbj9yHPDx5l"
   },
   "source": [
    "### Comparando ֎ LLM vs LRM 🧠\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8hig7SEawnuX"
   },
   "source": [
    "> *Agora, utilizando o problema anterior, vamsos comparar a diferença entre os modelos com base em um mesmo contexto e/ou problema.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ARtpn5Fc2VgT"
   },
   "source": [
    "#### CENÁRIO 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "EFQKdnzCELm6",
    "outputId": "c882f408-d837-45e1-e5cd-86f0d6c5620a"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## 🔬 Comparação Lado a Lado — LLM vs LRM (MESMO prompt)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### 📝 Prompt usado nos dois modelos\n",
       "> \n",
       "Você é um analista de investimentos.\n",
       "\n",
       "Dado o cenário:\n",
       "- {\"perfil\": \"conservador\", \"inflacao_projetada_aa\": 5.0, \"cdi_aa\": 14.5, \"vencimentos_anos\": [2026, 2027], \"objetivo\": \"renda previsível com baixa volatilidade\"}\n",
       "\n",
       "Decida entre:\n",
       "- título atrelado ao CDI;\n",
       "- prefixado curto;\n",
       "- IPCA+ longo.\n",
       "\n",
       "Importante:\n",
       "\n",
       "1. Resposta e raciocínio sempre em portugês do brasil.\n",
       "2. Estruture o raciocínio em etapas (riscos, liquidez, duration, sensibilidade à inflação).\n",
       "3. Apresente a recomendação final e uma política de rebalanceamento simples.\n",
       "4. Use números aproximados e justificativas claras.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "## 🧠 Modelo: `llama-3.3-70b-versatile`\n",
       "**⏱ Tempo de execução:** 3.35s\n",
       "\n",
       "\n",
       "\n",
       "### 📥 Pergunta\n",
       "\n",
       "Você é um analista de investimentos.\n",
       "\n",
       "Dado o cenário:\n",
       "- {\"perfil\": \"conservador\", \"inflacao_projetada_aa\": 5.0, \"cdi_aa\": 14.5, \"vencimentos_anos\": [2026, 2027], \"objetivo\": \"renda previsível com baixa volatilidade\"}\n",
       "\n",
       "Decida entre:\n",
       "- título atrelado ao CDI;\n",
       "- prefixado curto;\n",
       "- IPCA+ longo.\n",
       "\n",
       "Importante:\n",
       "\n",
       "1. Resposta e raciocínio sempre em portugês do brasil.\n",
       "2. Estruture o raciocínio em etapas (riscos, liquidez, duration, sensibilidade à inflação).\n",
       "3. Apresente a recomendação final e uma política de rebalanceamento simples.\n",
       "4. Use números aproximados e justificativas claras.\n",
       "\n",
       "### 📤 Resposta\n",
       "\n",
       "### Análise de Investimento para Perfil Conservador\n",
       "\n",
       "#### Etapa 1: Riscos\n",
       "O perfil do investidor é conservador, o que significa que ele busca minimizar riscos e prioriza a estabilidade dos investimentos. Nesse contexto, é importante considerar os riscos associados a cada opção de investimento:\n",
       "- **Título atrelado ao CDI**: O risco é relativamente baixo, pois o retorno é indexado à taxa de juros do CDI, que é uma referência para a taxa de juros no mercado financeiro brasileiro. No entanto, existe um risco de crédito associado ao emissor do título.\n",
       "- **Prefixado curto**: O risco também é baixo, pois o investimento tem um vencimento curto, reduzindo o impacto de mudanças nas taxas de juros. Além disso, o risco de crédito é menor devido ao curto prazo.\n",
       "- **IPCA+ longo**: Este investimento apresenta um risco maior devido ao seu longo prazo, o que o torna mais sensível a mudanças nas expectativas de inflação e nas taxas de juros. Além disso, o risco de crédito pode ser maior se o emissor não for de alta qualidade.\n",
       "\n",
       "#### Etapa 2: Liquidez\n",
       "A liquidez é importante para investidores que precisam acessar seus recursos rapidamente. Considerando as opções:\n",
       "- **Título atrelado ao CDI**: A liquidez pode variar dependendo do título específico, mas geralmente é possível resgatar o investimento antes do vencimento, embora possa haver penalidades.\n",
       "- **Prefixado curto**: Devido ao seu curto prazo, o prefixado curto oferece uma boa liquidez, pois o investidor pode esperar pelo vencimento, que ocorre em um prazo relativamente curto.\n",
       "- **IPCA+ longo**: A liquidez é menor devido ao longo prazo do investimento, o que pode dificultar a recuperação dos recursos antes do vencimento sem incorrer em penalidades ou perdas.\n",
       "\n",
       "#### Etapa 3: Duration\n",
       "A duration é uma medida de como o valor de um investimento é afetado por mudanças nas taxas de juros. Para um investidor conservador:\n",
       "- **Título atrelado ao CDI**: A duration é baixa, pois o retorno é indexado à taxa de juros do CDI, o que ajuda a manter o valor do investimento estável em face de mudanças nas taxas de juros.\n",
       "- **Prefixado curto**: A duration também é baixa, pois o investimento tem um vencimento curto, minimizando o impacto de mudanças nas taxas de juros.\n",
       "- **IPCA+ longo**: A duration é alta, o que significa que o investimento é mais sensível a mudanças nas taxas de juros, podendo resultar em perdas se as taxas subirem significativamente.\n",
       "\n",
       "#### Etapa 4: Sensibilidade à Inflação\n",
       "Considerando a inflação projetada de 5.0% ao ano:\n",
       "- **Título atrelado ao CDI**: Não oferece proteção direta contra a inflação, pois o retorno é baseado na taxa de juros do CDI, que não é necessariamente indexada à inflação.\n",
       "- **Prefixado curto**: Também não oferece proteção direta contra a inflação, pois o retorno é fixo e não acompanha a inflação.\n",
       "- **IPCA+ longo**: Oferece proteção contra a inflação, pois o retorno é indexado ao IPCA (Índice de Preços ao Consumidor Amplo), o que ajuda a manter o poder de compra do investimento ao longo do tempo.\n",
       "\n",
       "### Recomendação Final\n",
       "Considerando o perfil conservador do investidor, a necessidade de renda previsível com baixa volatilidade e os vencimentos projetados para 2026 e 2027, a recomendação é investir em **título atrelado ao CDI**. Este investimento oferece um retorno estável, indexado à taxa de juros do CDI, o que ajuda a manter a renda previsível. Além disso, o risco de crédito e a sensibilidade a mudanças nas taxas de juros são relativamente baixos, o que se alinha com o perfil de risco do investidor.\n",
       "\n",
       "### Política de Rebalanceamento\n",
       "Para manter a carteira alinhada com o perfil de risco e os objetivos do investidor, é recomendável revisar e rebalancear a carteira a cada 6 meses. Isso envolve:\n",
       "- Verificar se o perfil de risco do investidor mudou.\n",
       "- Avaliar o desempenho dos investimentos e ajustar a alocação de ativos se necessário.\n",
       "- Considerar a reinvestimento de dividendos ou juros para manter a carteira crescendo de acordo com os objetivos do investidor.\n",
       "\n",
       "Essa abordagem ajudará a manter a carteira diversificada, minimizar riscos e maximizar os retornos dentro do perfil de risco aceitável para o investidor.\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "## 🧠 Modelo: `deepseek-r1-distill-llama-70b`\n",
       "**⏱ Tempo de execução:** 6.21s\n",
       "\n",
       "\n",
       "\n",
       "## 🧐 Raciocínio                                \n",
       "================================================\n",
       "\n",
       "Ok, vamos analisar o cenário apresentado. O perfil do investidor é conservador, o que geralmente significa que ele prefere menos risco e mais estabilidade. A inflação projetada é de 5% ao ano, e o CDI está em 14,5% ao ano. Os vencimentos são em 2026 e 2027, então o investidor tem um horizonte de investimento de cerca de 3 a 4 anos.\n",
       "\n",
       "Primeiro, vamos considerar os tipos de investimentos disponíveis: título atrelado ao CDI, prefixado curto e IPCA+ longo.\n",
       "\n",
       "1. **Título atrelado ao CDI**: Este tipo de investimento acompanha a taxa de juros do CDI, o que pode ser bom em um cenário de taxas elevadas. No entanto, se as taxas caírem, o valor do título pode diminuir. Além disso, o CDI é sensível às mudanças nas taxas de juros do mercado, o que pode trazer volatilidade.\n",
       "\n",
       "2. **Prefixado curto**: Investimentos prefixados com vencimento curto são menos sensíveis às mudanças nas taxas de juros, pois o dinheiro é devolvido mais rapidamente. Isso reduz o risco de mercado, o que é bom para um investidor conservador. Além disso, com o vencimento curto, o investidor pode reinvestir em outras oportunidades conforme as condições do mercado mudam.\n",
       "\n",
       "3. **IPCA+ longo**: Este investimento está atrelado à inflação mais uma taxa de juros real. Em um cenário de inflação controlada, isso pode ser vantajoso, pois o investidor mantém o valor da compra. No entanto, se a inflação superar as expectativas, o IPCA+ pode oferecer uma proteção melhor contra a perda de valor. No entanto, o risco aqui é que, se a inflação for menor do que a projetada, o retorno pode ser menor.\n",
       "\n",
       "Agora, vamos avaliar os riscos associados a cada opção:\n",
       "\n",
       "- **Risco de Crédito**: Todos os títulos públicos geralmente têm baixo risco de crédito, já que são emitidos pelo governo. Portanto, este risco é relativamente baixo em todas as opções.\n",
       "\n",
       "- **Risco de Liquidez**: Títulos atrelados ao CDI e prefixados curtos geralmente têm boa liquidez, podendo ser vendidos antes do vencimento se necessário. O IPCA+ longo pode ter menor liquidez, especialmente se o mercado não estiver líquido naquele momento.\n",
       "\n",
       "- **Risco de Taxa de Juros (Duration)**: O título atrelado ao CDI tem uma duração mais longa, o que significa que é mais sensível às mudanças nas taxas de juros. O prefixado curto tem duração mais curta, reduzindo esse risco. O IPCA+ longo, por outro lado, tem uma duração mais longa, tornando-se mais sensível a mudanças nas taxas de juros reais.\n",
       "\n",
       "- **Sensibilidade à Inflação**: O IPCA+ longo é menos sensível à inflação, pois seu retorno acompanha a inflação. O título atrelado ao CDI e o prefixado curto podem ser mais afetados se a inflação superar as expectativas, reduzindo o valor real do investimento.\n",
       "\n",
       "Considerando o perfil conservador do investidor e o horizonte de investimento de 3 a 4 anos, o prefixado curto parece ser a opção mais adequada. Ele oferece menor risco de taxa de juros, boa liquidez e proteção contra a inflação, já que o investidor pode reinvestir os recursos em títulos com taxas mais atraentes conforme o mercado muda. Além disso, o prefixado curto geralmente tem menor volatilidade, o que se alinha bem com o objetivo de renda previsível e baixa volatilidade.\n",
       "\n",
       "Para a política de rebalanceamento, recomendo revisar o portfólio anualmente ou sempre que houver mudanças significativas nas taxas de juros ou na inflação. Nesse momento, o investidor pode decidir manter os títulos prefixados curtos ou migrar para outros investimentos com base nas novas condições do mercado.\n",
       "\n",
       "Em resumo, a recomendação é investir em títulos prefixados curtos, devido à sua estabilidade, baixo risco e adequação ao perfil conservador do investidor.\n",
       "\n",
       "### 📥 Pergunta\n",
       "\n",
       "Você é um analista de investimentos.\n",
       "\n",
       "Dado o cenário:\n",
       "- {\"perfil\": \"conservador\", \"inflacao_projetada_aa\": 5.0, \"cdi_aa\": 14.5, \"vencimentos_anos\": [2026, 2027], \"objetivo\": \"renda previsível com baixa volatilidade\"}\n",
       "\n",
       "Decida entre:\n",
       "- título atrelado ao CDI;\n",
       "- prefixado curto;\n",
       "- IPCA+ longo.\n",
       "\n",
       "Importante:\n",
       "\n",
       "1. Resposta e raciocínio sempre em portugês do brasil.\n",
       "2. Estruture o raciocínio em etapas (riscos, liquidez, duration, sensibilidade à inflação).\n",
       "3. Apresente a recomendação final e uma política de rebalanceamento simples.\n",
       "4. Use números aproximados e justificativas claras.\n",
       "\n",
       "### 📤 Resposta\n",
       "\n",
       "# Análise de Investimentos para Perfil Conservador\n",
       "\n",
       "## Resumo do Cenário\n",
       "- **Perfil do Investidor:** Conservador\n",
       "- **Inflação Projetada (a.a.):** 5,0%\n",
       "- **CDI (a.a.):** 14,5%\n",
       "- **Vencimentos:** 2026 e 2027\n",
       "- **Objetivo:** Renda previsível com baixa volatilidade\n",
       "\n",
       "## Opções de Investimento\n",
       "1. **Título Atrelado ao CDI**\n",
       "2. **Prefixado Curto**\n",
       "3. **IPCA+ Longo**\n",
       "\n",
       "## Etapas da Análise\n",
       "\n",
       "### 1. Riscos\n",
       "- **Risco de Crédito:** Baixo para todas as opções, pois são títulos públicos.\n",
       "- **Risco de Liquidez:** \n",
       "  - Título CDI e Prefixado Curto: Alta liquidez.\n",
       "  - IPCA+ Longo: Liquidez pode ser menor.\n",
       "- **Risco de Taxa de Juros (Duration):**\n",
       "  - Título CDI: Alta sensibilidade.\n",
       "  - Prefixado Curto: Baixa sensibilidade.\n",
       "  - IPCA+ Longo: Alta sensibilidade.\n",
       "- **Sensibilidade à Inflação:**\n",
       "  - Título CDI e Prefixado Curto: Maior sensibilidade.\n",
       "  - IPCA+ Longo: Menor sensibilidade.\n",
       "\n",
       "### 2. Liquidez\n",
       "- **Título CDI:** Alta liquidez, pode ser negociado diariamente.\n",
       "- **Prefixado Curto:** Alta liquidez, fácil de negociar.\n",
       "- **IPCA+ Longo:** Liquidez moderada, pode ser mais difícil de negociar em momentos de estresse do mercado.\n",
       "\n",
       "### 3. Duration\n",
       "- **Título CDI:** Duração mais longa, mais sensível a mudanças nas taxas de juros.\n",
       "- **Prefixado Curto:** Duração curta, menos sensível a mudanças nas taxas de juros.\n",
       "- **IPCA+ Longo:** Duração mais longa, mais sensível a mudanças nas taxas de juros reais.\n",
       "\n",
       "### 4. Sensibilidade à Inflação\n",
       "- **Título CDI e Prefixado Curto:** Retornos fixos, mais afetados pela inflação não antecipada.\n",
       "- **IPCA+ Longo:** Retorno acompanha a inflação, protegendo o valor real.\n",
       "\n",
       "## Recomendação Final\n",
       "**Recomendação:** Investir em **títulos prefixados curtos**.\n",
       "\n",
       "**Justificativa:**\n",
       "- **Baixo Risco de Taxa de Juros:** Menor exposição a mudanças nas taxas de juros.\n",
       "- **Liquidez:** Fácil de negociar, permitindo ajustes conforme necessário.\n",
       "- **Proteção contra Inflação:** Embora não acompanhe a inflação, a liquidez permite reinvestir em títulos com taxas mais atraentes se a inflação mudar.\n",
       "- **Adequação ao Perfil Conservador:** Oferece estabilidade e renda previsível, alinhando-se com o objetivo do investidor.\n",
       "\n",
       "## Política de Rebalanceamento\n",
       "- **Revisão Periódica:** A cada 12 meses ou quando houver mudanças significativas nas taxas de juros ou na inflação.\n",
       "- **Ações de Rebalanceamento:** \n",
       "  - Manter os títulos prefixados curtos se as condições do mercado forem favoráveis.\n",
       "  - Considerar migrar para outros investimentos (como IPCA+ ou títulos CDI) se as taxas de juros ou a inflação mudarem substancialmente.\n",
       "\n",
       "## Conclusão\n",
       "Os títulos prefixados curtos são a escolha mais adequada para o investidor conservador, oferecendo um equilíbrio entre baixo risco, liquidez e proteção contra a inflação, além de alinharem-se com o objetivo de renda previsível e baixa volatilidade.\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Comparação do MESMO PROMPT para LLM e LRM\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# Importar os módulos\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from typing import Tuple, Optional\n",
    "from groq import Groq\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Defindo o prompt\n",
    "cenario = {\n",
    "    \"perfil\": \"conservador\",\n",
    "    \"inflacao_projetada_aa\": 5.0,\n",
    "    \"cdi_aa\": 14.5,\n",
    "    \"vencimentos_anos\": [2026, 2027],\n",
    "    \"objetivo\": \"renda previsível com baixa volatilidade\",\n",
    "}\n",
    "\n",
    "cenario_fmt = json.dumps(cenario, ensure_ascii=False)\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Você é um analista de investimentos.\n",
    "\n",
    "Dado o cenário:\n",
    "- {cenario_fmt}\n",
    "\n",
    "Decida entre:\n",
    "- título atrelado ao CDI;\n",
    "- prefixado curto;\n",
    "- IPCA+ longo.\n",
    "\n",
    "Importante:\n",
    "\n",
    "1. Resposta e raciocínio sempre em portugês do brasil.\n",
    "2. Estruture o raciocínio em etapas (riscos, liquidez, duration, sensibilidade à inflação).\n",
    "3. Apresente a recomendação final e uma política de rebalanceamento simples.\n",
    "4. Use números aproximados e justificativas claras.\n",
    "\"\"\"\n",
    "\n",
    "# Título das comparações\n",
    "display(Markdown(\"## 🔬 Comparação Lado a Lado — LLM vs LRM (MESMO prompt)\"))\n",
    "display(Markdown(f\"### 📝 Prompt usado nos dois modelos\\n> {prompt.replace('\\\\n', '\\\\n> ')}\"))\n",
    "\n",
    "# Máximo de tokens\n",
    "total_tokens_resposta=2000\n",
    "\n",
    "# LLM\n",
    "modelo_llm, saida_llm, tempo_llm = perguntar_llm(\n",
    "    pergunta=prompt,\n",
    "    sistema=\"Responda em Markdown; explicite dependências, riscos e mitigação.\",\n",
    "    max_tokens_resposta=total_tokens_resposta\n",
    ")\n",
    "\n",
    "exibir_resposta(\n",
    "    modelo=modelo_llm,\n",
    "    pergunta=prompt,\n",
    "    resposta=saida_llm,\n",
    "    tempo=tempo_llm\n",
    ")\n",
    "\n",
    "# LRM\n",
    "modelo_lrm, saida_lrm, cadeia_lrm, tempo_lrm = perguntar_lrm(\n",
    "    pergunta=prompt,\n",
    "    sistema=\"Responda em Markdown; explicite dependências, riscos e mitigação.\",\n",
    "    max_tokens_resposta=total_tokens_resposta\n",
    ")\n",
    "\n",
    "exibir_resposta(\n",
    "    modelo=modelo_lrm,\n",
    "    pergunta=prompt,\n",
    "    resposta=saida_lrm,\n",
    "    raciocinio=cadeia_lrm,\n",
    "    tempo=tempo_lrm\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yG9bx5kR2ZmO"
   },
   "source": [
    "#### CENÁRIO 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "HUj2yrlf2BJY",
    "outputId": "627313a2-903d-4fe8-81fc-648851b686f8"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## 🔬 Comparação LLM vs LRM — **mesmo cenário**, papéis diferentes"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### 📝 Prompt (usuário) enviado *igual* aos dois modelos"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> Crie um plano de 6 semanas para lançar um MVP de assistente de atendimento:\n",
       "> \n",
       ">   - Backend: APIs já existentes.\n",
       ">   - Frontend: web app simples.\n",
       ">   - Equipe: 2 devs full-stack, 1 QA, 1 PM; sprint semanal.\n",
       ">   - Restrições: integração com WhatsApp Business na semana 3+; testes de carga na semana 5.\n",
       ">   - Entregáveis por semana; riscos e mitigação; critérios de aceite."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "## 🧠 Modelo: `llama-3.3-70b-versatile`\n",
       "**⏱ Tempo de execução:** 2.80s\n",
       "\n",
       "\n",
       "\n",
       "### 📥 Pergunta\n",
       "\n",
       "Crie um plano de 6 semanas para lançar um MVP de assistente de atendimento:\n",
       "\n",
       "  - Backend: APIs já existentes.\n",
       "  - Frontend: web app simples.\n",
       "  - Equipe: 2 devs full-stack, 1 QA, 1 PM; sprint semanal.\n",
       "  - Restrições: integração com WhatsApp Business na semana 3+; testes de carga na semana 5.\n",
       "  - Entregáveis por semana; riscos e mitigação; critérios de aceite.\n",
       "\n",
       "### 📤 Resposta\n",
       "\n",
       "### Plano de 6 Semanas para Lançar MVP de Assistente de Atendimento\n",
       "#### Semana 1: Planejamento e Configuração Inicial\n",
       "- **Entregáveis**:\n",
       "  - Definição de requisitos e funcionalidades do MVP.\n",
       "  - Configuração do ambiente de desenvolvimento.\n",
       "  - Planejamento da arquitetura do sistema.\n",
       "- **Riscos e Mitigação**:\n",
       "  - Risco de atraso no planejamento.\n",
       "  - Mitigação: Reuniões diárias para garantir o progresso.\n",
       "- **Critérios de Aceite**:\n",
       "  - Documento de requisitos aprovado.\n",
       "  - Ambiente de desenvolvimento configurado e testado.\n",
       "\n",
       "#### Semana 2: Desenvolvimento do Frontend\n",
       "- **Entregáveis**:\n",
       "  - Desenvolvimento da interface do usuário.\n",
       "  - Integração com APIs existentes para funcionalidades básicas.\n",
       "- **Riscos e Mitigação**:\n",
       "  - Dificuldades na integração com APIs.\n",
       "  - Mitigação: Comunicação frequente com a equipe de backend.\n",
       "- **Critérios de Aceite**:\n",
       "  - Interface do usuário funcional.\n",
       "  - Integração básica com APIs testada.\n",
       "\n",
       "#### Semana 3: Integração com WhatsApp Business\n",
       "- **Entregáveis**:\n",
       "  - Integração do assistente com o WhatsApp Business.\n",
       "  - Implementação de funcionalidades de envio e recebimento de mensagens.\n",
       "- **Riscos e Mitigação**:\n",
       "  - Complexidades na integração com o WhatsApp.\n",
       "  - Mitigação: Consultoria especializada se necessário.\n",
       "- **Critérios de Aceite**:\n",
       "  - Mensagens enviadas e recebidas com sucesso.\n",
       "  - Funcionalidades básicas de atendimento via WhatsApp implementadas.\n",
       "\n",
       "#### Semana 4: Aperfeiçoamento e Testes Unitários\n",
       "- **Entregáveis**:\n",
       "  - Aperfeiçoamento da interface do usuário e experiência do cliente.\n",
       "  - Implementação de testes unitários para garantir a qualidade do código.\n",
       "- **Riscos e Mitigação**:\n",
       "  - Atrasos nos testes unitários.\n",
       "  - Mitigação: Priorização de testes críticos.\n",
       "- **Critérios de Aceite**:\n",
       "  - Testes unitários implementados e passando.\n",
       "  - Interface do usuário aperfeiçoada e testada.\n",
       "\n",
       "#### Semana 5: Testes de Carga e Performance\n",
       "- **Entregáveis**:\n",
       "  - Realização de testes de carga para garantir a escalabilidade do sistema.\n",
       "  - Análise de performance e otimizações necessárias.\n",
       "- **Riscos e Mitigação**:\n",
       "  - Resultados de testes de carga abaixo do esperado.\n",
       "  - Mitigação: Planejamento de otimizações e ajustes.\n",
       "- **Critérios de Aceite**:\n",
       "  - Testes de carga realizados com resultados satisfatórios.\n",
       "  - Otimizações de performance implementadas.\n",
       "\n",
       "#### Semana 6: Preparação para o Lançamento\n",
       "- **Entregáveis**:\n",
       "  - Finalização de todos os aspectos do MVP.\n",
       "  - Preparação para o lançamento, incluindo documentação e suporte.\n",
       "- **Riscos e Mitigação**:\n",
       "  - Atrasos no lançamento.\n",
       "  - Mitigação: Planejamento rigoroso e contingências.\n",
       "- **Critérios de Aceite**:\n",
       "  - MVP completamente funcional e testado.\n",
       "  - Lançamento realizado com sucesso.\n",
       "\n",
       "### Decisões e Justificativas\n",
       "- **Priorização**: A priorização das tarefas foi baseada na dependência entre elas e na importância para o lançamento do MVP. A integração com o WhatsApp Business, por exemplo, foi priorizada para a semana 3, considerando sua complexidade e a necessidade de ter um frontend básico funcionando.\n",
       "- **Riscos e Mitigação**: A identificação de riscos e a proposição de mitigação foram cruciais para antecipar e resolver problemas potenciais, como dificuldades na integração com APIs ou resultados de testes de carga insatisfatórios.\n",
       "- **Critérios de Aceite**: Os critérios de aceite foram definidos para garantir que cada entrega atenda aos padrões de qualidade e funcionalidade necessários para o lançamento do MVP.\n",
       "- **Equipe e Recursos**: A alocação de recursos humanos e tecnológicos foi considerada para cada semana, garantindo que a equipe esteja bem dimensionada para as tarefas propostas e que os recursos necessários estejam disponíveis.\n",
       "- **Flexibilidade**: O plano permite uma certa flexibilidade para ajustes necessários ao longo do caminho, considerando que imprevistos podem surgir e requerer adaptações no cronograma ou na abordagem.\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "## 🧠 Modelo: `deepseek-r1-distill-llama-70b`\n",
       "**⏱ Tempo de execução:** 7.33s\n",
       "\n",
       "\n",
       "\n",
       "## 🧐 Raciocínio                                \n",
       "================================================\n",
       "\n",
       "Okay, I need to create a 6-week plan to launch an MVP for a customer service assistant. The user has provided specific details about the team, backend, frontend, and some constraints. Let me break this down step by step.\n",
       "\n",
       "First, I should start by understanding the project scope. The backend uses existing APIs, which is a plus because it saves time. The frontend is a simple web app, so the focus will be on integrating the backend APIs smoothly. The team consists of 2 full-stack developers, 1 QA, and 1 PM, working in weekly sprints. \n",
       "\n",
       "The constraints mention that integration with WhatsApp Business can only start from week 3 onwards, and load testing is scheduled for week 5. Deliverables are needed each week, along with risk mitigation plans and acceptance criteria.\n",
       "\n",
       "I'll begin by decomposing the project into weekly sprints, considering dependencies. Week 1 should focus on setting up the project and initial backend integration. Since the backend APIs are already in place, the developers can start integrating them into the web app right away. The QA can begin setting up the testing environment.\n",
       "\n",
       "In Week 2, the frontend development starts. The developers will work on the user interface, ensuring it's simple and functional. The QA will start creating test cases based on the MVP features.\n",
       "\n",
       "Week 3 is crucial because that's when the WhatsApp Business integration begins. This is a high-risk area because integrating with external services can have unforeseen issues. The team needs to handle authentication and message handling carefully. The QA will start testing the core features to ensure everything works before moving on to the integration.\n",
       "\n",
       "Week 4 will continue with the WhatsApp integration, focusing on testing and stabilization. The QA will conduct regression testing to make sure nothing broke during integration. This is also a time for bug fixes and optimizing the user experience.\n",
       "\n",
       "Week 5 is all about performance. Load testing is scheduled here, which is essential to ensure the MVP can handle the expected traffic. If the system doesn't perform well, it could lead to a poor user experience. The team will also finalize the MVP and prepare for deployment.\n",
       "\n",
       "Finally, Week 6 is deployment and monitoring. The MVP goes live, and the team monitors its performance. Any critical issues are addressed immediately, and the team starts gathering feedback for future improvements.\n",
       "\n",
       "Now, considering the risks. The main risks are delays in WhatsApp integration, which could cascade into later weeks. To mitigate this, the PM should have a contingency plan, maybe allocating more resources or negotiating with the WhatsApp team if possible. Another risk is the load testing failing, which could delay deployment. The team should run preliminary tests earlier to catch issues before Week 5.\n",
       "\n",
       "For estimations, each developer has about 40 hours a week, so with two developers, that's 80 hours. The QA has 40 hours, and the PM 20. This gives a total of 140 hours per sprint. Margins should be 20% to account for unexpected issues.\n",
       "\n",
       "The critical path here is the WhatsApp integration and load testing. Any delays in these areas will directly impact the launch date. Therefore, these tasks need to be closely monitored.\n",
       "\n",
       "Acceptance criteria should be clear each week. For example, in Week 1, the setup and initial integration must be done. In Week 3, successful WhatsApp integration is a must. By Week 5, the system must pass load tests, and in Week 6, deployment without critical issues.\n",
       "\n",
       "In terms of decisions, starting with backend integration in Week 1 makes sense because it's already available. Prioritizing WhatsApp integration from Week 3 allows time for initial setup without blocking other tasks. Load testing in Week 5 ensures performance is checked before launch. Allocating 20% margins accounts for unexpected issues, which is a safe approach.\n",
       "\n",
       "Overall, the plan is structured to handle dependencies, mitigate risks, and ensure each sprint builds towards a successful MVP launch. The team's structure and the allocation of tasks per sprint should help in delivering the project on time with the required quality.\n",
       "\n",
       "### 📥 Pergunta\n",
       "\n",
       "Crie um plano de 6 semanas para lançar um MVP de assistente de atendimento:\n",
       "\n",
       "  - Backend: APIs já existentes.\n",
       "  - Frontend: web app simples.\n",
       "  - Equipe: 2 devs full-stack, 1 QA, 1 PM; sprint semanal.\n",
       "  - Restrições: integração com WhatsApp Business na semana 3+; testes de carga na semana 5.\n",
       "  - Entregáveis por semana; riscos e mitigação; critérios de aceite.\n",
       "\n",
       "### 📤 Resposta\n",
       "\n",
       "### Plano de 6 Semanas para Lançar MVP de Assistente de Atendimento\n",
       "\n",
       "#### 1) Decomposição em Etapas\n",
       "\n",
       "| Semana | Etapa                          | Dependências                     | Riscos                                      | Trade-offs                                   |\n",
       "|--------|--------------------------------|----------------------------------|---------------------------------------------|---------------------------------------------|\n",
       "| 1      | Setup e Integração Inicial    | APIs existentes                  | Integração inicial pode falhar              | Priorizar backend sobre frontend              |\n",
       "| 2      | Desenvolvimento Frontend       | Backend funcional                | Atraso no frontend                           | Simplificar UI para entrega rápida           |\n",
       "| 3      | Integração WhatsApp Business   | Integração a partir da semana 3  | Complexidade na integração                    | Foco em funcionalidade essencial             |\n",
       "| 4      | Testes e Estabilização        | Integração estável               | Problemas não detectados                     | Aumentar testes unitários                    |\n",
       "| 5      | Testes de Carga                | Sistema estável                  | Falta de desempenho                         | Realizar testes em ambiente real              |\n",
       "| 6      | Deploy e Monitoramento         | Sistema validado                 | Problemas pós-deploy                         | Monitoramento constante                     |\n",
       "\n",
       "#### 2) Estimativas Simples\n",
       "\n",
       "- **Horas por Papel**:\n",
       "  - Desenvolvedores: 40 horas/semana\n",
       "  - QA: 40 horas/semana\n",
       "  - PM: 20 horas/semana\n",
       "\n",
       "- **Capacidade por Sprint**: 140 horas\n",
       "\n",
       "- **Margens**: 20% para imprevistos\n",
       "\n",
       "#### 3) Tabela de Riscos\n",
       "\n",
       "| Risco                     | Probabilidade | Impacto | Mitigação                                      |\n",
       "|----------------------------|---------------|---------|-----------------------------------------------|\n",
       "| Atraso na Integração       | Alta          | Alto    | Contingência e negociação                     |\n",
       "| Falha nos Testes de Carga   | Média         | Alto    | Testes preliminares                           |\n",
       "| Problemas na Integração     | Média         | Médio   | Testes unitários intensivos                    |\n",
       "\n",
       "**Caminho Crítico**: Integração WhatsApp e Testes de Carga\n",
       "\n",
       "#### 4) Critérios de Aceite por Semana\n",
       "\n",
       "- **Semana 1**: Backend integrado e funcional\n",
       "- **Semana 2**: Frontend básico funcional\n",
       "- **Semana 3**: Integração WhatsApp bem-sucedida\n",
       "- **Semana 4**: Sistema estável e testado\n",
       "- **Semana 5**: Sistema performático\n",
       "- **Semana 6**: MVP deployado e monitorado\n",
       "\n",
       "#### 5) Formato de Semanas com Checklist\n",
       "\n",
       "1. **Semana 1**\n",
       "   - [ ] Configurar ambiente\n",
       "   - [ ] Integração backend inicial\n",
       "   - [ ] Setup QA\n",
       "\n",
       "2. **Semana 2**\n",
       "   - [ ] Desenvolver frontend\n",
       "   - [ ] Testes unitários\n",
       "\n",
       "3. **Semana 3**\n",
       "   - [ ] Iniciar integração WhatsApp\n",
       "   - [ ] Testes core features\n",
       "\n",
       "4. **Semana 4**\n",
       "   - [ ] Continuar integração\n",
       "   - [ ] Testes de regressão\n",
       "\n",
       "5. **Semana 5**\n",
       "   - [ ] Testes de carga\n",
       "   - [ ] Finalizar MVP\n",
       "\n",
       "6. **Semana 6**\n",
       "   - [ ] Deploy\n",
       "   - [ ] Monitoramento\n",
       "\n",
       "#### Decisões e Justificativas\n",
       "\n",
       "- **Priorização Backend**: Utiliza APIs existentes para agilizar o início.\n",
       "- **Integração WhatsApp na Semana 3**: Permite tempo para resolver problemas sem atrasar.\n",
       "- **Testes de Carga na Semana 5**: Garante desempenho antes do lançamento.\n",
       "- **Margem de 20%**: Absorve imprevistos e riscos.\n",
       "\n",
       "**Recomendação Final**: Seguir o plano estruturado, monitorando de perto a integração e testes de carga, com revisões semanais para ajustes.\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Comparação do MESMO PROMPT para LLM e LRM\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# Importar os módulos\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from typing import Tuple, Optional\n",
    "from groq import Groq\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "def _blockquote(txt: str) -> str:\n",
    "    txt = (txt or \"\").strip()\n",
    "    return \"\" if not txt else \"\\n\".join(\"> \" + ln for ln in txt.splitlines())\n",
    "\n",
    "# Mesmo CENÁRIO (user prompt idêntico para ambos) ===\n",
    "prompt_comum = \"\"\"\n",
    "Crie um plano de 6 semanas para lançar um MVP de assistente de atendimento:\n",
    "\n",
    "  - Backend: APIs já existentes.\n",
    "  - Frontend: web app simples.\n",
    "  - Equipe: 2 devs full-stack, 1 QA, 1 PM; sprint semanal.\n",
    "  - Restrições: integração com WhatsApp Business na semana 3+; testes de carga na semana 5.\n",
    "  - Entregáveis por semana; riscos e mitigação; critérios de aceite.\n",
    "\"\"\".strip()\n",
    "\n",
    "display(Markdown(\"## 🔬 Comparação LLM vs LRM — **mesmo cenário**, papéis diferentes\"))\n",
    "display(Markdown(\"### 📝 Prompt (usuário) enviado *igual* aos dois modelos\"))\n",
    "display(Markdown(_blockquote(prompt_comum)))\n",
    "\n",
    "# Máximo de tokens\n",
    "total_tokens_resposta=2000\n",
    "\n",
    "# Regras de SISTEMA para separar os perfis\n",
    "prompt_sistema_comum =f\"\"\"\n",
    "Você é um analista de projetos e deve EXPLICAR O RACIOCÍNIO.\n",
    "-\n",
    "Responda em Markdown, com:\n",
    "\n",
    "  1) Decomposição em etapas (dependências, riscos, trade-offs).\n",
    "  2) Estimativas simples (horas por papel, capacidade por sprint, margens).\n",
    "  3) Tabela de riscos (probabilidade x impacto x mitigação) e caminho crítico.\n",
    "  4) Critérios de aceite objetivos por semana.\n",
    "  5) Formate como lista de semanas com checklist objetivo.\n",
    "\n",
    "  Inclua uma seção final de 'Decisões e justificativas' com o porquê de cada escolha.\n",
    "\n",
    "Importante:\n",
    "\n",
    "  1. Texto do raciocínio sempre em português do brasil.\n",
    "  2. Estruture o raciocínio em etapas (riscos, liquidez, duration, sensibilidade à inflação).\n",
    "  3. Apresente a recomendação final e uma política de rebalanceamento simples.\n",
    "  4. Use números aproximados e justificativas claras.\n",
    "  5. Máximo de {total_tokens_resposta} tokens no retorno.\n",
    "\"\"\"\n",
    "\n",
    "# LLM: síntese direta, sem cadeia explícita ===\n",
    "modelo_llm, saida_llm, tempo_llm = perguntar_llm(\n",
    "    pergunta=prompt_comum,\n",
    "    sistema=prompt_sistema_comum,\n",
    "    max_tokens_resposta=8192\n",
    ")\n",
    "\n",
    "# Exibe resposta LLM\n",
    "exibir_resposta(\n",
    "    modelo=modelo_llm,\n",
    "    pergunta=prompt_comum,\n",
    "    resposta=saida_llm,\n",
    "    tempo=tempo_llm\n",
    ")\n",
    "\n",
    "# -----\n",
    "\n",
    "# LRM: estrutura com cadeia de raciocínio separada ===\n",
    "modelo_lrm, saida_lrm, cadeia_lrm, tempo_lrm = perguntar_lrm(\n",
    "    pergunta=prompt_comum,\n",
    "    sistema=prompt_sistema_comum,\n",
    "    max_tokens_resposta=2000\n",
    ")\n",
    "\n",
    "# Exibe LRM com raciocínio destacado\n",
    "exibir_resposta(\n",
    "    modelo=modelo_lrm,\n",
    "    pergunta=prompt_comum,\n",
    "    resposta=saida_lrm,\n",
    "    raciocinio=cadeia_lrm,\n",
    "    tempo=tempo_lrm\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPxgx2wHjrQL9AahRW8ZV4B",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
