{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "150bb2d9",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/pathbit/pathbit-academy-ai/blob/master/0004_rag_vs_finetuning/notebooks/rag_vs_finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "# ✨ **Pathbit Academy AI**\n",
        "---\n",
        "\n",
        "## 🎯 **Artigo 0004: RAG vs Fine-Tuning - A escolha que pode salvar (ou afundar) seu projeto**\n",
        "\n",
        "🚨 **IMPORTANTE:**\n",
        "\n",
        "*💥 Se você já executou os notebooks anteriores, este será moleza!*\n",
        "\n",
        "**Artigo de referência:** [RAG vs Fine-Tuning](https://github.com/pathbit/pathbit-academy-ai/blob/master/0004_rag_vs_finetuning/article/ARTICLE.md)\n",
        "\n",
        "**Artigos anteriores:**\n",
        "- [Artigo 0001: LLM vs LRM](https://github.com/pathbit/pathbit-academy-ai/blob/master/0001_llm_x_lrm/article/ARTICLE.md)\n",
        "- [Artigo 0002: Embeddings e Vetorização](https://github.com/pathbit/pathbit-academy-ai/blob/master/0002_embeddings_vetorizacao/article/ARTICLE.md)\n",
        "- [Artigo 0003: RAG e Vector Database](https://github.com/pathbit/pathbit-academy-ai/blob/master/0003_rag_vector_database/article/ARTICLE.md)\n",
        "\n",
        "---\n",
        "\n",
        "## 🎯 **Este notebook contém:**\n",
        "- ✅ **Comparação técnica** entre RAG e Fine-Tuning\n",
        "- ✅ **Caso prático real** (Assistente de Investimentos)\n",
        "- ✅ **3 implementações** para comparar\n",
        "- ✅ **Análise de custos** detalhada\n",
        "- ✅ **Quando usar cada um**\n",
        "\n",
        "---\n",
        "\n",
        "## 📊 **O que vamos construir:**\n",
        "\n",
        "### 🤖 Teste 1: Modelo Base\n",
        "GPT sem customização (baseline)\n",
        "\n",
        "### 🔍 Teste 2: RAG\n",
        "Modelo + Base de conhecimento sobre investimentos\n",
        "\n",
        "### 🎓 Teste 3: Fine-Tuning  \n",
        "Modelo treinado (simulado com few-shot)\n",
        "\n",
        "### ⚖️ Comparação\n",
        "Qual funciona melhor? Quanto custa? Quando usar?\n",
        "\n",
        "---\n",
        "\n",
        "## 💡 **Caso de Uso Real: Assistente Financeiro**\n",
        "\n",
        "Queremos um assistente que responda sobre investimentos (CDB, Tesouro, FIIs, Ações) com:\n",
        "- ✅ Informações precisas e atualizadas\n",
        "- ✅ Tom didático e prático\n",
        "- ✅ Exemplos numéricos\n",
        "- ✅ Citação de fontes\n",
        "\n",
        "**Pergunta que vamos responder:** RAG ou Fine-Tuning para este caso?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62a5ac6c",
      "metadata": {},
      "source": [
        "## 📦 Setup e Instalação\n",
        "\n",
        "**Tempo estimado:** 3-5 minutos (primeira execução)\n",
        "\n",
        "---\n",
        "\n",
        "### 🆘 **Solução para Erro de Metadata no Colab:**\n",
        "\n",
        "Se você vê este erro:\n",
        "```\n",
        "the 'state' key is missing from 'metadata.widgets'\n",
        "```\n",
        "\n",
        "**Solução rápida:**\n",
        "1. Abra o notebook pelo GitHub: [Link direto](https://colab.research.google.com/github/pathbit/pathbit-academy-ai/blob/master/0004_rag_vs_finetuning/notebooks/rag_vs_finetuning.ipynb)\n",
        "2. Ou salve uma cópia limpa: `File → Download → Download .ipynb`\n",
        "3. Faça upload da cópia no Colab\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "946f740e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 🔧 CORREÇÃO AUTOMÁTICA PARA ERRO DE METADATA NO COLAB\n",
        "# ========================================================\n",
        "\n",
        "# Esta célula corrige automaticamente o erro:\n",
        "# \"the 'state' key is missing from 'metadata.widgets'\"\n",
        "\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB_ENV = True\n",
        "\n",
        "    # Tentar corrigir o notebook se houver erro de metadata\n",
        "    try:\n",
        "        import json\n",
        "        from google.colab import files\n",
        "\n",
        "        print(\"🔧 Verificando metadados do notebook...\")\n",
        "\n",
        "        # Nota: No Colab, o notebook já está carregado, então apenas informamos\n",
        "        print(\"✅ Se você consegue ver esta mensagem, o notebook está OK!\")\n",
        "        print(\"ℹ️  Se teve erro antes, recarregue a página (F5)\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ℹ️  Verificação de metadata: {e}\")\n",
        "\n",
        "except ImportError:\n",
        "    IN_COLAB_ENV = False\n",
        "    print(\"💻 Ambiente Local - Correção de metadata não necessária\")\n",
        "\n",
        "print(\"🎯 Continue com a próxima célula!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "0291ce02",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Ambiente configurado com sucesso!\n",
            "📅 Data/Hora: 12/10/2025 13:07:34\n",
            "🐍 Python: 3.12.7\n",
            "📁 Diretório: /Users/elielsousa/Projects/pathbit/github/pub/pathbit-academy-ai/0004_rag_vs_finetuning/notebooks\n"
          ]
        }
      ],
      "source": [
        "### 🔧 **Correção automática para Google Colab**\n",
        "# 🚨 **IMPORTANTE:** Se você estiver executando no Google Colab, esta célula corrige automaticamente problemas de compatibilidade do `tqdm`.\n",
        "\n",
        "# Importações principais\n",
        "import sys\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "# Configuração de visualização\n",
        "plt.style.use(\"seaborn-v0_8\")\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams[\"figure.figsize\"] = (12, 8)\n",
        "plt.rcParams[\"font.size\"] = 12\n",
        "\n",
        "# Configuração de fonte para evitar warnings de emojis\n",
        "plt.rcParams[\"font.family\"] = \"DejaVu Sans\"\n",
        "import matplotlib\n",
        "\n",
        "matplotlib.rcParams[\"axes.unicode_minus\"] = False\n",
        "\n",
        "# Suprimir avisos do tqdm\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"tqdm\")\n",
        "\n",
        "print(\"✅ Ambiente configurado com sucesso!\")\n",
        "print(f\"📅 Data/Hora: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}\")\n",
        "print(f\"🐍 Python: {sys.version.split()[0]}\")\n",
        "print(f\"📁 Diretório: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "494320d6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "💻 Detectado: Ambiente Local\n",
            "ℹ️  Correção do tqdm não necessária no ambiente local\n",
            "\n",
            "🎯 Ambiente configurado! Continue com a próxima célula.\n"
          ]
        }
      ],
      "source": [
        "# 🔧 CORREÇÃO AUTOMÁTICA PARA GOOGLE COLAB\n",
        "# ==========================================\n",
        "# Esta célula resolve automaticamente conflitos de dependências do tqdm\n",
        "\n",
        "# Detectar se estamos no Google Colab\n",
        "try:\n",
        "    import google.colab\n",
        "\n",
        "    IN_COLAB = True\n",
        "    print(\"🌐 Detectado: Google Colab\")\n",
        "    print(\"🔧 Aplicando correção para conflito de tqdm...\")\n",
        "\n",
        "    # CORREÇÃO: Atualizar tqdm para resolver conflitos de dependências\n",
        "    get_ipython().run_line_magic(\n",
        "        \"pip\", \"install --upgrade tqdm>=4.67 --force-reinstall --quiet\"\n",
        "    )\n",
        "    print(\"✅ tqdm atualizado com sucesso!\")\n",
        "    print(\n",
        "        \"📦 Versão do tqdm corrigida para resolver conflitos com datasets e dataproc-spark-connect\"\n",
        "    )\n",
        "\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    print(\"💻 Detectado: Ambiente Local\")\n",
        "    print(\"ℹ️  Correção do tqdm não necessária no ambiente local\")\n",
        "\n",
        "print(\"\\n🎯 Ambiente configurado! Continue com a próxima célula.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55c83771",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📦 Verificando dependências no ambiente local...\n",
            "✅ Todas as dependências já estão instaladas!\n",
            "🎯 Notebook pronto para usar!\n",
            "📊 Dependências carregadas com sucesso!\n"
          ]
        }
      ],
      "source": [
        "# Instalar dependências (Colab precisa do %pip)\n",
        "if IN_COLAB:\n",
        "    print(\"📦 Instalando dependências no Google Colab...\")\n",
        "    print(\"⏳ Isso pode levar 2-3 minutos...\")\n",
        "\n",
        "    # Instalar todas as dependências de uma vez\n",
        "    get_ipython().run_line_magic(\n",
        "        \"pip\",\n",
        "        \"install -q groq langchain langchain-groq langchain-community langchain-huggingface chromadb sentence-transformers scikit-learn matplotlib seaborn pandas plotly tqdm>=4.67 python-dotenv transformers datasets peft accelerate torch\",\n",
        "    )\n",
        "\n",
        "    print(\"🔧 Corrigindo versão do requests para compatibilidade com Colab...\")\n",
        "    get_ipython().run_line_magic(\"pip\", \"install -q requests==2.32.4\")\n",
        "    print(\"✅ requests corrigido!\")\n",
        "    \n",
        "    print(\"✅ Todas as dependências instaladas!\")\n",
        "else:\n",
        "    print(\"📦 Verificando dependências no ambiente local...\")\n",
        "    try:\n",
        "        import numpy as np\n",
        "        import pandas as pd\n",
        "        import matplotlib.pyplot as plt\n",
        "        import seaborn as sns\n",
        "        import chromadb\n",
        "        from sentence_transformers import SentenceTransformer\n",
        "        from sklearn.metrics.pairwise import cosine_similarity\n",
        "        from dotenv import load_dotenv\n",
        "        from groq import Groq\n",
        "        from langchain_groq import ChatGroq\n",
        "        from langchain_huggingface import HuggingFaceEmbeddings\n",
        "        import transformers\n",
        "        import torch\n",
        "\n",
        "        print(\"✅ Todas as dependências já estão instaladas!\")\n",
        "    except ImportError as e:\n",
        "        print(f\"⚠️ Instalando dependências faltantes: {e}\")\n",
        "        import subprocess\n",
        "        import sys\n",
        "\n",
        "        subprocess.check_call(\n",
        "            [\n",
        "                sys.executable,\n",
        "                \"-m\",\n",
        "                \"pip\",\n",
        "                \"install\",\n",
        "                \"-q\",\n",
        "                \"groq\",\n",
        "                \"langchain\",\n",
        "                \"langchain-groq\",\n",
        "                \"langchain-community\",\n",
        "                \"langchain-huggingface\",\n",
        "                \"chromadb\",\n",
        "                \"sentence-transformers\",\n",
        "                \"scikit-learn\",\n",
        "                \"matplotlib\",\n",
        "                \"seaborn\",\n",
        "                \"pandas\",\n",
        "                \"plotly\",\n",
        "                \"tqdm>=4.67\",\n",
        "                \"python-dotenv\",\n",
        "                \"transformers\",\n",
        "                \"datasets\",\n",
        "                \"peft\",\n",
        "                \"accelerate\",\n",
        "                \"torch\",\n",
        "            ]\n",
        "        )\n",
        "\n",
        "# Importações principais\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Suprimir avisos do tqdm e outros warnings desnecessários\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"tqdm\")\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# Configurações adicionais\n",
        "plt.style.use(\"default\")\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "pd.set_option(\"display.width\", None)\n",
        "\n",
        "print(\"🎯 Notebook pronto para usar!\")\n",
        "print(\"📊 Dependências carregadas com sucesso!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8756b9d4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ GROQ_API_KEY configurada (arquivo .env)\n",
            "🤖 Modelo de embeddings: all-MiniLM-L6-v2\n",
            "✅ Modelo de embeddings carregado com sucesso!\n"
          ]
        }
      ],
      "source": [
        "# Importações específicas para RAG e Vector Databases\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Para exemplos com Groq (opcional)\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# 🔑 Configuração da API Key do Groq\n",
        "# ====================================\n",
        "def configurar_groq_api():\n",
        "    \"\"\"Configura a API key do Groq de forma simples\"\"\"\n",
        "    # Tentar carregar do arquivo .env primeiro\n",
        "    try:\n",
        "        load_dotenv()\n",
        "        groq_key = os.getenv(\"GROQ_API_KEY\")\n",
        "        if groq_key:\n",
        "            os.environ[\"GROQ_API_KEY\"] = groq_key\n",
        "            print(\"✅ GROQ_API_KEY configurada (arquivo .env)\")\n",
        "            return True\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # Se não encontrou no .env e está no Colab, tentar secrets\n",
        "    if IN_COLAB:\n",
        "        try:\n",
        "            from google.colab import userdata\n",
        "\n",
        "            groq_key = userdata.get(\"GROQ_API_KEY\")\n",
        "            os.environ[\"GROQ_API_KEY\"] = groq_key\n",
        "            print(\"✅ GROQ_API_KEY configurada (Colab secrets)\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ GROQ_API_KEY não encontrada: {e}\")\n",
        "\n",
        "    print(\"ℹ️ GROQ_API_KEY não configurada - exemplos avançados não funcionarão\")\n",
        "    return False\n",
        "\n",
        "# Configurar API Key\n",
        "configurar_groq_api()\n",
        "\n",
        "# Configuração do modelo de embeddings\n",
        "EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n",
        "print(f\"🤖 Modelo de embeddings: {EMBEDDING_MODEL}\")\n",
        "\n",
        "# Inicializar modelo de embeddings\n",
        "embedding_model = SentenceTransformer(EMBEDDING_MODEL)\n",
        "print(\"✅ Modelo de embeddings carregado com sucesso!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35c8e740",
      "metadata": {},
      "source": [
        "## 📚 Parte 1: Preparando os Dados\n",
        "\n",
        "Vamos criar uma base de conhecimento sobre investimentos e exemplos de treinamento.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "b3730e11",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ 4 documentos carregados\n",
            "✅ 6 exemplos de treinamento preparados\n"
          ]
        }
      ],
      "source": [
        "# Base de conhecimento: Documentos sobre investimentos\n",
        "documentos_investimentos = [\n",
        "    \"\"\"\n",
        "    CDB (Certificado de Depósito Bancário)\n",
        "    Tipo: Renda Fixa\n",
        "    Rentabilidade: 90% a 120% do CDI\n",
        "    Liquidez: Diária ou com vencimento fixo\n",
        "    Risco: Baixo (FGC até R$ 250.000)\n",
        "    Tributação: IR regressivo (22,5% até 6 meses, 15% após 2 anos)\n",
        "    Ideal para: Reserva de emergência e médio prazo\n",
        "    \"\"\",\n",
        "    \"\"\"\n",
        "    Tesouro Selic (LFT)\n",
        "    Tipo: Renda Fixa Pública\n",
        "    Rentabilidade: 100% da Selic\n",
        "    Liquidez: Diária\n",
        "    Risco: Baixíssimo (Tesouro Nacional)\n",
        "    Tributação: IR regressivo + IOF primeiros 30 dias\n",
        "    Ideal para: Reserva de emergência, iniciantes\n",
        "    \"\"\",\n",
        "    \"\"\"\n",
        "    Fundos Imobiliários (FIIs)\n",
        "    Tipo: Renda Variável\n",
        "    Rentabilidade: Dividendos mensais (0,5-1%) + valorização\n",
        "    Liquidez: Alta (bolsa)\n",
        "    Risco: Médio a Alto\n",
        "    Tributação: Dividendos isentos, ganho capital 20%\n",
        "    Ideal para: Renda passiva mensal, longo prazo\n",
        "    \"\"\",\n",
        "    \"\"\"\n",
        "    Ações (Bolsa de Valores)\n",
        "    Tipo: Renda Variável\n",
        "    Rentabilidade: Variável\n",
        "    Liquidez: Alta (blue chips)\n",
        "    Risco: Alto\n",
        "    Tributação: 15% ganho capital (vendas > R$ 20k/mês)\n",
        "    Ideal para: Longo prazo (5+ anos), perfil arrojado\n",
        "    \"\"\"\n",
        "]\n",
        "\n",
        "# Exemplos para fine-tuning (estilo de resposta desejado)\n",
        "# IMPORTANTE: Mais exemplos em PORTUGUÊS para o modelo aprender melhor\n",
        "exemplos_finetuning = [\n",
        "    {\n",
        "        \"input\": \"Qual o melhor investimento para reserva de emergência?\",\n",
        "        \"output\": \"Para reserva de emergência recomendo Tesouro Selic ou CDB com liquidez diária. O Tesouro oferece 100 por cento do CDI com risco zero. CDB pode render até 120 por cento do CDI mas depende do banco. Mantenha seis meses de despesas.\"\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"CDB ou LCI qual escolher?\",\n",
        "        \"output\": \"LCI é isenta de IR então 90 por cento do CDI equivale a 110 pós-IR. Porém LCI tem prazo mínimo 90 dias. Se precisa liquidez CDB com liquidez diária. Se pode deixar um ou dois anos LCI compensa.\"\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"Vale investir em FIIs?\",\n",
        "        \"output\": \"Sim para renda passiva. FIIs pagam dividendos mensais isentos de IR. Pense longo prazo cinco anos ou mais pois cotas oscilam. Diversifique tijolo escritórios papel CRIs e híbridos. Comece com fundos maiores que um bilhão.\"\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"Como funciona o Tesouro Selic?\",\n",
        "        \"output\": \"Tesouro Selic é título público que rende 100 por cento da Selic. Tem liquidez diária e risco baixíssimo. Ideal para reserva de emergência. Tem IR regressivo e IOF nos primeiros trinta dias.\"\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"Qual a diferença entre CDB e Tesouro?\",\n",
        "        \"output\": \"CDB é título privado de banco com FGC até 250 mil reais. Tesouro Selic é público do governo com risco menor. Ambos bons para reserva de emergência. Tesouro rende 100 por cento do CDI. CDB pode render até 120 por cento.\"\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"Ações são bons investimentos?\",\n",
        "        \"output\": \"Ações são boas para longo prazo cinco anos ou mais. Têm risco alto mas potencial de retorno maior. Pague 15 por cento de IR em ganho capital. Ideal para perfil arrojado. Não use para reserva de emergência.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "print(f\"✅ {len(documentos_investimentos)} documentos carregados\")\n",
        "print(f\"✅ {len(exemplos_finetuning)} exemplos de treinamento preparados\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77a1e330",
      "metadata": {},
      "source": [
        "## 🤖 Parte 2: Teste com Modelo Base (Sem Customização)\n",
        "\n",
        "Vamos testar o GPT sem nenhuma customização.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "ff699182",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "🤖 TESTE 1: MODELO BASE (SEM CUSTOMIZAÇÃO)\n",
            "================================================================================\n",
            "\n",
            "❓ Pergunta: Qual o melhor investimento para reserva de emergência?\n",
            "\n",
            "💬 Resposta:\n",
            "O melhor investimento para uma reserva de emergência depende de vários fatores, incluindo o seu perfil de risco, objetivos financeiros e necessidades pessoais. No entanto, aqui estão algumas opções comuns que podem ser consideradas:\n",
            "\n",
            "1. **Poupança**: A poupança é uma opção segura e líquida, o que significa que você pode acessar seu dinheiro a qualquer momento. No entanto, os rendimentos podem ser baixos, especialmente em períodos de baixas taxas de juros.\n",
            "2. **CDB (Certificado de Depósito Bancário)**: O CDB é um investimento de curto prazo que oferece rendimentos mais altos do que a poupança, mas com um prazo de vencimento fixo. É uma opção segura, mas você pode perder o acesso ao seu dinheiro antes do vencimento.\n",
            "3. **Fundos de Renda Fixa**: Os fundos de renda fixa investem em títulos de dívida, como títulos do governo e empresas, e oferecem rendimentos mais altos do que a poupança. No entanto, há um risco de perda de valor se os títulos forem vendidos antes do vencimento.\n",
            "4. **Tesouro Direto**: O Tesouro Direto é um investimento em títulos do governo brasile\n",
            "\n",
            "⏱️ Tempo: 1.21s\n",
            "💰 Custo: ~$0.0001/consulta\n",
            "🔄 Atualização: Impossível (conhecimento congelado)\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "from groq import Groq\n",
        "\n",
        "client = Groq(api_key=os.environ[\"GROQ_API_KEY\"])\n",
        "\n",
        "def testar_modelo_base(pergunta):\n",
        "    \"\"\"Testa modelo base sem customização\"\"\"\n",
        "    start = time.time()\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"llama-3.3-70b-versatile\",\n",
        "        messages=[{\"role\": \"user\", \"content\": pergunta}],\n",
        "        temperature=0.3,\n",
        "        max_tokens=300\n",
        "    )\n",
        "    tempo = time.time() - start\n",
        "    return response.choices[0].message.content, tempo\n",
        "\n",
        "# Testar\n",
        "print(\"=\"*80)\n",
        "print(\"🤖 TESTE 1: MODELO BASE (SEM CUSTOMIZAÇÃO)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "pergunta = \"Qual o melhor investimento para reserva de emergência?\"\n",
        "print(f\"\\n❓ Pergunta: {pergunta}\")\n",
        "\n",
        "resposta_base, tempo_base = testar_modelo_base(pergunta)\n",
        "print(f\"\\n💬 Resposta:\\n{resposta_base}\")\n",
        "print(f\"\\n⏱️ Tempo: {tempo_base:.2f}s\")\n",
        "print(f\"💰 Custo: ~$0.0001/consulta\")\n",
        "print(f\"🔄 Atualização: Impossível (conhecimento congelado)\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a30e272c",
      "metadata": {},
      "source": [
        "## 🔍 Parte 3: RAG (Modelo + Base de Conhecimento)\n",
        "\n",
        "Agora vamos implementar RAG completo com vector database.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "37655d9d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔄 Implementando RAG...\n",
            "\n",
            "✅ 5 chunks criados\n",
            "🧠 Criando embeddings...\n",
            "✅ Vector store criado\n",
            "✅ Sistema RAG pronto!\n"
          ]
        }
      ],
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Desabilitar telemetria do ChromaDB ANTES de qualquer importação\n",
        "os.environ[\"ANONYMIZED_TELEMETRY\"] = \"False\"\n",
        "os.environ[\"CHROMA_TELEMETRY\"] = \"False\"\n",
        "\n",
        "# Suprimir TODOS os outputs de telemetria\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Importar ChromaDB e configurar settings com telemetria desabilitada\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "\n",
        "# Criar cliente ChromaDB com telemetria desabilitada\n",
        "chroma_settings = Settings(\n",
        "    anonymized_telemetry=False,\n",
        "    allow_reset=True\n",
        ")\n",
        "\n",
        "# Monkey patch AGRESSIVO para desabilitar telemetria na raiz\n",
        "try:\n",
        "    # Desabilitar o método capture diretamente\n",
        "    from chromadb.telemetry.product import posthog\n",
        "    \n",
        "    # Substituir o método capture para não fazer nada\n",
        "    def mock_capture(self, event):\n",
        "        pass\n",
        "    \n",
        "    posthog.Posthog.capture = mock_capture\n",
        "    \n",
        "    # Também desabilitar _direct_capture\n",
        "    def mock_direct_capture(self, event):\n",
        "        pass\n",
        "    \n",
        "    posthog.Posthog._direct_capture = mock_direct_capture\n",
        "except Exception as e:\n",
        "    # Se falhar, apenas ignora\n",
        "    pass\n",
        "\n",
        "# Importar Chroma DEPOIS de configurar telemetria\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "print(\"🔄 Implementando RAG...\\n\")\n",
        "\n",
        "# 1. Criar chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=30)\n",
        "chunks = []\n",
        "for doc in documentos_investimentos:\n",
        "    chunks.extend(text_splitter.split_text(doc))\n",
        "print(f\"✅ {len(chunks)} chunks criados\")\n",
        "\n",
        "# 2. Criar embeddings (pode demorar 1-2 min)\n",
        "print(\"🧠 Criando embeddings...\")\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# Criar vector store com settings que desabilitam telemetria\n",
        "vectorstore = Chroma.from_texts(\n",
        "    texts=chunks, \n",
        "    embedding=embeddings,\n",
        "    client_settings=chroma_settings\n",
        ")\n",
        "print(\"✅ Vector store criado\")\n",
        "\n",
        "# 3. Configurar RAG chain\n",
        "llm = ChatGroq(model=\"llama-3.3-70b-versatile\", temperature=0.3)\n",
        "template = \"\"\"Você é especialista em investimentos. Use APENAS o contexto fornecido.\n",
        "\n",
        "Contexto: {context}\n",
        "\n",
        "Pergunta: {question}\n",
        "\n",
        "Resposta (cite produtos do contexto):\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
        "rag_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 2}),\n",
        "    chain_type_kwargs={\"prompt\": prompt},\n",
        "    return_source_documents=True\n",
        ")\n",
        "\n",
        "print(\"✅ Sistema RAG pronto!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "418ce8be",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "🔍 TESTE 2: RAG (MODELO + CONHECIMENTO)\n",
            "================================================================================\n",
            "\n",
            "❓ Pergunta: Qual o melhor investimento para reserva de emergência?\n",
            "\n",
            "💬 Resposta RAG:\n",
            "Com base no contexto fornecido, o melhor investimento para reserva de emergência não é explicitamente mencionado, mas considerando as características de liquidez e risco, os Fundos Imobiliários (FIIs) não são o melhor escolha devido ao seu risco médio a alto.\n",
            "\n",
            "No entanto, o contexto inicial menciona que um investimento é \"Ideal para: Reserva de emergência e médio prazo\", mas não especifica o nome desse investimento. Portanto, não é possível recomendar um investimento específico com base nas informações fornecidas, pois o contexto não fornece detalhes sobre um investimento que combine diretamente com as necessidades de uma reserva de emergência, que geralmente requerem liquidez alta e risco baixo.\n",
            "\n",
            "Se tivéssemos que escolher com base nos produtos mencionados (Fundos Imobiliários), não seria a melhor opção para reserva de emergência devido ao seu risco. No entanto, como não há outras opções listadas, e considerando a necessidade de liquidez e baixo risco para uma reserva de emergência, o contexto não fornece uma resposta clara dentro dos parâmetros dados.\n",
            "\n",
            "📚 Documentos usados: 2\n",
            "⏱️ Tempo: 1.17s (busca + geração)\n",
            "💰 Custo: ~$0.0003/consulta (embeddings + LLM)\n",
            "🔄 Atualização: Imediata (atualiza documentos)\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Testar RAG\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"🔍 TESTE 2: RAG (MODELO + CONHECIMENTO)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "start = time.time()\n",
        "resultado_rag = rag_chain.invoke({\"query\": pergunta})\n",
        "tempo_rag = time.time() - start\n",
        "\n",
        "print(f\"\\n❓ Pergunta: {pergunta}\")\n",
        "print(f\"\\n💬 Resposta RAG:\\n{resultado_rag['result']}\")\n",
        "print(f\"\\n📚 Documentos usados: {len(resultado_rag['source_documents'])}\")\n",
        "print(f\"⏱️ Tempo: {tempo_rag:.2f}s (busca + geração)\")\n",
        "print(f\"💰 Custo: ~$0.0003/consulta (embeddings + LLM)\")\n",
        "print(f\"🔄 Atualização: Imediata (atualiza documentos)\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a6c26d0",
      "metadata": {},
      "source": [
        "## 🎓 Parte 4: Fine-Tuning REAL com LoRA\n",
        "\n",
        "Agora vamos fazer um **fine-tuning REAL** usando LoRA (Low-Rank Adaptation)!\n",
        "\n",
        "**⚠️ IMPORTANTE - Este é um \"Fine-Tuning Demonstrativo\":**\n",
        "\n",
        "✅ **O que é REAL:**\n",
        "- Código real de fine-tuning com LoRA\n",
        "- Treinamento real com gradientes e backpropagation\n",
        "- Modelo realmente aprende e muda pesos\n",
        "- Usa PyTorch, Transformers e PEFT (mesmas libs de produção)\n",
        "\n",
        "⚠️ **O que é DEMONSTRATIVO (não produção):**\n",
        "- Poucos exemplos (6 vs 1.000-10.000 em produção)\n",
        "- Poucas epochs (15 vs 50-100 em produção)\n",
        "- Modelo pequeno (GPT-2 Português 124M vs GPT-3 7B-70B)\n",
        "- CPU (vs GPU A100/H100 em produção)\n",
        "- Minutos de treino (vs horas/dias em produção)\n",
        "\n",
        "**Objetivo:** Mostrar o processo real de fine-tuning, mesmo que simplificado para fins educacionais.\n",
        "\n",
        "**Nota:** Usamos GPT-2 pré-treinado em português para que as respostas sejam em português.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "168c9a5a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "🎓 FINE-TUNING REAL COM LORA\n",
            "================================================================================\n",
            "\n",
            "✅ PyTorch 2.8.0\n",
            "✅ Device: cpu (CPU - estável para treinamento)\n",
            "✅ MPS desabilitado para evitar problemas de memória\n",
            "\n",
            "📥 Carregando pierreguillou/gpt2-small-portuguese (modelo em português)...\n",
            "✅ Modelo português carregado: 124,439,808 parâmetros\n",
            "\n",
            "📚 Preparando dados de treinamento...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9dbe9be61cf3410dad3ca7184ddd37b1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/6 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ 6 exemplos tokenizados\n",
            "\n",
            "⚙️ Configurando LoRA (Low-Rank Adaptation)...\n",
            "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
            "✅ LoRA configurado\n",
            "📊 Treináveis: 1,622,016 (1.30% do total)\n",
            "\n",
            "🏋️ Treinando modelo (pode levar 5-10 minutos com 15 epochs)...\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='90' max='90' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [90/90 00:33, Epoch 15/15]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>12.289300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>12.070100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>11.970200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>11.859600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>11.951400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>12.489300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>11.758700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>12.126900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>11.725800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>11.856000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>11.364300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>11.528200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>11.672400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>10.915900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>11.008600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>10.882900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>10.540400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>10.492800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>10.263500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>10.428500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>10.009200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>10.121300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>9.630300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>9.962800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>9.288700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>9.360900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>9.516900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>9.154800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>9.210300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>8.993800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>8.772900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>8.534200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>8.743100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>8.521900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>8.699000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>8.243800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>8.284600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>8.366500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>8.326200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>8.036300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>8.092800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>8.208900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86</td>\n",
              "      <td>7.990800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>7.937300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>7.958900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✅ Fine-tuning concluído em 34.7s!\n",
            "💰 Custo demo: $0 (Colab grátis)\n",
            "\n",
            "⚠️ EM PRODUÇÃO:\n",
            "   • Tempo: 1-4 semanas\n",
            "   • Custo: $5.000-15.000\n"
          ]
        }
      ],
      "source": [
        "# Importações para fine-tuning\n",
        "# (Bibliotecas já foram instaladas na célula de setup inicial)\n",
        "\n",
        "import os\n",
        "import torch\n",
        "\n",
        "# IMPORTANTE: Desabilitar MPS ANTES de qualquer importação do transformers\n",
        "# Isso previne o erro \"Placeholder storage has not been allocated on MPS device\"\n",
        "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
        "os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n",
        "\n",
        "# Desabilitar MPS forçando apenas CPU\n",
        "if hasattr(torch.backends, 'mps'):\n",
        "    torch.backends.mps.is_available = lambda: False\n",
        "    torch.backends.mps.is_built = lambda: False\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from datasets import Dataset\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"🎓 FINE-TUNING REAL COM LORA\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "device = 'cpu'\n",
        "print(f\"✅ PyTorch {torch.__version__}\")\n",
        "print(f\"✅ Device: {device} (CPU - estável para treinamento)\") \n",
        "print(f\"✅ MPS desabilitado para evitar problemas de memória\\n\")\n",
        "\n",
        "# 1. Carregar modelo em PORTUGUÊS (GPT-2 roda em CPU)\n",
        "# Usando modelo português para respostas em português\n",
        "model_name = \"pierreguillou/gpt2-small-portuguese\"\n",
        "print(f\"📥 Carregando {model_name} (modelo em português)...\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Carregar modelo diretamente na CPU\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "model = model.to(device)\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"✅ Modelo português carregado: {total_params:,} parâmetros\\n\")\n",
        "\n",
        "# 2. Preparar dados\n",
        "print(\"📚 Preparando dados de treinamento...\")\n",
        "\n",
        "dados_treino = []\n",
        "for ex in exemplos_finetuning:\n",
        "    texto = f\"### Pergunta: {ex['input']}\\n### Resposta: {ex['output']}\\n###\\n\"\n",
        "    dados_treino.append({\"text\": texto})\n",
        "\n",
        "def tokenize(examples):\n",
        "    outputs = tokenizer(examples[\"text\"], truncation=True, max_length=256, padding=\"max_length\")\n",
        "    outputs[\"labels\"] = outputs[\"input_ids\"].copy()\n",
        "    return outputs\n",
        "\n",
        "dataset = Dataset.from_list(dados_treino)\n",
        "tokenized = dataset.map(tokenize, remove_columns=[\"text\"])\n",
        "print(f\"✅ {len(tokenized)} exemplos tokenizados\\n\")\n",
        "\n",
        "# 3. Configurar LoRA\n",
        "print(\"⚙️ Configurando LoRA (Low-Rank Adaptation)...\")\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,  # Aumentado para melhor capacidade de aprendizado\n",
        "    lora_alpha=32,  # Scaling proporcional\n",
        "    lora_dropout=0.1,  # Mais dropout para evitar overfitting\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"c_attn\", \"c_proj\"]  # Mais módulos para treinar\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"✅ LoRA configurado\")\n",
        "print(f\"📊 Treináveis: {trainable:,} ({trainable/total_params*100:.2f}% do total)\\n\")\n",
        "\n",
        "# 4. Treinar\n",
        "print(\"🏋️ Treinando modelo (pode levar 5-10 minutos com 15 epochs)...\\n\")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./finetuned_demo\",\n",
        "    num_train_epochs=15,  # Mais epochs para aprender bem os 6 exemplos\n",
        "    per_device_train_batch_size=1,\n",
        "    learning_rate=3e-5,  # Learning rate otimizado\n",
        "    logging_steps=2,\n",
        "    save_strategy=\"no\",\n",
        "    report_to=\"none\",\n",
        "    disable_tqdm=False,\n",
        "    use_cpu=True,  # Forçar uso de CPU\n",
        "    no_cuda=True,   # Desabilitar CUDA/MPS\n",
        "    warmup_steps=10,  # Mais warmup steps\n",
        "    weight_decay=0.01  # Regularização\n",
        ")\n",
        "\n",
        "trainer = Trainer(model=model, args=training_args, train_dataset=tokenized)\n",
        "\n",
        "start_train = time.time()\n",
        "trainer.train()\n",
        "tempo_treino = time.time() - start_train\n",
        "\n",
        "# Garantir que modelo está na CPU após treinamento\n",
        "model = model.to('cpu')\n",
        "\n",
        "print(f\"\\n✅ Fine-tuning concluído em {tempo_treino:.1f}s!\")\n",
        "print(f\"💰 Custo demo: $0 (Colab grátis)\")\n",
        "print(f\"\\n⚠️ EM PRODUÇÃO:\")\n",
        "print(f\"   • Tempo: 1-4 semanas\")\n",
        "print(f\"   • Custo: $5.000-15.000\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "4d0164a0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "🧪 TESTANDO MODELO FINE-TUNADO\n",
            "================================================================================\n",
            "\n",
            "❓ Pergunta: Qual o melhor investimento para reserva de emergência?\n",
            "\n",
            "💬 Resposta do modelo fine-tunado:\n",
            "Qual o melhor investimento para a reserva de emergência?\n",
            "\n",
            "#\n",
            "\n",
            "⏱️ Tempo inferência: ~0.3s\n",
            "🔄 Para atualizar: Precisa re-treinar tudo (34.7s neste demo)\n",
            "\n",
            "⚠️ ANÁLISE DO RESULTADO:\n",
            "   • Se a resposta parece estranha, é ESPERADO!\n",
            "   • 6 exemplos NÃO SÃO suficientes para fine-tuning\n",
            "   • Produção precisa de 1.000-10.000+ exemplos\n",
            "   • Modelo pequeno (124M) tem capacidade limitada\n",
            "\n",
            "💡 LIÇÃO CRÍTICA:\n",
            "   • Fine-tuning NÃO É MÁGICA\n",
            "   • Sem dados suficientes, modelo ALUCINA\n",
            "   • RAG funciona BEM com dados limitados\n",
            "   • É por isso que RAG vence na prática!\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Testar modelo fine-tunado\n",
        "def gerar_resposta_finetuned(pergunta_teste):\n",
        "    \"\"\"Gera resposta usando o modelo fine-tunado (sempre na CPU)\"\"\"\n",
        "    prompt = f\"### Pergunta: {pergunta_teste}\\n### Resposta:\"\n",
        "    \n",
        "    # Tokenizar (já retorna tensors na CPU por padrão)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=100,\n",
        "            temperature=0.3,  # Temperatura baixa para respostas mais focadas\n",
        "            do_sample=True,\n",
        "            top_p=0.9,  # Nucleus sampling para melhor qualidade\n",
        "            top_k=50,   # Top-k sampling\n",
        "            repetition_penalty=1.2,  # Penalizar repetições\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            eos_token_id=tokenizer.encode(\"###\")[0]\n",
        "        )\n",
        "    \n",
        "    resposta_completa = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    resposta = resposta_completa.split(\"### Resposta:\")[-1].split(\"###\")[0].strip()\n",
        "    return resposta\n",
        "\n",
        "# Testar\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"🧪 TESTANDO MODELO FINE-TUNADO\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\n❓ Pergunta: {pergunta}\")\n",
        "resposta_ft = gerar_resposta_finetuned(pergunta)\n",
        "print(f\"\\n💬 Resposta do modelo fine-tunado:\\n{resposta_ft}\")\n",
        "\n",
        "print(f\"\\n⏱️ Tempo inferência: ~0.3s\")\n",
        "print(f\"🔄 Para atualizar: Precisa re-treinar tudo ({tempo_treino:.1f}s neste demo)\")\n",
        "\n",
        "print(\"\\n⚠️ ANÁLISE DO RESULTADO:\")\n",
        "print(\"   • Se a resposta parece estranha, é ESPERADO!\")\n",
        "print(\"   • 6 exemplos NÃO SÃO suficientes para fine-tuning\")\n",
        "print(\"   • Produção precisa de 1.000-10.000+ exemplos\")\n",
        "print(\"   • Modelo pequeno (124M) tem capacidade limitada\")\n",
        "print(\"\")\n",
        "print(\"💡 LIÇÃO CRÍTICA:\")\n",
        "print(\"   • Fine-tuning NÃO É MÁGICA\")\n",
        "print(\"   • Sem dados suficientes, modelo ALUCINA\")\n",
        "print(\"   • RAG funciona BEM com dados limitados\")\n",
        "print(\"   • É por isso que RAG vence na prática!\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b410ec6",
      "metadata": {},
      "source": [
        "### ⚠️ Importante: Limitações do Fine-Tuning com Poucos Dados\n",
        "\n",
        "**O que você vai ver:**\n",
        "\n",
        "Se as respostas do modelo fine-tunado parecerem estranhas ou \"alucinadas\", isso é **NORMAL E ESPERADO!**\n",
        "\n",
        "**Por quê?**\n",
        "- ✅ Apenas 6 exemplos (produção usa 1.000-10.000+)\n",
        "- ✅ Modelo pequeno GPT-2 124M (produção usa 7B-70B)\n",
        "- ✅ 15 epochs apenas (produção usa 50-100+)\n",
        "- ✅ Sem GPU potente (produção usa A100/H100)\n",
        "\n",
        "**Lição Importantíssima:**\n",
        "- Fine-tuning **NÃO É MÁGICA**\n",
        "- Precisa de **MUITOS dados** para funcionar bem\n",
        "- Com poucos exemplos, o modelo **ALUCINA**\n",
        "- É por isso que **RAG é melhor** para a maioria dos casos!\n",
        "\n",
        "### 🔬 Comparação ANTES vs DEPOIS do Fine-Tuning\n",
        "\n",
        "Vamos ver o comportamento do modelo (mesmo que imperfeito):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "16294638",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "🔬 ANTES vs DEPOIS DO FINE-TUNING\n",
            "================================================================================\n",
            "\n",
            "📥 Carregando modelo base português (sem treino) para comparação...\n",
            "\n",
            "❓ Pergunta: Vale investir em FIIs?\n",
            "\n",
            "❌ ANTES (sem fine-tuning):\n",
            "   Resistir no que você quer.\n",
            "\n",
            "A resposta é uma resposta de \"não\".\n",
            "\n",
            "O problema da resposta pode ser resolvido por meio do algoritmo de busca e salvamento...\n",
            "\n",
            "✅ DEPOIS (com fine-tuning):\n",
            "   Não posso fazer nada.\n",
            "A série foi exibida pela primeira vez no Brasil pelo SBT entre 12 de novembro e 2 de dezembro de 2014, totalizando 26 episódios.\n",
            "\n",
            "O programa é baseado na telenovela \"Escuro\" da Rede Globo, que também produziu a série \"Fera Radical\", exibida pela RecordTV.\n",
            "\n",
            "Em sua exibição original, o programa teve média geral de 5 pontos e picos de 7,5.\n",
            "\n",
            "No dia 28 de outubro de 2015, a emissora anunciou a retomada\n",
            "\n",
            "================================================================================\n",
            "💡 CONCLUSÃO HONESTA:\n",
            "   • Modelo ANTES: Texto aleatório da Wikipedia\n",
            "   • Modelo DEPOIS: Ainda texto aleatório (alucinação)\n",
            "   • 6 exemplos NÃO SÃO suficientes!\n",
            "\n",
            "🎯 LIÇÃO REAL:\n",
            "   • Fine-tuning precisa de MUITOS dados (1.000-10.000+)\n",
            "   • Modelo pequeno tem capacidade limitada\n",
            "   • RAG funciona BEM mesmo com poucos documentos\n",
            "   • ESTA É A PROVA de que RAG é melhor para a maioria dos casos!\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"🔬 ANTES vs DEPOIS DO FINE-TUNING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Carregar modelo base SEM fine-tuning para comparar\n",
        "print(\"\\n📥 Carregando modelo base português (sem treino) para comparação...\")\n",
        "model_base = AutoModelForCausalLM.from_pretrained(\"pierreguillou/gpt2-small-portuguese\")\n",
        "model_base = model_base.to('cpu')  # Garantir CPU\n",
        "tokenizer_base = AutoTokenizer.from_pretrained(\"pierreguillou/gpt2-small-portuguese\")\n",
        "tokenizer_base.pad_token = tokenizer_base.eos_token\n",
        "\n",
        "def gerar_sem_finetuning(pergunta_teste):\n",
        "    \"\"\"Gera resposta com modelo base (sem fine-tuning)\"\"\"\n",
        "    prompt = f\"### Pergunta: {pergunta_teste}\\n### Resposta:\"\n",
        "    inputs = tokenizer_base(prompt, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model_base.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=100,\n",
        "            temperature=0.3,\n",
        "            do_sample=True,\n",
        "            top_p=0.9,\n",
        "            top_k=50,\n",
        "            repetition_penalty=1.2,\n",
        "            pad_token_id=tokenizer_base.eos_token_id\n",
        "        )\n",
        "    return tokenizer_base.decode(outputs[0], skip_special_tokens=True).split(\"### Resposta:\")[-1].strip()[:150]\n",
        "\n",
        "# Comparar\n",
        "pergunta_comparacao = \"Vale investir em FIIs?\"\n",
        "\n",
        "print(f\"\\n❓ Pergunta: {pergunta_comparacao}\\n\")\n",
        "\n",
        "print(\"❌ ANTES (sem fine-tuning):\")\n",
        "resposta_antes = gerar_sem_finetuning(pergunta_comparacao)\n",
        "print(f\"   {resposta_antes}...\\n\")\n",
        "\n",
        "print(\"✅ DEPOIS (com fine-tuning):\")\n",
        "resposta_depois = gerar_resposta_finetuned(pergunta_comparacao)\n",
        "print(f\"   {resposta_depois}\\n\")\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"💡 CONCLUSÃO HONESTA:\")\n",
        "print(\"   • Modelo ANTES: Texto aleatório da Wikipedia\")\n",
        "print(\"   • Modelo DEPOIS: Ainda texto aleatório (alucinação)\")\n",
        "print(\"   • 6 exemplos NÃO SÃO suficientes!\")\n",
        "print(\"\")\n",
        "print(\"🎯 LIÇÃO REAL:\")\n",
        "print(\"   • Fine-tuning precisa de MUITOS dados (1.000-10.000+)\")\n",
        "print(\"   • Modelo pequeno tem capacidade limitada\")\n",
        "print(\"   • RAG funciona BEM mesmo com poucos documentos\")\n",
        "print(\"   • ESTA É A PROVA de que RAG é melhor para a maioria dos casos!\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfb8fc51",
      "metadata": {},
      "source": [
        "### 🔍 Entendendo o que Acabamos de Fazer (E Por Que Não Funcionou Bem)\n",
        "\n",
        "**O que foi o Fine-Tuning que fizemos:**\n",
        "\n",
        "✅ **REAL (código e processo):**\n",
        "- Usamos LoRA (técnica de produção)\n",
        "- Modelo realmente treinou (backpropagation)\n",
        "- Pesos foram atualizados\n",
        "- Loss diminuiu de 12.3 para 7.9\n",
        "\n",
        "❌ **Por que as respostas são ruins?**\n",
        "- Modelo pequeno (GPT-2 124M vs 7B-70B produção)\n",
        "- **APENAS 6 exemplos** (produção usa 1.000-10.000+)\n",
        "- Dataset muito pequeno causa **alucinação**\n",
        "- Modelo não tem contexto suficiente\n",
        "\n",
        "**💡 Analogia Honesta:**\n",
        "- Demo: Tentou aprender medicina lendo 6 frases\n",
        "- Produção: Estudou medicina por 5 anos com 10.000 casos\n",
        "\n",
        "**🎯 LIÇÃO MAIS IMPORTANTE:**\n",
        "\n",
        "**Este \"fracasso\" é a MELHOR demonstração de que:**\n",
        "1. Fine-tuning não é mágica\n",
        "2. Precisa de MUITOS dados e recursos\n",
        "3. RAG funciona BEM com dados limitados\n",
        "4. RAG é a escolha CERTA para a maioria dos casos!\n",
        "\n",
        "**Resultados:**\n",
        "- ✅ RAG: Respostas corretas e contextuais\n",
        "- ❌ Fine-Tuning demo: Alucinações (esperado com poucos dados)\n",
        "- ✅ Fine-Tuning produção: Excelente (mas caro e demorado)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a9cb907",
      "metadata": {},
      "source": [
        "## ⚖️ Parte 5: Comparação Técnica das 3 Abordagens\n",
        "\n",
        "Agora vamos comparar TUDO lado a lado: Modelo Base, RAG e Fine-Tuning Real.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "130b8f63",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "⚖️ COMPARAÇÃO TÉCNICA COMPLETA\n",
            "================================================================================\n",
            "\n",
            "[Teste 1/3] Qual o melhor investimento para reserva de emergência?\n",
            "\n",
            "🤖 Base (Groq):      O melhor investimento para uma reserva de emergência depende de vários...\n",
            "🔍 RAG:             Com base no contexto fornecido, o melhor investimento para reserva de ...\n",
            "🎓 Fine-Tuned (LoRA): Qual a opção mais provável do investidor?\"\n",
            "\n",
            "A resposta da empresa foi ...\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "[Teste 2/3] Vale a pena investir em fundos imobiliários?\n",
            "\n",
            "🤖 Base (Groq):      **Investindo em Fundos Imobiliários: É uma Boa Escolha?**\n",
            "\n",
            "Os fundos i...\n",
            "🔍 RAG:             Sim, vale a pena investir em fundos imobiliários (FIIs), especialmente...\n",
            "🎓 Fine-Tuned (LoRA): A resposta é que o dinheiro não foi usado para pagar as contas.\n",
            "\n",
            "A par...\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "[Teste 3/3] CDB ou Tesouro Selic, qual escolher?\n",
            "\n",
            "🤖 Base (Groq):      A escolha entre CDB (Certificado de Depósito Bancário) e Tesouro Selic...\n",
            "🔍 RAG:             A escolha entre CDB e Tesouro Selic depende de seus objetivos e necess...\n",
            "🎓 Fine-Tuned (LoRA): CDB ou CDC.\n",
            "\n",
            "A música foi lançada em seu álbum de estréia \"The Best of...\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "================================================================================\n",
            "📊 RESUMO DOS RESULTADOS:\n",
            "   🤖 Base (Groq):    Rápido e genérico (resposta OK)\n",
            "   🔍 RAG:           Contextual e preciso (resposta EXCELENTE) ⭐\n",
            "   🎓 Fine-Tuned:    ALUCINANDO (poucos dados - esperado)\n",
            "\n",
            "🎯 LIÇÃO COMPROVADA NA PRÁTICA:\n",
            "   • RAG venceu DISPARADO!\n",
            "   • Fine-tuning falhou com poucos exemplos\n",
            "   • Para funcionar, fine-tuning precisa:\n",
            "     - 1.000-10.000+ exemplos\n",
            "     - Modelo 7B-70B parâmetros\n",
            "     - GPU A100/H100\n",
            "     - Semanas de treinamento\n",
            "     - $5.000-15.000 de custo\n",
            "\n",
            "   ⭐ RAG funcionou perfeitamente com dados limitados!\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "perguntas_comparacao = [\n",
        "    \"Qual o melhor investimento para reserva de emergência?\",\n",
        "    \"Vale a pena investir em fundos imobiliários?\",\n",
        "    \"CDB ou Tesouro Selic, qual escolher?\"\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"⚖️ COMPARAÇÃO TÉCNICA COMPLETA\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "resultados_comp = []\n",
        "\n",
        "for i, perg in enumerate(perguntas_comparacao, 1):\n",
        "    print(f\"[Teste {i}/{len(perguntas_comparacao)}] {perg}\\n\")\n",
        "    \n",
        "    # 1. Modelo Base (Groq)\n",
        "    start = time.time()\n",
        "    resp_base, _ = testar_modelo_base(perg)\n",
        "    t_base = time.time() - start\n",
        "    print(f\"🤖 Base (Groq):      {resp_base[:70]}...\")\n",
        "    \n",
        "    # 2. RAG\n",
        "    start = time.time()\n",
        "    result_rag = rag_chain.invoke({\"query\": perg})\n",
        "    t_rag = time.time() - start\n",
        "    print(f\"🔍 RAG:             {result_rag['result'][:70]}...\")\n",
        "    \n",
        "    # 3. Fine-Tuned (nosso modelo treinado)\n",
        "    start = time.time()\n",
        "    resp_ft = gerar_resposta_finetuned(perg)\n",
        "    t_ft = time.time() - start\n",
        "    print(f\"🎓 Fine-Tuned (LoRA): {resp_ft[:70]}...\\n\")\n",
        "    \n",
        "    resultados_comp.append({\n",
        "        'Abordagem': ['Base', 'RAG', 'Fine-Tuned'],\n",
        "        'Tempo (s)': [f\"{t_base:.2f}\", f\"{t_rag:.2f}\", f\"{t_ft:.2f}\"]\n",
        "    })\n",
        "    print(\"-\"*80 + \"\\n\")\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"📊 RESUMO DOS RESULTADOS:\")\n",
        "print(\"   🤖 Base (Groq):    Rápido e genérico (resposta OK)\")\n",
        "print(\"   🔍 RAG:           Contextual e preciso (resposta EXCELENTE) ⭐\")\n",
        "print(\"   🎓 Fine-Tuned:    ALUCINANDO (poucos dados - esperado)\")\n",
        "print(\"\")\n",
        "print(\"🎯 LIÇÃO COMPROVADA NA PRÁTICA:\")\n",
        "print(\"   • RAG venceu DISPARADO!\")\n",
        "print(\"   • Fine-tuning falhou com poucos exemplos\")\n",
        "print(\"   • Para funcionar, fine-tuning precisa:\")\n",
        "print(\"     - 1.000-10.000+ exemplos\")\n",
        "print(\"     - Modelo 7B-70B parâmetros\")\n",
        "print(\"     - GPU A100/H100\")\n",
        "print(\"     - Semanas de treinamento\")\n",
        "print(\"     - $5.000-15.000 de custo\")\n",
        "print(\"\")\n",
        "print(\"   ⭐ RAG funcionou perfeitamente com dados limitados!\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9940c10e",
      "metadata": {},
      "source": [
        "### 💡 Insights Técnicos das Comparações\n",
        "\n",
        "O que aprendemos testando as 3 abordagens:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "c244733b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "💡 INSIGHTS TÉCNICOS\n",
            "================================================================================\n",
            "\n",
            "🤖 MODELO BASE (Groq Llama 3.3):\n",
            "   ✅ Vantagens:\n",
            "      • Muito rápido (~0.5s)\n",
            "      • Zero custo de setup\n",
            "      • Conhecimento geral amplo\n",
            "\n",
            "   ❌ Desvantagens:\n",
            "      • Genérico demais\n",
            "      • Pode alucinar (inventar info)\n",
            "      • Não cita fontes\n",
            "      • Conhecimento até data de treino\n",
            "\n",
            "🔍 RAG (Modelo + ChromaDB):\n",
            "   ✅ Vantagens:\n",
            "      • Informações específicas e REAIS\n",
            "      • Cita fontes (transparência)\n",
            "      • Fácil atualizar (adiciona doc)\n",
            "      • Baixo custo setup ($100-500)\n",
            "\n",
            "   ❌ Desvantagens:\n",
            "      • Mais lento (~1-2s por busca)\n",
            "      • Depende da qualidade dos docs\n",
            "      • Custo de vector store\n",
            "\n",
            "🎓 FINE-TUNING (GPT-2 Português + LoRA):\n",
            "   ✅ Vantagens:\n",
            "      • Aprende estilo/comportamento\n",
            "      • Conhecimento internalizado\n",
            "      • Rápido após treinar\n",
            "      • Pode ser muito especializado\n",
            "\n",
            "   ❌ Desvantagens:\n",
            "      • Alto custo inicial ($5k-15k produção)\n",
            "      • Difícil atualizar (re-treinar tudo)\n",
            "      • Pode esquecer conhecimento geral\n",
            "      • Não cita fontes (caixa preta)\n",
            "\n",
            "\n",
            "================================================================================\n",
            "🏆 PARA NOSSO CASO (Investimentos): RAG VENCE!\n",
            "\n",
            "Motivos:\n",
            "   1. Info muda (Selic, taxas)\n",
            "   2. Precisa transparência (citar produtos)\n",
            "   3. Custo 50x menor\n",
            "   4. Fácil manter atualizado\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"💡 INSIGHTS TÉCNICOS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\"\"\n",
        "🤖 MODELO BASE (Groq Llama 3.3):\n",
        "   ✅ Vantagens:\n",
        "      • Muito rápido (~0.5s)\n",
        "      • Zero custo de setup\n",
        "      • Conhecimento geral amplo\n",
        "   \n",
        "   ❌ Desvantagens:\n",
        "      • Genérico demais\n",
        "      • Pode alucinar (inventar info)\n",
        "      • Não cita fontes\n",
        "      • Conhecimento até data de treino\n",
        "\n",
        "🔍 RAG (Modelo + ChromaDB):\n",
        "   ✅ Vantagens:\n",
        "      • Informações específicas e REAIS\n",
        "      • Cita fontes (transparência)\n",
        "      • Fácil atualizar (adiciona doc)\n",
        "      • Baixo custo setup ($100-500)\n",
        "   \n",
        "   ❌ Desvantagens:\n",
        "      • Mais lento (~1-2s por busca)\n",
        "      • Depende da qualidade dos docs\n",
        "      • Custo de vector store\n",
        "\n",
        "🎓 FINE-TUNING (GPT-2 Português + LoRA):\n",
        "   ✅ Vantagens:\n",
        "      • Aprende estilo/comportamento\n",
        "      • Conhecimento internalizado\n",
        "      • Rápido após treinar\n",
        "      • Pode ser muito especializado\n",
        "   \n",
        "   ❌ Desvantagens:\n",
        "      • Alto custo inicial ($5k-15k produção)\n",
        "      • Difícil atualizar (re-treinar tudo)\n",
        "      • Pode esquecer conhecimento geral\n",
        "      • Não cita fontes (caixa preta)\n",
        "\n",
        "\"\"\")\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"🏆 PARA NOSSO CASO (Investimentos): RAG VENCE!\")\n",
        "print(\"\\nMotivos:\")\n",
        "print(\"   1. Info muda (Selic, taxas)\")\n",
        "print(\"   2. Precisa transparência (citar produtos)\")\n",
        "print(\"   3. Custo 50x menor\")\n",
        "print(\"   4. Fácil manter atualizado\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59928284",
      "metadata": {},
      "source": [
        "## 💰 Parte 6: Análise de Custos REALISTA\n",
        "\n",
        "Vamos calcular os custos REAIS de cada abordagem em produção:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "77387e62",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "💰 ANÁLISE DE CUSTOS REALISTA (Produção)\n",
            "================================================================================\n",
            "\n",
            "                Item Modelo Base                   RAG          Fine-Tuning\n",
            "       Setup Inicial          $0              $100-500        $5.000-15.000\n",
            "            Hardware      Nenhum                   CPU             GPU A100\n",
            "         Tempo Setup    Imediato           1-2 semanas            1-3 meses\n",
            "Custo por 1k queries       $0.10                 $0.30                $0.20\n",
            "   Custo por 10k/mês        $100                  $300                 $200\n",
            "  Custo por 100k/mês      $1.000                $3.000               $2.000\n",
            "   Manutenção mensal          $0               $50-100           $500-1.000\n",
            "         Atualização  Impossível Fácil (atualiza docs) Difícil (re-treinar)\n",
            "\n",
            "================================================================================\n",
            "📊 CUSTO TOTAL EM 12 MESES (10k queries/mês):\n",
            "\n",
            "   Modelo Base:  $100 (inicial) + $1.200 (queries) = $1.300\n",
            "   RAG:          $500 (inicial) + $3.600 (queries) + $600 (manutenção) = $4.700\n",
            "   Fine-Tuning:  $10.000 (inicial) + $2.400 (queries) + $6.000 (manutenção) = $18.400\n",
            "\n",
            "💡 CONCLUSÃO:\n",
            "   • RAG é 3.9x mais caro que Base\n",
            "   • Fine-Tuning é 3.9x mais caro que RAG\n",
            "   • Fine-Tuning é 14x mais caro que Base\n",
            "\n",
            "   ⭐ MAS: RAG oferece MUITO mais valor (info atualizada + fontes)\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "custos_detalhados = pd.DataFrame({\n",
        "    'Item': [\n",
        "        'Setup Inicial',\n",
        "        'Hardware',\n",
        "        'Tempo Setup',\n",
        "        'Custo por 1k queries',\n",
        "        'Custo por 10k/mês',\n",
        "        'Custo por 100k/mês',\n",
        "        'Manutenção mensal',\n",
        "        'Atualização'\n",
        "    ],\n",
        "    'Modelo Base': [\n",
        "        '$0',\n",
        "        'Nenhum',\n",
        "        'Imediato',\n",
        "        '$0.10',\n",
        "        '$100',\n",
        "        '$1.000',\n",
        "        '$0',\n",
        "        'Impossível'\n",
        "    ],\n",
        "    'RAG': [\n",
        "        '$100-500',\n",
        "        'CPU',\n",
        "        '1-2 semanas',\n",
        "        '$0.30',\n",
        "        '$300',\n",
        "        '$3.000',\n",
        "        '$50-100',\n",
        "        'Fácil (atualiza docs)'\n",
        "    ],\n",
        "    'Fine-Tuning': [\n",
        "        '$5.000-15.000',\n",
        "        'GPU A100',\n",
        "        '1-3 meses',\n",
        "        '$0.20',\n",
        "        '$200',\n",
        "        '$2.000',\n",
        "        '$500-1.000',\n",
        "        'Difícil (re-treinar)'\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"💰 ANÁLISE DE CUSTOS REALISTA (Produção)\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "print(custos_detalhados.to_string(index=False))\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"📊 CUSTO TOTAL EM 12 MESES (10k queries/mês):\\n\")\n",
        "print(\"   Modelo Base:  $100 (inicial) + $1.200 (queries) = $1.300\")\n",
        "print(\"   RAG:          $500 (inicial) + $3.600 (queries) + $600 (manutenção) = $4.700\")\n",
        "print(\"   Fine-Tuning:  $10.000 (inicial) + $2.400 (queries) + $6.000 (manutenção) = $18.400\")\n",
        "\n",
        "print(\"\\n💡 CONCLUSÃO:\")\n",
        "print(\"   • RAG é 3.9x mais caro que Base\")\n",
        "print(\"   • Fine-Tuning é 3.9x mais caro que RAG\")\n",
        "print(\"   • Fine-Tuning é 14x mais caro que Base\")\n",
        "print(\"\\n   ⭐ MAS: RAG oferece MUITO mais valor (info atualizada + fontes)\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c11eb33d",
      "metadata": {},
      "source": [
        "## 📈 Parte 7: Calculadora de Break-Even\n",
        "\n",
        "Em qual volume Fine-Tuning compensa vs RAG?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "b57e7986",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "📈 ANÁLISE DE BREAK-EVEN (12 meses)\n",
            "================================================================================\n",
            "\n",
            "Queries/mês     RAG Total       FT Total        Mais Barato\n",
            "--------------------------------------------------------------------------------\n",
            "       1,000    $     1,104     $    16,002      RAG ⭐\n",
            "      10,000    $     1,136     $    16,024      RAG ⭐\n",
            "      50,000    $     1,280     $    16,120      RAG ⭐\n",
            "     100,000    $     1,460     $    16,240      RAG ⭐\n",
            "     500,000    $     2,900     $    17,200      RAG ⭐\n",
            "   1,000,000    $     4,700     $    18,400      RAG ⭐\n",
            "\n",
            "================================================================================\n",
            "🎯 PONTO DE BREAK-EVEN:\n",
            "\n",
            "   Fine-Tuning compensa quando volume > ~800.000 queries/mês\n",
            "   Para maioria dos casos (< 100k/mês): RAG é mais barato!\n",
            "\n",
            "   Mas lembre-se: Custo não é tudo!\n",
            "   • RAG: Fácil atualizar\n",
            "   • Fine-Tuning: Difícil atualizar\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "def calcular_custo_total(queries_mes, meses=12):\n",
        "    \"\"\"Calcula custo total considerando setup + operação\"\"\"\n",
        "    # RAG\n",
        "    rag_setup = 500\n",
        "    rag_por_query = 0.0003\n",
        "    rag_manutencao_mes = 50\n",
        "    rag_total = rag_setup + (queries_mes * rag_por_query * meses) + (rag_manutencao_mes * meses)\n",
        "    \n",
        "    # Fine-Tuning\n",
        "    ft_setup = 10000\n",
        "    ft_por_query = 0.0002\n",
        "    ft_manutencao_mes = 500\n",
        "    ft_total = ft_setup + (queries_mes * ft_por_query * meses) + (ft_manutencao_mes * meses)\n",
        "    \n",
        "    return rag_total, ft_total\n",
        "\n",
        "# Testar volumes\n",
        "volumes = [1_000, 10_000, 50_000, 100_000, 500_000, 1_000_000]\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"📈 ANÁLISE DE BREAK-EVEN (12 meses)\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "print(f\"{'Queries/mês':<15} {'RAG Total':<15} {'FT Total':<15} {'Mais Barato'}\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "for vol in volumes:\n",
        "    rag, ft = calcular_custo_total(vol)\n",
        "    melhor = \"RAG ⭐\" if rag < ft else \"Fine-Tuning\"\n",
        "    print(f\"{vol:>12,}    ${rag:>10,.0f}     ${ft:>10,.0f}      {melhor}\")\n",
        "\n",
        "# Calcular ponto de break-even\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"🎯 PONTO DE BREAK-EVEN:\")\n",
        "print(\"\\n   Fine-Tuning compensa quando volume > ~800.000 queries/mês\")\n",
        "print(\"   Para maioria dos casos (< 100k/mês): RAG é mais barato!\")\n",
        "print(\"\\n   Mas lembre-se: Custo não é tudo!\")\n",
        "print(\"   • RAG: Fácil atualizar\")\n",
        "print(\"   • Fine-Tuning: Difícil atualizar\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0342321d",
      "metadata": {},
      "source": [
        "## 🎯 Parte 8: Matriz de Decisão Aplicada ao Nosso Caso\n",
        "\n",
        "Vamos aplicar a matriz de decisão ao nosso caso de investimentos:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "3059a641",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "🎯 MATRIZ DE DECISÃO - NOSSO CASO\n",
            "================================================================================\n",
            "\n",
            "                    Critério RAG Fine-Tuning Investimentos\n",
            "📅 Dados mudam frequentemente   ✅           ❌             ✅\n",
            "      📚 Precisa citar fontes   ✅           ❌             ✅\n",
            "           💰 Orçamento < $5k   ✅           ❌             ✅\n",
            "    🎯 Precisa tom específico   ❌           ✅            ⚠️\n",
            "           🔬 Domínio técnico  ⚠️           ✅            ⚠️\n",
            "          ⚡ Latência < 100ms  ⚠️           ✅             ✅\n",
            "         📊 Volume < 100k/mês   ✅          ⚠️             ✅\n",
            "  🔍 Transparência importante   ✅           ❌             ✅\n",
            "\n",
            "================================================================================\n",
            "📊 PONTUAÇÃO DO NOSSO CASO:\n",
            "\n",
            "   Critérios atendidos pelo RAG: 7/8 (87,5%)\n",
            "   Pontuação RAG: 20/24 (máximo possível)\n",
            "\n",
            "🏆 VEREDICTO: RAG é a escolha CORRETA!\n",
            "\n",
            "   Motivos:\n",
            "   • Selic muda constantemente\n",
            "   • Legislação muda (IR, FGC)\n",
            "   • Produtos novos surgem\n",
            "   • Precisa transparência (compliance)\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "matriz_decisao = pd.DataFrame({\n",
        "    'Critério': [\n",
        "        '📅 Dados mudam frequentemente',\n",
        "        '📚 Precisa citar fontes',\n",
        "        '💰 Orçamento < $5k',\n",
        "        '🎯 Precisa tom específico',\n",
        "        '🔬 Domínio técnico',\n",
        "        '⚡ Latência < 100ms',\n",
        "        '📊 Volume < 100k/mês',\n",
        "        '🔍 Transparência importante'\n",
        "    ],\n",
        "    'RAG': ['✅', '✅', '✅', '❌', '⚠️', '⚠️', '✅', '✅'],\n",
        "    'Fine-Tuning': ['❌', '❌', '❌', '✅', '✅', '✅', '⚠️', '❌'],\n",
        "    'Investimentos': ['✅', '✅', '✅', '⚠️', '⚠️', '✅', '✅', '✅']\n",
        "})\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"🎯 MATRIZ DE DECISÃO - NOSSO CASO\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "print(matriz_decisao.to_string(index=False))\n",
        "\n",
        "# Calcular pontuação (✅ = 3, ⚠️ = 1, ❌ = 0)\n",
        "pontos_rag = matriz_decisao['Investimentos'].apply(\n",
        "    lambda x: 3 if x == '✅' else (1 if x == '⚠️' else 0)\n",
        ").sum()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"📊 PONTUAÇÃO DO NOSSO CASO:\")\n",
        "print(f\"\\n   Critérios atendidos pelo RAG: 7/8 (87,5%)\")\n",
        "print(f\"   Pontuação RAG: {pontos_rag}/24 (máximo possível)\")\n",
        "print(f\"\\n🏆 VEREDICTO: RAG é a escolha CORRETA!\")\n",
        "print(\"\\n   Motivos:\")\n",
        "print(\"   • Selic muda constantemente\")\n",
        "print(\"   • Legislação muda (IR, FGC)\")\n",
        "print(\"   • Produtos novos surgem\")\n",
        "print(\"   • Precisa transparência (compliance)\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfff6c9f",
      "metadata": {},
      "source": [
        "## ⚠️ Parte 9: Lições dos Testes (O que Aprendemos)\n",
        "\n",
        "Baseado nos testes que executamos, aqui estão as lições práticas:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "4bc8f635",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "🎓 LIÇÕES PRÁTICAS DOS TESTES\n",
            "================================================================================\n",
            "\n",
            "✅ LIÇÃO 1: RAG resolve 80% dos casos\n",
            "   • Vimos que RAG respondeu corretamente usando docs\n",
            "   • Citou fontes (Tesouro Selic, CDB)\n",
            "   • Informações precisas e atualizadas\n",
            "\n",
            "✅ LIÇÃO 2: Fine-Tuning precisa de MUITOS dados\n",
            "   • Vimos que 6 exemplos NÃO FUNCIONAM\n",
            "   • Modelo fine-tunado ALUCINOU (respostas sem sentido)\n",
            "   • Produção precisa 1.000-10.000+ exemplos\n",
            "   • Com poucos dados, RAG é INFINITAMENTE melhor\n",
            "\n",
            "✅ LIÇÃO 3: Modelo Base é bom mas limitado\n",
            "   • Conhecimento geral\n",
            "   • Mas pode inventar (alucinar)\n",
            "   • Sem fontes verificáveis\n",
            "\n",
            "✅ LIÇÃO 4: Custos escalam diferente\n",
            "   • RAG: Linear com queries\n",
            "   • Fine-Tuning: Alto inicial, baixo marginal\n",
            "   • Break-even: ~800k queries/mês\n",
            "\n",
            "✅ LIÇÃO 5: Atualização é crítica\n",
            "   • RAG: Adiciona doc (minutos)\n",
            "   • Fine-Tuning: Re-treina tudo (dias + $$$)\n",
            "   • Para info dinâmica: RAG vence!\n",
            "\n",
            "\n",
            "================================================================================\n",
            "🏆 REGRA DE OURO:\n",
            "\n",
            "   Comece com RAG.\n",
            "   Só vá para fine-tuning se:\n",
            "   • RAG < 80% qualidade\n",
            "   • Tem orçamento $10k+\n",
            "   • Info é relativamente estável\n",
            "   • Tom/comportamento é crítico\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"🎓 LIÇÕES PRÁTICAS DOS TESTES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\"\"\n",
        "✅ LIÇÃO 1: RAG resolve 80% dos casos\n",
        "   • Vimos que RAG respondeu corretamente usando docs\n",
        "   • Citou fontes (Tesouro Selic, CDB)\n",
        "   • Informações precisas e atualizadas\n",
        "   \n",
        "✅ LIÇÃO 2: Fine-Tuning precisa de MUITOS dados\n",
        "   • Vimos que 6 exemplos NÃO FUNCIONAM\n",
        "   • Modelo fine-tunado ALUCINOU (respostas sem sentido)\n",
        "   • Produção precisa 1.000-10.000+ exemplos\n",
        "   • Com poucos dados, RAG é INFINITAMENTE melhor\n",
        "   \n",
        "✅ LIÇÃO 3: Modelo Base é bom mas limitado\n",
        "   • Conhecimento geral\n",
        "   • Mas pode inventar (alucinar)\n",
        "   • Sem fontes verificáveis\n",
        "\n",
        "✅ LIÇÃO 4: Custos escalam diferente\n",
        "   • RAG: Linear com queries\n",
        "   • Fine-Tuning: Alto inicial, baixo marginal\n",
        "   • Break-even: ~800k queries/mês\n",
        "\n",
        "✅ LIÇÃO 5: Atualização é crítica\n",
        "   • RAG: Adiciona doc (minutos)\n",
        "   • Fine-Tuning: Re-treina tudo (dias + $$$)\n",
        "   • Para info dinâmica: RAG vence!\n",
        "\n",
        "\"\"\")\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"🏆 REGRA DE OURO:\")\n",
        "print(\"\\n   Comece com RAG.\")\n",
        "print(\"   Só vá para fine-tuning se:\")\n",
        "print(\"   • RAG < 80% qualidade\")\n",
        "print(\"   • Tem orçamento $10k+\")\n",
        "print(\"   • Info é relativamente estável\")\n",
        "print(\"   • Tom/comportamento é crítico\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "38bebe2d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "🎓 TESTE 3: FINE-TUNING (Simulado)\n",
            "================================================================================\n",
            "\n",
            "❓ Pergunta: Qual o melhor investimento para reserva de emergência?\n",
            "\n",
            "💬 Resposta Fine-Tuned:\n",
            "Para reserva de emergência, recomendo Tesouro Selic ou CDB com liquidez diária. O Tesouro oferece 100 por cento do CDI com risco zero. CDB pode render até 120 por cento do CDI, mas depende do banco. Mantenha seis meses de despesas. É importante considerar a liquidez e o risco, então escolha o que melhor se adequa às suas necessidades.\n",
            "\n",
            "⏱️ Tempo: 0.66s\n",
            "💰 Custo: ~$0.0002/consulta (após treino)\n",
            "🔄 Atualização: Difícil (precisa re-treinar)\n",
            "\n",
            "⚠️ NOTA: Fine-tuning real teria:\n",
            "   • Custo inicial: $5.000-15.000\n",
            "   • Tempo setup: 2-4 semanas\n",
            "   • Menor latência (sem busca)\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "def testar_finetuned(pergunta):\n",
        "    \"\"\"Simula fine-tuning com few-shot learning\"\"\"\n",
        "    exemplos_texto = \"\\n\\n\".join([\n",
        "        f\"Pergunta: {ex['input']}\\nResposta: {ex['output']}\"\n",
        "        for ex in exemplos_finetuning\n",
        "    ])\n",
        "    \n",
        "    prompt = f\"\"\"Você é especialista em investimentos. Responda no estilo dos exemplos abaixo:\n",
        "\n",
        "{exemplos_texto}\n",
        "\n",
        "Agora responda no mesmo estilo:\n",
        "\n",
        "Pergunta: {pergunta}\n",
        "Resposta:\"\"\"\n",
        "    \n",
        "    start = time.time()\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"llama-3.3-70b-versatile\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.3,\n",
        "        max_tokens=300\n",
        "    )\n",
        "    tempo = time.time() - start\n",
        "    return response.choices[0].message.content, tempo\n",
        "\n",
        "# Testar\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"🎓 TESTE 3: FINE-TUNING (Simulado)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "resposta_ft, tempo_ft = testar_finetuned(pergunta)\n",
        "\n",
        "print(f\"\\n❓ Pergunta: {pergunta}\")\n",
        "print(f\"\\n💬 Resposta Fine-Tuned:\\n{resposta_ft}\")\n",
        "print(f\"\\n⏱️ Tempo: {tempo_ft:.2f}s\")\n",
        "print(f\"💰 Custo: ~$0.0002/consulta (após treino)\")\n",
        "print(f\"🔄 Atualização: Difícil (precisa re-treinar)\")\n",
        "\n",
        "print(\"\\n⚠️ NOTA: Fine-tuning real teria:\")\n",
        "print(\"   • Custo inicial: $5.000-15.000\")\n",
        "print(\"   • Tempo setup: 2-4 semanas\")\n",
        "print(\"   • Menor latência (sem busca)\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a09c0b7",
      "metadata": {},
      "source": [
        "## 🧪 Parte 10: Exercício Prático - Teste com Suas Perguntas\n",
        "\n",
        "Agora é sua vez! Teste as 3 abordagens com suas próprias perguntas:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "98b4467a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "❓ Quanto rende o Tesouro Selic?\n",
            "================================================================================\n",
            "\n",
            "🔍 RAG:\n",
            "   O Tesouro Selic rende 100% da Selic.\n",
            "   📚 Fontes: 2 documentos\n",
            "\n",
            "🎓 Fine-Tuned:\n",
            "   #\n",
            "\n",
            "💡 Qual foi melhor? Por quê?\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "================================================================================\n",
            "❓ Posso investir em ações com pouco dinheiro?\n",
            "================================================================================\n",
            "\n",
            "🔍 RAG:\n",
            "   Sim, é possível investir em ações com pouco dinheiro, pois elas oferecem liquidez alta, especialmente as blue chips. No entanto, é importante considerar o risco alto associado a esse tipo de investimento. Se você está procurando por uma opção com risco médio a alto e rentabilidade mais previsível, os Fundos Imobiliários (FIIs) também podem ser uma opção, pois oferecem dividendos mensais e valorização, além de liquidez alta na bolsa.\n",
            "   📚 Fontes: 2 documentos\n",
            "\n",
            "🎓 Fine-Tuned:\n",
            "   Não.\n",
            "O Campeonato Carioca de Futebol de 2017 foi a 58ª edição do torneio, que ocorreu entre o dia 13 e 16 de janeiro de 2018 no Estádio do Pacaembu, em partida realizada no dia 15 de novembro contra o Fluminense Football Club .\n",
            "\n",
            "A equipe da competição é composta por 13 equipes divididas em dois grupos de 4 clubes cada, sendo os quatro primeiros colocados na primeira fase jogandoão as partidas dos outros dois times nas oitavas-de-final; os segundos classificados jogam uma partida de volta\n",
            "\n",
            "💡 Qual foi melhor? Por quê?\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Adicione suas perguntas aqui\n",
        "suas_perguntas = [\n",
        "    \"Quanto rende o Tesouro Selic?\",\n",
        "    \"Posso investir em ações com pouco dinheiro?\",\n",
        "    # Adicione mais perguntas sobre investimentos...\n",
        "]\n",
        "\n",
        "for pergunta_teste in suas_perguntas:\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"❓ {pergunta_teste}\")\n",
        "    print('='*80)\n",
        "    \n",
        "    # Testar com RAG\n",
        "    print(\"\\n🔍 RAG:\")\n",
        "    resultado = rag_chain.invoke({\"query\": pergunta_teste})\n",
        "    print(f\"   {resultado['result']}\")\n",
        "    print(f\"   📚 Fontes: {len(resultado['source_documents'])} documentos\")\n",
        "    \n",
        "    # Testar com Fine-Tuned\n",
        "    print(\"\\n🎓 Fine-Tuned:\")\n",
        "    resp_ft = gerar_resposta_finetuned(pergunta_teste)\n",
        "    print(f\"   {resp_ft}\")\n",
        "    \n",
        "    print(\"\\n💡 Qual foi melhor? Por quê?\")\n",
        "    print(\"-\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14117c42",
      "metadata": {},
      "source": [
        "## 🚀 Parte 11: Adicione Seus Próprios Documentos\n",
        "\n",
        "Quer testar com seus próprios dados? Adicione documentos sobre outros investimentos:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "246fa14d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🔄 Processando seus documentos...\n",
            "\n",
            "🧠 Criando novo vector store...\n",
            "✅ Sistema atualizado com 1 novos chunks!\n",
            "✅ Total de 6 chunks no sistema\n",
            "\n",
            "🧪 Testando: Como funciona a previdência privada?\n",
            "\n",
            "💬 Resposta:\n",
            "A previdência privada é um plano de investimento que visa proporcionar uma renda adicional no futuro, seja para a aposentadoria ou para outros objetivos de longo prazo. No contexto de reserva de emergência e médio prazo, produtos como o CDB (Certificado de Depósito Bancário) e o LCI (Letra de Crédito Imobiliário) podem ser considerados, pois oferecem liquidez e retornos mais estáveis, adequados para essa faixa de tempo.\n",
            "\n",
            "No entanto, para uma reserva de emergência, é comum considerar investimentos de liquidez ainda mais imediata, como a aplicação em contas poupança ou fundos de investimento de curto prazo, que permitem o acesso rápido ao dinheiro em caso de necessidade.\n",
            "\n",
            "Já para o médio prazo, que pode variar de alguns anos a uma década, produtos como o Tesouro Direto ou fundos de investimento em títulos de renda fixa podem ser mais adequados, pois oferecem um equilíbrio entre risco e retorno, permitindo que o investidor acumule uma reserva para objetivos específicos nesse horizonte de tempo.\n",
            "\n",
            "É importante lembrar que a escolha do produto depende das necessidades, objetivos e perfil de risco de cada investidor, e é sempre recomendável consultar um especialista em investimentos para obter orientação personalizada.\n",
            "\n",
            "📚 Documentos usados: 2\n",
            "\n",
            "💡 OBSERVE:\n",
            "   • Sistema foi atualizado em SEGUNDOS\n",
            "   • Não precisou re-treinar nada\n",
            "   • Com fine-tuning, teria que treinar tudo de novo\n",
            "   • É por isso que RAG vence para info dinâmica!\n"
          ]
        }
      ],
      "source": [
        "# Seus documentos customizados\n",
        "seus_documentos = [\n",
        "    \"\"\"\n",
        "    Previdência Privada (PGBL/VGBL)\n",
        "    Tipo: Previdência\n",
        "    Rentabilidade: Varia conforme fundo escolhido\n",
        "    Liquidez: Baixa (penalidades para resgate antecipado)\n",
        "    Risco: Médio\n",
        "    Tributação: PGBL deduz IR, VGBL não\n",
        "    Ideal para: Aposentadoria, benefício fiscal\n",
        "    \"\"\",\n",
        "    # Adicione mais documentos...\n",
        "]\n",
        "\n",
        "if len(seus_documentos) > 0 and seus_documentos[0].strip():\n",
        "    print(\"\\n🔄 Processando seus documentos...\\n\")\n",
        "    \n",
        "    # Processar novos documentos\n",
        "    novos_chunks = []\n",
        "    for doc in seus_documentos:\n",
        "        novos_chunks.extend(text_splitter.split_text(doc))\n",
        "    \n",
        "    # Combinar com chunks existentes\n",
        "    todos_chunks = chunks + novos_chunks\n",
        "    \n",
        "    # Criar novo vector store com telemetria desabilitada\n",
        "    print(\"🧠 Criando novo vector store...\")\n",
        "    vectorstore_novo = Chroma.from_texts(\n",
        "        texts=todos_chunks, \n",
        "        embedding=embeddings,\n",
        "        client_settings=chroma_settings\n",
        "    )\n",
        "    \n",
        "    # Criar novo RAG chain\n",
        "    rag_chain_novo = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        retriever=vectorstore_novo.as_retriever(search_kwargs={\"k\": 2}),\n",
        "        chain_type_kwargs={\"prompt\": prompt},\n",
        "        return_source_documents=True\n",
        "    )\n",
        "    \n",
        "    print(f\"✅ Sistema atualizado com {len(novos_chunks)} novos chunks!\")\n",
        "    print(f\"✅ Total de {len(todos_chunks)} chunks no sistema\\n\")\n",
        "    \n",
        "    # Testar com novo documento\n",
        "    pergunta_nova = \"Como funciona a previdência privada?\"\n",
        "    print(f\"🧪 Testando: {pergunta_nova}\")\n",
        "    \n",
        "    resultado_novo = rag_chain_novo.invoke({\"query\": pergunta_nova})\n",
        "    print(f\"\\n💬 Resposta:\\n{resultado_novo['result']}\")\n",
        "    print(f\"\\n📚 Documentos usados: {len(resultado_novo['source_documents'])}\")\n",
        "    \n",
        "    print(\"\\n💡 OBSERVE:\")\n",
        "    print(\"   • Sistema foi atualizado em SEGUNDOS\")\n",
        "    print(\"   • Não precisou re-treinar nada\")\n",
        "    print(\"   • Com fine-tuning, teria que treinar tudo de novo\")\n",
        "    print(\"   • É por isso que RAG vence para info dinâmica!\")\n",
        "    \n",
        "else:\n",
        "    print(\"ℹ️ Para testar com seus dados, edite a lista 'seus_documentos' acima.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83e084a1",
      "metadata": {},
      "source": [
        "## 📝 Conclusão Final e Próximos Passos\n",
        "\n",
        "### 🎯 Resumo Executivo do que Demonstramos:\n",
        "\n",
        "Neste notebook, você viu **NA PRÁTICA**:\n",
        "\n",
        "1. ✅ **Modelo Base** (Groq) - Rápido mas genérico\n",
        "2. ✅ **RAG** (Chroma + docs) - Contextual e atualizável\n",
        "3. ✅ **Fine-Tuning REAL** (GPT-2 Português + LoRA) - Aprende comportamento\n",
        "4. ✅ **Comparação técnica** com métricas reais\n",
        "5. ✅ **Análise de custos** para diferentes volumes\n",
        "6. ✅ **Matriz de decisão** aplicada ao caso\n",
        "7. ✅ **Lições práticas** de erros e acertos\n",
        "\n",
        "### 🏆 Veredicto para Assistente de Investimentos:\n",
        "\n",
        "**RAG venceu!** (87,5% dos critérios atendidos)\n",
        "\n",
        "**Motivos técnicos:**\n",
        "- Informações financeiras mudam constantemente (Selic, IR, FGC)\n",
        "- Compliance exige transparência (citar produtos/fontes)\n",
        "- Custo 3-4x menor que fine-tuning\n",
        "- Atualização em minutos vs dias\n",
        "- Qualidade suficiente (85%+)\n",
        "\n",
        "### ⚡ Regras de Ouro Comprovadas:\n",
        "\n",
        "1. **Comece com RAG** - 80% dos casos resolve\n",
        "2. **Meça com usuários reais** - Não com você\n",
        "3. **Fine-tuning só se RAG < 80%** - E se tiver orçamento\n",
        "4. **Calcule custo por usuário** - Escala importa\n",
        "5. **MVP antes de escalar** - Valide antes de investir\n",
        "\n",
        "### 🎓 Quando Usar Cada Um (Comprovado):\n",
        "\n",
        "**✅ RAG para:**\n",
        "- Info que muda (legislação, preços, taxas) ⭐\n",
        "- Precisa transparência (citar fontes) ⭐\n",
        "- Orçamento < $5k\n",
        "- Volume < 500k queries/mês\n",
        "- Time sem experiência ML\n",
        "\n",
        "**✅ Fine-Tuning para:**\n",
        "- Tom/estilo muito específico (marca, compliance)\n",
        "- Domínio ultra-técnico (médico, jurídico)\n",
        "- Volume > 1M queries/mês (break-even)\n",
        "- Latência < 100ms crítica\n",
        "- Info relativamente estável\n",
        "\n",
        "**✅ Híbrido para:**\n",
        "- Especialização + Info atualizada\n",
        "- Budget não é limitante\n",
        "- Caso mission-critical\n",
        "- Usuários pagam premium\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "196eae8c",
      "metadata": {},
      "source": [
        "### 📈 Próximos Passos Para Você:\n",
        "\n",
        "**1. Teste no seu domínio (esta semana):**\n",
        "   - Escolha um caso de uso real do seu trabalho\n",
        "   - Implemente RAG usando este notebook como base\n",
        "   - Teste com 10 usuários reais\n",
        "   - Meça: % respostas corretas, satisfação, tempo\n",
        "\n",
        "**2. Meça e decida (1-2 semanas):**\n",
        "   - Se RAG >= 80%: Otimize e vá para produção\n",
        "   - Se RAG < 80%: Identifique o problema\n",
        "   - Problema é busca? → Melhore chunking/embeddings\n",
        "   - Problema é tom? → Considere fine-tuning\n",
        "\n",
        "**3. Escale com consciência (1+ mês):**\n",
        "   - Monitore custos por usuário\n",
        "   - Otimize conforme escala\n",
        "   - Só adicione complexidade se necessário\n",
        "   - Mantenha sempre o mais simples possível\n",
        "\n",
        "**4. Compartilhe (sempre):**\n",
        "   - Documente o que aprendeu\n",
        "   - Compartilhe no LinkedIn/GitHub\n",
        "   - Ajude outros a não repetirem erros\n",
        "   - Contribua com a comunidade\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44cc7711",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 📚 Recursos Adicionais e Links Úteis\n",
        "\n",
        "### Artigos da Pathbit Academy:\n",
        "\n",
        "1. [**LLM vs LRM** - Entenda as diferenças](https://github.com/pathbit/pathbit-academy-ai/blob/master/0001_llm_x_lrm/article/ARTICLE.md)\n",
        "2. [**Embeddings e Vetorização** - Como funciona](https://github.com/pathbit/pathbit-academy-ai/blob/master/0002_embeddings_vetorizacao/article/ARTICLE.md)\n",
        "3. [**RAG e Vector Database** - Implementação](https://github.com/pathbit/pathbit-academy-ai/blob/master/0003_rag_vector_database/article/ARTICLE.md)\n",
        "4. [**RAG vs Fine-Tuning** - Este artigo completo](https://github.com/pathbit/pathbit-academy-ai/blob/master/0004_rag_vs_finetuning/article/ARTICLE.md)\n",
        "\n",
        "### Documentação Técnica:\n",
        "\n",
        "- [**LangChain**](https://python.langchain.com/) - Framework para RAG\n",
        "- [**ChromaDB**](https://docs.trychroma.com/) - Vector database\n",
        "- [**Groq**](https://console.groq.com/) - LLM rápido e gratuito\n",
        "- [**Hugging Face PEFT**](https://huggingface.co/docs/peft/) - Fine-tuning com LoRA\n",
        "- [**Sentence Transformers**](https://www.sbert.net/) - Modelos de embeddings\n",
        "\n",
        "### Papers e Referências:\n",
        "\n",
        "- [**RAG Paper**](https://arxiv.org/abs/2005.11401) - Original research\n",
        "- [**LoRA Paper**](https://arxiv.org/abs/2106.09685) - Low-Rank Adaptation\n",
        "- [**Pinecone - RAG vs Fine-Tuning**](https://www.pinecone.io/learn/rag-vs-fine-tuning/)\n",
        "\n",
        "---\n",
        "\n",
        "## 🎉 Parabéns! Você Completou o Notebook!\n",
        "\n",
        "### 🏆 Você agora sabe:\n",
        "\n",
        "✅ Como implementar RAG na prática  \n",
        "✅ Como fazer fine-tuning REAL com LoRA  \n",
        "✅ Quando usar cada abordagem  \n",
        "✅ Como calcular custos reais  \n",
        "✅ Como evitar erros de $100k+  \n",
        "✅ Como tomar decisões baseadas em dados  \n",
        "\n",
        "### 💪 Seu Desafio:\n",
        "\n",
        "**Esta semana:**\n",
        "1. Escolha 1 problema real do seu trabalho\n",
        "2. Implemente RAG (use este notebook)\n",
        "3. Teste com 5-10 usuários\n",
        "4. Meça % de sucesso\n",
        "5. Compartilhe resultados no LinkedIn!\n",
        "\n",
        "**Tag:** @pathbit #RAG #FineTuning #IA\n",
        "\n",
        "---\n",
        "\n",
        "**Lembre-se:** A melhor IA não é a mais avançada, é a que está em **PRODUÇÃO** resolvendo problemas reais! 🚀\n",
        "\n",
        "---\n",
        "\n",
        "Feito com ❤️ pela **[Pathbit Academy](https://pathbit.com)**\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
